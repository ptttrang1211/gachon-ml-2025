{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **ë©€í‹°ëª¨ë‹¬ LLM ì‚¬ìš©í•˜ê¸°**"
      ],
      "metadata": {
        "id": "LRi1YSEBU1cf"
      },
      "id": "LRi1YSEBU1cf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "ğŸ’¡ **NOTE**: ì´ ë…¸íŠ¸ë¶ì˜ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë ¤ë©´ GPUë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. êµ¬ê¸€ ì½”ë©ì—ì„œëŠ” **ëŸ°íƒ€ì„ > ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½ > í•˜ë“œì›¨ì–´ ê°€ì†ê¸° > T4 GPU**ë¥¼ ì„ íƒí•˜ì„¸ìš”.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "6trJcno3X-yN"
      },
      "id": "6trJcno3X-yN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ë¼ì´ë¸ŒëŸ¬ë¦¬**"
      ],
      "metadata": {
        "id": "3EWKy7AUag73"
      },
      "id": "3EWKy7AUag73"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "187e9370-9be0-41de-b20d-660539575eae",
      "metadata": {
        "id": "187e9370-9be0-41de-b20d-660539575eae"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "print(\"torch:\", torch.__version__)\n",
        "print(\"torchvision:\", torchvision.__version__)\n",
        "print(\"CUDA available?:\", torch.cuda.is_available())\n",
        "print(\"GPU:\", torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6c93e80-995d-40a7-b7f8-4f16d204be4f",
      "metadata": {
        "collapsed": true,
        "id": "e6c93e80-995d-40a7-b7f8-4f16d204be4f"
      },
      "outputs": [],
      "source": [
        "# âš™ï¸ 1. íŒ¨í‚¤ì§€ ì„¤ì¹˜ (í•„ìš” ì‹œ)\n",
        "# transformers(í—ˆê¹…í˜ì´ìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬), pillow ì„¤ì¹˜ê°€ ì•„ì§ ì•ˆ ë˜ì–´ ìˆìœ¼ë©´ ì•„ë˜ ì£¼ì„ì„ í•´ì œí•˜ê³  ì‹¤í–‰í•˜ì„¸ìš”.\n",
        "!pip install transformers==4.48.3 pillow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ì„¤ì¹˜ëœ transformers ë²„ì „ í™•ì¸  -->(ex: Version: 4.55.4)\n",
        "!pip list | grep -E 'transformers'"
      ],
      "metadata": {
        "id": "26gPEoY-YlEw"
      },
      "id": "26gPEoY-YlEw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "HXKDLa_pp3FF"
      },
      "id": "HXKDLa_pp3FF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **CLIP ê¸°ë°˜ ëª¨ë¸**\n",
        "- CLIP(Contrastive Language-Image Pre-training, ëŒ€ì¡°í•™ìŠµ ì–¸ì–´-ì´ë¯¸ì§€ ì‚¬ì „ í›ˆë ¨ ëª¨ë¸)\n",
        "- ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ì˜ ì„ë² ë”©ì„ ëª¨ë‘ ê³„ì‚°í•  ìˆ˜ ìˆëŠ” ì„ë² ë”© ëª¨ë¸"
      ],
      "metadata": {
        "id": "-kjoOOe0U2Bl"
      },
      "id": "-kjoOOe0U2Bl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### âœ… **ì˜ˆì œ : CLIPìœ¼ë¡œ ì´ë¯¸ì§€ì— ë§ëŠ” ìº¡ì…˜ ì„ íƒí•˜ê¸°**\n",
        "\n",
        "- **ëª¨ë¸ëª…** : **openai/clip-vit-base-patch32**\n",
        "- **ì´ íŒŒë¼ë¯¸í„°**: ì•½ 1.5ì–µ (150M)\n",
        "- **í•µì‹¬ ê¸°ëŠ¥**: ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ë§¤ì¹­ (Zero-Shot ë¶„ë¥˜, ì´ë¯¸ì§€ ê²€ìƒ‰ ë“±)\n",
        "    - **base**: ëª¨ë¸ì˜ í¬ê¸°, ë³´í†µ 'Small', 'Base', 'Large' ë“±ìœ¼ë¡œ êµ¬ë¶„\n",
        "    - **patch32** : ì´ë¯¸ì§€ë¥¼ 32x32 í”½ì…€ í¬ê¸°ì˜ 'íŒ¨ì¹˜(Patch)' ì¡°ê°ë“¤ë¡œ ë‚˜ëˆ„ì–´ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©\n",
        "    - ì´ ëª¨ë¸ì€ **ì œë¡œìƒ· ì´ë¯¸ì§€ ë¶„ë¥˜(Zero-Shot Image Classification**)ì— ê°€ì¥ ë„ë¦¬ ì‚¬ìš©ë¨\n",
        "    - ëª¨ë¸ì—ê²Œ íŠ¹ì • ì´ë¯¸ì§€ í•œ ì¥ê³¼ [\"ê°•ì•„ì§€ ì‚¬ì§„\", \"ê³ ì–‘ì´ ì‚¬ì§„\", \"ìë™ì°¨ ì‚¬ì§„\"] ê°™ì€ í…ìŠ¤íŠ¸ í›„ë³´ë“¤ì„ ì£¼ë©´, ëª¨ë¸ì´ í•´ë‹¹ ì´ë¯¸ì§€ë¥¼ í•™ìŠµí•œ ì ì´ ì—†ì–´ë„ ì–´ë–¤ í…ìŠ¤íŠ¸ ì„¤ëª…ê³¼ ê°€ì¥ ìœ ì‚¬í•œì§€ ì•Œë ¤ì¤„ ìˆ˜ ìˆìŒ\n",
        "\n",
        "|êµ¬ì„± ìš”ì†Œ|\tì‚¬ìš©ëœ íŠ¹ì • ëª¨ë¸|\tíŒŒë¼ë¯¸í„°|\tí•™ìŠµ ì—¬ë¶€ (CLIP ê¸°ì¤€)|\tì—­í•  ë° íŠ¹ì§•|\n",
        "|---|---|---|---|---|\n",
        "|**1. Vision Encoder**(ì‹œê° ì¸ì½”ë”)|\tViT-Base (Patch 32)|\t~8,800ë§Œ (88M)\t|ğŸ”¥ **Trained** (í•™ìŠµ)|\tì´ë¯¸ì§€ë¥¼ 32x32 í”½ì…€ì˜ 'íŒ¨ì¹˜(patch)'ë¡œ ì˜ë¼ ì‹œí€€ìŠ¤ë¡œ ë§Œë“  í›„, ì´ë¯¸ì§€ì˜ ì‹œê°ì  íŠ¹ì§•ì„ ì¶”ì¶œí•˜ì—¬ **í•˜ë‚˜ì˜ ë²¡í„°(ì„ë² ë”©)**ë¡œ ì••ì¶•í•©ë‹ˆë‹¤.|\n",
        "|**2. Text Encoder** (í…ìŠ¤íŠ¸ ì¸ì½”ë”)|\tCausal Transformer (BERT-like)|\t~6,300ë§Œ (63M)\t|ğŸ”¥ **Trained**(í•™ìŠµ)\t|ì…ë ¥ëœ í…ìŠ¤íŠ¸(ë¬¸ì¥)ë¥¼ í† í°í™”í•˜ê³ , í…ìŠ¤íŠ¸ì˜ ì˜ë¯¸ì  íŠ¹ì§•ì„ ì¶”ì¶œí•˜ì—¬ **í•˜ë‚˜ì˜ ë²¡í„°(ì„ë² ë”©)**ë¡œ ì••ì¶•í•©ë‹ˆë‹¤.|\n",
        "|(í•™ìŠµ ëª©í‘œ)|\t**Contrastive Learning**|\t-|\t-\t|4ì–µ ê°œì˜ (ì´ë¯¸ì§€, í…ìŠ¤íŠ¸) ìŒ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬, 1. 'ì¼ì¹˜í•˜ëŠ”' ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ë²¡í„°ëŠ” ê°€ê¹ê²Œ (ìœ ì‚¬ë„ â†‘) 2. 'ì¼ì¹˜í•˜ì§€ ì•ŠëŠ”' ìŒì˜ ë²¡í„°ëŠ” ë©€ê²Œ (ìœ ì‚¬ë„ â†“) ë§Œë“¤ë„ë¡ ë‘ ì¸ì½”ë”ë¥¼ í•¨ê»˜ í•™ìŠµì‹œí‚µë‹ˆë‹¤.|"
      ],
      "metadata": {
        "id": "2vks4vggVTfH"
      },
      "id": "2vks4vggVTfH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59b6745d-cc5f-41be-ac8c-6e8b839e6bc9",
      "metadata": {
        "id": "59b6745d-cc5f-41be-ac8c-6e8b839e6bc9"
      },
      "outputs": [],
      "source": [
        "# ë¼ì´ë¸ŒëŸ¬ë¦¬ ê°€ì ¸ì˜¤ê¸°\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from transformers import CLIPTokenizerFast, CLIPImageProcessor, CLIPModel\n",
        "# from transformers import CLIPTokenizerFast, CLIPProcessor, CLIPModel\n",
        "\n",
        "# ë””ë°”ì´ìŠ¤ ì„¤ì •\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "\n",
        "# ì‚¬ìš©í•  CLIP ê¸°ë°˜ ì„ë² ë”© ëª¨ë¸\n",
        "model_name = \"openai/clip-vit-base-patch32\"\n",
        "\n",
        "# 1) ì´ë¯¸ì§€ ì „ì²˜ê¸°: Fast ë²„ì „ ì‚¬ìš©\n",
        "clip_processor = CLIPImageProcessor.from_pretrained(model_name, use_fast=True)\n",
        "\n",
        "# 2) í† í¬ë‚˜ì´ì €: Fast í† í¬ë‚˜ì´ì € ì‚¬ìš©\n",
        "clip_tokenizer = CLIPTokenizerFast.from_pretrained(model_name)\n",
        "\n",
        "# 3) CLIP ê¸°ë°˜ ëª¨ë¸ ì •ì˜:  Safetensors í¬ë§·ìœ¼ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ë¶ˆëŸ¬ì˜¤ë„ë¡ ì„¤ì •\n",
        "model = CLIPModel.from_pretrained(\n",
        "    model_name,\n",
        "    use_safetensors=True    # ì´ ì˜µì…˜ì„ ì¶”ê°€í•˜ì„¸ìš”\n",
        ").to(device)\n",
        "\n",
        "\n",
        "# í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬í•˜ê¸°\n",
        "captions = [\n",
        "    \"a photo of a cat\",\n",
        "    \"a photo of a dog\",\n",
        "    \"a landscape with mountains\"\n",
        "]\n",
        "# Fast í† í¬ë‚˜ì´ì €ë¥¼ ì´ìš©í•´ í† í°í™” + íŒ¨ë”©\n",
        "text_inputs = clip_tokenizer(\n",
        "    captions,\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True,      # ê¸¸ì´ ë§ì¶¤\n",
        "    truncation=True    # í•„ìš” ì‹œ ìë¥´ê¸°\n",
        ")\n",
        "input_ids = text_inputs.input_ids.to(device)\n",
        "attention_mask = text_inputs.attention_mask.to(device)\n",
        "\n",
        "\n",
        "# ì´ë¯¸ì§€ ì „ì²˜ë¦¬\n",
        "image_path = \"Golden-Retriever.jpg\"\n",
        "image = Image.open(image_path).convert(\"RGB\")\n",
        "image_inputs = clip_processor(\n",
        "    images=image,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "pixel_values = image_inputs.pixel_values.to(device)\n",
        "pixel_values.shape\n",
        "\n",
        "\n",
        "image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9caee6a-92f0-49dd-bc88-2fc79aa452eb",
      "metadata": {
        "id": "a9caee6a-92f0-49dd-bc88-2fc79aa452eb"
      },
      "outputs": [],
      "source": [
        "# ì„ë² ë”© ìƒì„±í•˜ê¸°\n",
        "with torch.no_grad():\n",
        "    # ì´ë¯¸ì§€ ì„ë² ë”© (batch_size=1, 512ì°¨ì›)\n",
        "    image_embedding = model.get_image_features(pixel_values=pixel_values)\n",
        "\n",
        "    # í…ìŠ¤íŠ¸ ì„ë² ë”© (batch_size=len(texts), 512ì°¨ì›)\n",
        "    text_embedding = model.get_text_features(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **ìœ ì‚¬ë„ ê³„ì‚°í•˜ê¸°** (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)"
      ],
      "metadata": {
        "id": "ifUwn1Q5edXr"
      },
      "id": "ifUwn1Q5edXr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6caeb9d9-f923-49f5-b05c-0e6ab614b4f8",
      "metadata": {
        "id": "6caeb9d9-f923-49f5-b05c-0e6ab614b4f8"
      },
      "outputs": [],
      "source": [
        "# 8. ìœ ì‚¬ë„ ê³„ì‚° (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)\n",
        "# ë¨¼ì € ì„ë² ë”©ì„ ì •ê·œí™”(normalize)í•©ë‹ˆë‹¤.\n",
        "image_embedding = image_embedding / image_embedding.norm(p=2, dim=-1, keepdim=True)\n",
        "text_embedding = text_embedding / text_embedding.norm(p=2, dim=-1, keepdim=True)\n",
        "\n",
        "# ì½”ì‚¬ì¸ ìœ ì‚¬ë„: image_embeds @ text_embeds.T\n",
        "similarity = (image_embedding @ text_embedding.T).squeeze(0)  # shape: (len(texts),)\n",
        "# CPUë¡œ ì˜®ê²¨ì„œ ì¶œë ¥\n",
        "similarity = similarity.cpu().tolist()\n",
        "\n",
        "# ìœ ì‚¬ë„ ê²°ê³¼ ì¶œë ¥\n",
        "for text, score in zip(captions, similarity):\n",
        "    print(f\"â€˜{text}â€™ ìœ ì‚¬ë„: {score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Zero-shot ë¶„ë¥˜í•˜ê¸°**\n",
        "    - ë¡œì§“ ë³€í™˜ ì‚¬ìš©\n",
        "        - ë¡œì§“ ë³€í™˜(Logit Transformation)ì´ë€ 0ê³¼ 1 ì‚¬ì´ì˜ í™•ë¥ (p) ê°’ì„ ì„ ì´ìš©í•´ ì‹¤ìˆ˜ ì „ì²´( $-\\infty$ ~ $+\\infty$) ë²”ìœ„ì˜ ê°’(ë¡œì§€íŠ¸)ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ìˆ˜í•™ì  í•¨ìˆ˜\n",
        "$log(\\frac{p}{1-p})$, ë¡œì§“ ë³€í™˜ìœ¼ë¡œ ê°’ì„ ì¦í­ì‹œì¼œ ê°’ì„ ê°•ì¡°í•¨\n"
      ],
      "metadata": {
        "id": "H5Z9VOeveq9c"
      },
      "id": "H5Z9VOeveq9c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "233f3d08-e221-4152-aa0d-c3810c7be3eb",
      "metadata": {
        "id": "233f3d08-e221-4152-aa0d-c3810c7be3eb"
      },
      "outputs": [],
      "source": [
        "# 10. Zero-shot ë¶„ë¥˜ ì˜ˆì œ\n",
        "# CLIPModelì˜ ë¡œì§€íŠ¸(ë¡œê·¸ì‡) ìŠ¤ì½”ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì¥ ì„ íƒ\n",
        "with torch.no_grad():\n",
        "    outputs = model(\n",
        "        pixel_values=pixel_values,      # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ê²°ê³¼\n",
        "        input_ids=input_ids,            # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ê²°ê³¼\n",
        "        attention_mask=attention_mask\n",
        "    )\n",
        "    logits_per_image = outputs.logits_per_image.squeeze(0)  # ì´ë¯¸ì§€ â†’ í…ìŠ¤íŠ¸ ìŠ¤ì½”ì–´\n",
        "    probs = logits_per_image.softmax(dim=-1).cpu().tolist()\n",
        "    print(logits_per_image)\n",
        "    print(probs)\n",
        "\n",
        "# í™•ë¥  ê¸°ë°˜ìœ¼ë¡œ ê°€ì¥ ë†’ì€ ë¼ë²¨ í™•ì¸\n",
        "best_idx = int(torch.argmax(logits_per_image))\n",
        "print(f\"\\nZero-shot ë¶„ë¥˜ ê²°ê³¼: '{captions[best_idx]}' (í™•ë¥  {probs[best_idx]:.3f})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "gSLjKBuGlIA5"
      },
      "id": "gSLjKBuGlIA5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **BLIP ê¸°ë°˜ ëª¨ë¸**\n",
        "\n",
        "- **BLIP** **Bootstrapping Language-Image Pre-training** for Unified Vision-Language Understanding and Generation\n",
        "- \"í•˜ë‚˜ì˜ ëª¨ë¸ ì•„í‚¤í…ì²˜ë¡œ 'ì´í•´(Understanding)'ì™€ 'ìƒì„±(Generation)'ì„ ëª¨ë‘ ì˜í•˜ë©´ì„œ, ë™ì‹œì— 'ì§€ì €ë¶„í•œ ì›¹ ë°ì´í„°(Noisy Data)'ë¥¼ ìŠ¤ìŠ¤ë¡œ ì •ì œí•´ì„œ í•™ìŠµ íš¨ìœ¨ì„ ê·¹ëŒ€í™”í•˜ì!\""
      ],
      "metadata": {
        "id": "D0ThxRb2lINE"
      },
      "id": "D0ThxRb2lINE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### âœ… **ì˜ˆì œ : BLIP ê¸°ë°˜ ëª¨ë¸ë¡œ ì´ë¯¸ì§€ì— ìë™ ìº¡ì…”ë‹í•˜ê¸°**\n",
        "- **ëª¨ë¸ëª…** :  **Salesforce/blip-image-captioning-base**\n",
        "- **ì´ íŒŒë¼ë¯¸í„°**: ì•½ 2.24ì–µ (224M)\n",
        "- **í•µì‹¬ ê¸°ëŠ¥**: ì´ë¯¸ì§€ ìº¡ì…˜ ìƒì„± (ì´ë¯¸ì§€ë¥¼ ë³´ê³  ë¬¸ì¥ ìƒì„±)\n",
        "\n",
        "\n",
        "|êµ¬ì„± ìš”ì†Œ|\tì‚¬ìš©ëœ íŠ¹ì • ëª¨ë¸|\tíŒŒë¼ë¯¸í„°|\tí•™ìŠµ ì—¬ë¶€ (BLIP ê¸°ì¤€)|\tì—­í•  ë° íŠ¹ì§•|\n",
        "|---|---|---|---|---|\n",
        "|**1. Vision Encoder** (ì‹œê° ì¸ì½”ë”)|\tViT-Base (Patch 16)|\t~8,600ë§Œ (86M)\t|ğŸ”¥ **Trained** (í•™ìŠµ)\t|ì´ë¯¸ì§€ë¥¼ 16x16 í”½ì…€ íŒ¨ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì‹œê° íŠ¹ì§•(feature)ì„ ì¶”ì¶œí•©ë‹ˆë‹¤. (BLIP-2ì™€ ë‹¬ë¦¬, **BLIP-1ì€ ì´ ì¸ì½”ë”ë„ í•¨ê»˜ í•™ìŠµ**ì‹œí‚µë‹ˆë‹¤.)|\n",
        "|**2. Language Model** (ì–¸ì–´ ëª¨ë¸)|\tBERT-base (MED)\t|~1.38ì–µ (138M)\t|ğŸ”¥ **Trained** (í•™ìŠµ)|\t(í•µì‹¬) BERT-base ê¸°ë°˜ì˜ Encoder-Decoder êµ¬ì¡° (MED)ì…ë‹ˆë‹¤. 1. (**Encoder**): ì´ë¯¸ì§€ íŠ¹ì§•ê³¼ í…ìŠ¤íŠ¸ë¥¼ í•¨ê»˜ ì´í•´ (ITM) 2. (**Decoder**): ì´ë¯¸ì§€ íŠ¹ì§•ì„ Cross-Attentionìœ¼ë¡œ ì°¸ì¡°í•˜ë©° í…ìŠ¤íŠ¸ë¥¼ 'ìƒì„±' (LM) ì´ ëª¨ë¸ì€ (2)ë²ˆ ê¸°ëŠ¥(ìº¡ì…˜ ìƒì„±)ì— íŠ¹í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.|\n",
        "|**(ë°ì´í„°)**|\t**CapFilt** (ì •ì œ ë°ì´í„°)|\t-\t|-\t|BLIP-1ì˜ í•µì‹¬ ê¸°ì—¬. ì›¹ì—ì„œ ìˆ˜ì§‘í•œ ë…¸ì´ì¦ˆ ë°ì´í„°ë¥¼ ìì²´ ìƒì„±(Captioner) ë° í•„í„°ë§(Filter)í•˜ì—¬ '**ê¹¨ë—í•œ ë°ì´í„°'ë¥¼ ìë™ ìƒì„±**í•˜ì—¬ í•™ìŠµì— ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.|"
      ],
      "metadata": {
        "id": "BblEDezmpJf7"
      },
      "id": "BblEDezmpJf7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6178430d-c910-4d0a-9fa6-3f2fa1584cc3",
      "metadata": {
        "id": "6178430d-c910-4d0a-9fa6-3f2fa1584cc3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "import requests\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "\n",
        "\n",
        "# 1. í”„ë¡œì„¸ì„œì™€ ëª¨ë¸ ë¡œë“œ (ì´ë¯¸ì§€ ìº¡ì…”ë‹ìš©)\n",
        "model_name = \"Salesforce/blip-image-captioning-base\"\n",
        "blip_processor = BlipProcessor.from_pretrained(model_name)\n",
        "model = BlipForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "# 2. ì´ë¯¸ì§€ ì¤€ë¹„ (BLIP-2ì™€ ë™ì¼í•œ ì´ë¯¸ì§€)\n",
        "url = \"https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg\"\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. ì´ë¯¸ì§€ ìº¡ì…˜ ìƒì„± (LM ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸)\n",
        "\n",
        "# (1) ì¡°ê±´ ì—†ì´ ìƒì„± (Unconditional Captioning)\n",
        "# ì´ë¯¸ì§€ë§Œ ì…ë ¥ìœ¼ë¡œ ë„£ì–´ì¤ë‹ˆë‹¤.\n",
        "inputs_uncond = blip_processor(images=image, return_tensors=\"pt\")\n",
        "out_uncond = model.generate(**inputs_uncond, max_new_tokens=50)\n",
        "caption_uncond = blip_processor.decode(out_uncond[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"--- BLIP-1 ì´ë¯¸ì§€ ìº¡ì…”ë‹ (ì¡°ê±´ ì—†ìŒ) ---\")\n",
        "print(f\"ìƒì„±ëœ ìº¡ì…˜: {caption_uncond}\")\n",
        "\n",
        "\n",
        "# (2) ì¡°ê±´ë¶€ ìƒì„± (Conditional Captioning)\n",
        "# \"a man\"ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ìº¡ì…˜ì„ ìœ ë„í•©ë‹ˆë‹¤.\n",
        "text = \"a man \"\n",
        "inputs_cond = blip_processor(images=image, text=text, return_tensors=\"pt\")\n",
        "out_cond = model.generate(**inputs_cond, max_new_tokens=50)\n",
        "caption_cond = blip_processor.decode(out_cond[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"\\n--- BLIP-1 ì´ë¯¸ì§€ ìº¡ì…”ë‹ (ì¡°ê±´ë¶€: 'a man') ---\")\n",
        "print(f\"ìƒì„±ëœ ìº¡ì…˜: {caption_cond}\")"
      ],
      "metadata": {
        "id": "qFnjD8I-l_1L"
      },
      "id": "qFnjD8I-l_1L",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "0STlT5lAzVUm"
      },
      "id": "0STlT5lAzVUm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **BLIP-2**"
      ],
      "metadata": {
        "id": "aFK8DU_DzWj2"
      },
      "id": "aFK8DU_DzWj2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### âœ… **ì˜ˆì œ : BLIP-2 ê¸°ë°˜ ëª¨ë¸ë¡œ ì´ë¯¸ì§€ì— ì§ˆë¬¸ì„ í•˜ì!(VQA)**\n",
        "VQA(Visual Question Answering)\n",
        "\n",
        "- BLIP-2 ê¸°ë°˜ ëª¨ë¸ : **Salesforce/blip2-flan-t5-xxl**\n",
        "    - BLIP-2 ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•˜ë˜,\n",
        "    - LLMìœ¼ë¡œ Googleì˜ 'FLAN-T5-XXL' ëª¨ë¸ì„ íƒ‘ì¬í•œ,\n",
        "        - ëª¨ë¸ì´ ë‹¨ìˆœíˆ ì´ë¯¸ì§€ë¥¼ ë¬˜ì‚¬í•˜ëŠ” ê²ƒì„ ë„˜ì–´ 'ì§€ì‹œ(Instruction)'ë¥¼ ë”°ë¥´ëŠ” ì œë¡œìƒ·(Zero-Shot) VQA ì„±ëŠ¥ì´ ì¢‹ì€ ëª¨ë¸(VQA ë²¤ì¹˜ë§ˆí¬ì—ì„œ SOTA(State-of-the-Art)ë¥¼ ë‹¬ì„±)\n",
        "        - XXLì˜ ì˜ë¯¸ : FLAN-T5 ê³„ì—´ ì¤‘ì—ì„œë„ 110ì–µ ê°œ(11B)ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì§„ ì´ˆê±°ëŒ€ ëª¨ë¸\n",
        "    - Salesforceê°€ í•™ìŠµì‹œí‚¨ Vision-Language ëª¨ë¸\n",
        "\n",
        "|êµ¬ì„± ìš”ì†Œ|\tì‚¬ìš©ëœ íŠ¹ì • ëª¨ë¸|\tíŒŒë¼ë¯¸í„°|\tí•™ìŠµ ì—¬ë¶€ (BLIP-2 ê¸°ì¤€)|\tì—­í•  ë° íŠ¹ì§•|\n",
        "|---|---|---|---|---|\n",
        "|**1. Vision Encoder** (ì‹œê° ì¸ì½”ë”)|\tCLIP ViT-G/14|\t~10ì–µ (1B)\t|ğŸ§Š Frozen (ë™ê²°)\t|ì´ë¯¸ì§€ë¥¼ ë³´ê³  í”½ì…€ì—ì„œ ë³µì¡í•œ ì‹œê° íŠ¹ì§•(feature)ì„ ì¶”ì¶œí•©ë‹ˆë‹¤. (G/14ëŠ” ë§¤ìš° í° ViT ëª¨ë¸ì…ë‹ˆë‹¤.)|\n",
        "|**2. Q-Former** (ì‹œê°-ì–¸ì–´ ë²ˆì—­ê¸°)|\tBLIP-2 Q-Former|\t~1.8ì–µ (180M)|\tğŸ”¥ Trained (í•™ìŠµ)\t|ViTê°€ ì¶”ì¶œí•œ ë°©ëŒ€í•œ ì‹œê° íŠ¹ì§•ì„ LLMì´ ì´í•´í•  ìˆ˜ ìˆëŠ” 32ê°œì˜ ê°„ê²°í•œ 'ì‹œê° ìš”ì•½ ë²¡í„°'ë¡œ ì••ì¶•/ë²ˆì—­í•©ë‹ˆë‹¤.|\n",
        "|**3. LLM** (ì–¸ì–´ ë‘ë‡Œ)|\tFLAN-T5-XXL|\t~110ì–µ (11B)\t|ğŸ§Š Frozen (ë™ê²°)|\t(í•µì‹¬) Googleì´ T5 ëª¨ë¸ì„ 'ì§€ì‹œ(Instruction)' ë°ì´í„°ë¡œ ì¶”ê°€ í•™ìŠµì‹œí‚¨ ëª¨ë¸. ë‹¨ìˆœ ì§ˆì˜ì‘ë‹µì´ ì•„ë‹Œ, í”„ë¡¬í”„íŠ¸ë¡œ ì£¼ì–´ì§„ 'ì§€ì‹œ'ë¥¼ ìˆ˜í–‰í•˜ëŠ” ë° íŠ¹í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.|\n",
        "\n"
      ],
      "metadata": {
        "id": "hi59uYstqeMk"
      },
      "id": "hi59uYstqeMk"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes accelerate transformers"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Q54jgV5m10zF"
      },
      "id": "Q54jgV5m10zF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "import requests\n",
        "from transformers import Blip2Processor, Blip2ForConditionalGeneration, BitsAndBytesConfig\n",
        "\n",
        "# GPU ì„¤ì •\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# ì–‘ìí™” ì„¤ì • (ìµœì‹  ë°©ì‹)\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,  # 8bit ì–‘ìí™” í™œì„±í™”\n",
        "    bnb_8bit_compute_dtype=torch.float16,  # ê³„ì‚°ì€ float16ìœ¼ë¡œ\n",
        "    bnb_8bit_use_double_quant=True,  # ì´ì¤‘ ì–‘ìí™”ë¡œ ë”ìš± ë©”ëª¨ë¦¬ ì ˆì•½\n",
        "    bnb_8bit_quant_type=\"nf4\"  # ì •ê·œí™”ëœ 4ë¹„íŠ¸ ì–‘ìí™” íƒ€ì…\n",
        ")\n",
        "\n",
        "# ëª¨ë¸ ë¡œë“œ\n",
        "model_name = \"Salesforce/blip2-flan-t5-xxl\"\n",
        "processor = Blip2Processor.from_pretrained(model_name)\n",
        "model = Blip2ForConditionalGeneration.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=quantization_config,  # ìƒˆë¡œìš´ ë°©ì‹\n",
        "    device_map=\"auto\"  # GPU ìë™ ë°°ì¹˜\n",
        ")\n",
        "\n",
        "# ì´ë¯¸ì§€ ì¤€ë¹„\n",
        "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"  # ìê³  ìˆëŠ” ê³ ì–‘ì´\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "# ì´ë¯¸ì§€ ìº¡ì…”ë‹\n",
        "inputs = processor(image, return_tensors=\"pt\").to(device, torch.float16)\n",
        "generated_ids = model.generate(**inputs, max_new_tokens=50)\n",
        "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
        "\n",
        "print(f\"ì´ë¯¸ì§€ ì„¤ëª…: {generated_text}\")\n",
        "\n",
        "image"
      ],
      "metadata": {
        "id": "aPFeWmzIz8nE"
      },
      "id": "aPFeWmzIz8nE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. í”„ë¡¬í”„íŠ¸ ì¤€ë¹„ (FLAN-T5ì˜ í˜: 'ì§€ì‹œ'ë¥¼ ë‚´ë¦°ë‹¤)\n",
        "\n",
        "# í”„ë¡¬í”„íŠ¸ 1: ë‹¨ìˆœ VQA\n",
        "question1 = \"What is in the image?\"\n",
        "prompt1 = f\"Question: {question1} Answer:\"\n",
        "\n",
        "# í”„ë¡¬í”„íŠ¸ 2: ê³ ì–‘ì´ ìˆ«ì ì„¸ê¸°\n",
        "question2 = \"How many cats are in the image?\"\n",
        "prompt2 = f\"Question: {question2} Answer:\"\n",
        "\n",
        "# í”„ë¡¬í”„íŠ¸ 3: ì¶”ë¡  (Reasoning) ì§€ì‹œ\n",
        "question3 = \"What is the cat doing in the image?\"\n",
        "prompt3 = f\"Question: {question3} Answer:\"\n",
        "\n",
        "\n",
        "print(\"--- BLIP-2 (FLAN-T5-XXL) ì œë¡œìƒ· í…ŒìŠ¤íŠ¸ ---\")\n",
        "\n",
        "# 5. ì¶”ë¡  ì‹¤í–‰ (gradient ê³„ì‚° ë¶ˆí•„ìš”)\n",
        "with torch.no_grad():\n",
        "\n",
        "    # --- í”„ë¡¬í”„íŠ¸ 1 ì‹¤í–‰ ---\n",
        "    inputs1 = processor(images=image, text=prompt1, return_tensors=\"pt\").to(device, dtype=torch.float16)\n",
        "    generated_ids1 = model.generate(**inputs1, max_new_tokens=20)\n",
        "    answer1 = processor.batch_decode(generated_ids1, skip_special_tokens=True)[0].strip()\n",
        "    print(f\"ì§ˆë¬¸ 1: {question1}\")\n",
        "    print(f\"ë‹µë³€ 1: {answer1}\")\n",
        "\n",
        "    # --- í”„ë¡¬í”„íŠ¸ 2 ì‹¤í–‰ ---\n",
        "    inputs2 = processor(images=image, text=prompt2, return_tensors=\"pt\").to(device, dtype=torch.float16)\n",
        "    generated_ids2 = model.generate(**inputs2, max_new_tokens=10)\n",
        "    answer2 = processor.batch_decode(generated_ids2, skip_special_tokens=True)[0].strip()\n",
        "    print(f\"\\nì§ˆë¬¸ 2: {question2}\")\n",
        "    print(f\"ë‹µë³€ 2: {answer2}\")\n",
        "\n",
        "    # --- í”„ë¡¬í”„íŠ¸ 3 ì‹¤í–‰ ---\n",
        "    inputs3 = processor(images=image, text=prompt3, return_tensors=\"pt\").to(device, dtype=torch.float16)\n",
        "    generated_ids3 = model.generate(**inputs3, max_new_tokens=20)\n",
        "    answer3 = processor.batch_decode(generated_ids3, skip_special_tokens=True)[0].strip()\n",
        "    print(f\"\\nì§ˆë¬¸ 3: {question3}\")\n",
        "    print(f\"ë‹µë³€ 3: {answer3}\")"
      ],
      "metadata": {
        "id": "Jm3Sh6Wjz8r0"
      },
      "id": "Jm3Sh6Wjz8r0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### âœ… **ì˜ˆì œ : ì´ë¯¸ì§€ ì±—ë´‡**\n",
        "- ëª¨ë¸ëª… : Qwen/Qwen2-VL-2B-Instruct"
      ],
      "metadata": {
        "id": "jgFUXehQP8Mm"
      },
      "id": "jgFUXehQP8Mm"
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# Qwen2-VL í•œêµ­ì–´ ì´ë¯¸ì§€ ì±„íŒ…ë´‡\n",
        "# ==========================================\n",
        "\n",
        "# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
        "!pip install -q transformers accelerate qwen-vl-utils pillow torch\n",
        "\n",
        "import torch\n",
        "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
        "from qwen_vl_utils import process_vision_info\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from IPython.display import display, HTML\n",
        "import ipywidgets as widgets\n",
        "\n",
        "# ==========================================\n",
        "# 1. ëª¨ë¸ ë° í”„ë¡œì„¸ì„œ ë¡œë“œ\n",
        "# ==========================================\n",
        "print(\"ğŸ“¦ ëª¨ë¸ì„ ë¡œë”© ì¤‘ì…ë‹ˆë‹¤...\")\n",
        "\n",
        "# GPU ë©”ëª¨ë¦¬ê°€ ì œí•œì ì¸ ê²½ìš° 2B ëª¨ë¸ ì‚¬ìš© ê¶Œì¥\n",
        "model_name = \"Qwen/Qwen2-VL-2B-Instruct\"  # ë˜ëŠ” \"Qwen/Qwen2-VL-7B-Instruct\"\n",
        "\n",
        "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,  # ë©”ëª¨ë¦¬ ì ˆì•½\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_name)\n",
        "\n",
        "print(\"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!\")\n",
        "\n",
        "# ==========================================\n",
        "# 2. ì´ë¯¸ì§€ ì²˜ë¦¬ í•¨ìˆ˜\n",
        "# ==========================================\n",
        "def load_image(image_source):\n",
        "    \"\"\"\n",
        "    ì´ë¯¸ì§€ë¥¼ URL ë˜ëŠ” ë¡œì»¬ íŒŒì¼ì—ì„œ ë¡œë“œí•©ë‹ˆë‹¤.\n",
        "\n",
        "    Parameters:\n",
        "    - image_source: ì´ë¯¸ì§€ URL ë˜ëŠ” ë¡œì»¬ íŒŒì¼ ê²½ë¡œ\n",
        "\n",
        "    Returns:\n",
        "    - PIL Image ê°ì²´\n",
        "    \"\"\"\n",
        "    if image_source.startswith('http'):\n",
        "        response = requests.get(image_source)\n",
        "        image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "    else:\n",
        "        image = Image.open(image_source).convert(\"RGB\")\n",
        "    return image\n",
        "\n",
        "# ==========================================\n",
        "# 3. ì±„íŒ… í•¨ìˆ˜\n",
        "# ==========================================\n",
        "def chat_with_image(image, question, chat_history=[]):\n",
        "    \"\"\"\n",
        "    ì´ë¯¸ì§€ì— ëŒ€í•œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” í•¨ìˆ˜\n",
        "\n",
        "    Parameters:\n",
        "    - image: PIL Image ê°ì²´ ë˜ëŠ” ì´ë¯¸ì§€ ê²½ë¡œ\n",
        "    - question: ì‚¬ìš©ì ì§ˆë¬¸ (í•œêµ­ì–´)\n",
        "    - chat_history: ì´ì „ ëŒ€í™” ê¸°ë¡\n",
        "\n",
        "    Returns:\n",
        "    - ëª¨ë¸ì˜ ë‹µë³€\n",
        "    \"\"\"\n",
        "    # ì´ë¯¸ì§€ê°€ ê²½ë¡œì¸ ê²½ìš° ë¡œë“œ\n",
        "    if isinstance(image, str):\n",
        "        image = load_image(image)\n",
        "\n",
        "    # ë©”ì‹œì§€ êµ¬ì„±\n",
        "    messages = []\n",
        "\n",
        "    # ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶”ê°€\n",
        "    for user_q, bot_a in chat_history:\n",
        "        messages.append({\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"text\", \"text\": user_q}\n",
        "            ]\n",
        "        })\n",
        "        messages.append({\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"text\", \"text\": bot_a}\n",
        "            ]\n",
        "        })\n",
        "\n",
        "    # í˜„ì¬ ì§ˆë¬¸ ì¶”ê°€ (ì²« ì§ˆë¬¸ì¸ ê²½ìš° ì´ë¯¸ì§€ í¬í•¨)\n",
        "    if len(chat_history) == 0:\n",
        "        messages.append({\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image\", \"image\": image},\n",
        "                {\"type\": \"text\", \"text\": question}\n",
        "            ]\n",
        "        })\n",
        "    else:\n",
        "        messages.append({\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"text\", \"text\": question}\n",
        "            ]\n",
        "        })\n",
        "\n",
        "    # ì…ë ¥ í…ìŠ¤íŠ¸ ì¤€ë¹„\n",
        "    text = processor.apply_chat_template(\n",
        "        messages, tokenize=False, add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    # ì´ë¯¸ì§€ì™€ ë¹„ë””ì˜¤ ì •ë³´ ì²˜ë¦¬\n",
        "    image_inputs, video_inputs = process_vision_info(messages)\n",
        "\n",
        "    # ëª¨ë¸ ì…ë ¥ ì¤€ë¹„\n",
        "    inputs = processor(\n",
        "        text=[text],\n",
        "        images=image_inputs,\n",
        "        videos=video_inputs,\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    inputs = inputs.to(model.device)\n",
        "\n",
        "    # í…ìŠ¤íŠ¸ ìƒì„±\n",
        "    with torch.no_grad():\n",
        "        generated_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=512,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9\n",
        "        )\n",
        "\n",
        "    # ìƒì„±ëœ í† í°ì—ì„œ ì…ë ¥ ë¶€ë¶„ ì œê±°\n",
        "    generated_ids_trimmed = [\n",
        "        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
        "    ]\n",
        "\n",
        "    # í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”©\n",
        "    output_text = processor.batch_decode(\n",
        "        generated_ids_trimmed,\n",
        "        skip_special_tokens=True,\n",
        "        clean_up_tokenization_spaces=False\n",
        "    )[0]\n",
        "\n",
        "    return output_text\n",
        "\n",
        "# ==========================================\n",
        "# 4. ëŒ€í™”í˜• ìœ„ì ¯ ì¸í„°í˜ì´ìŠ¤\n",
        "# ==========================================\n",
        "class ImageChatbot:\n",
        "    def __init__(self):\n",
        "        self.image = None\n",
        "        self.chat_history = []\n",
        "        self.setup_widgets()\n",
        "\n",
        "    def setup_widgets(self):\n",
        "        \"\"\"ìœ„ì ¯ ì„¤ì •\"\"\"\n",
        "        # ì´ë¯¸ì§€ ì—…ë¡œë“œ ìœ„ì ¯\n",
        "        self.image_upload = widgets.FileUpload(\n",
        "            accept='image/*',\n",
        "            multiple=False,\n",
        "            description='ì´ë¯¸ì§€ ì—…ë¡œë“œ'\n",
        "        )\n",
        "        self.image_upload.observe(self.on_image_upload, names='value')\n",
        "\n",
        "        # ì´ë¯¸ì§€ URL ì…ë ¥\n",
        "        self.image_url = widgets.Text(\n",
        "            placeholder='ë˜ëŠ” ì´ë¯¸ì§€ URLì„ ì…ë ¥í•˜ì„¸ìš”',\n",
        "            description='ì´ë¯¸ì§€ URL:',\n",
        "            layout=widgets.Layout(width='80%')\n",
        "        )\n",
        "        self.url_button = widgets.Button(description='URL ë¡œë“œ')\n",
        "        self.url_button.on_click(self.on_url_load)\n",
        "\n",
        "        # ì§ˆë¬¸ ì…ë ¥\n",
        "        self.question_input = widgets.Text(\n",
        "            placeholder='ì´ë¯¸ì§€ì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš” (í•œêµ­ì–´)',\n",
        "            description='ì§ˆë¬¸:',\n",
        "            layout=widgets.Layout(width='80%')\n",
        "        )\n",
        "        self.question_input.continuous_update = False\n",
        "        self.question_input.observe(self.on_question_submit, names='value')\n",
        "\n",
        "        # ì¶œë ¥ ì˜ì—­\n",
        "        self.output = widgets.Output()\n",
        "\n",
        "        # ì´ˆê¸°í™” ë²„íŠ¼\n",
        "        self.reset_button = widgets.Button(\n",
        "            description='ëŒ€í™” ì´ˆê¸°í™”',\n",
        "            button_style='warning'\n",
        "        )\n",
        "        self.reset_button.on_click(self.on_reset)\n",
        "\n",
        "    def on_image_upload(self, change):\n",
        "        \"\"\"ì´ë¯¸ì§€ ì—…ë¡œë“œ ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬\"\"\"\n",
        "        if self.image_upload.value:\n",
        "            uploaded_file = list(self.image_upload.value.values())[0]\n",
        "            self.image = Image.open(BytesIO(uploaded_file['content'])).convert(\"RGB\")\n",
        "            self.chat_history = []  # ìƒˆ ì´ë¯¸ì§€ë©´ ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”\n",
        "\n",
        "            with self.output:\n",
        "                self.output.clear_output()\n",
        "                print(\"âœ… ì´ë¯¸ì§€ê°€ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
        "                display(self.image.resize((300, 300), Image.LANCZOS))\n",
        "\n",
        "    def on_url_load(self, button):\n",
        "        \"\"\"URL ë¡œë“œ ë²„íŠ¼ ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬\"\"\"\n",
        "        if self.image_url.value:\n",
        "            try:\n",
        "                self.image = load_image(self.image_url.value)\n",
        "                self.chat_history = []  # ìƒˆ ì´ë¯¸ì§€ë©´ ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”\n",
        "\n",
        "                with self.output:\n",
        "                    self.output.clear_output()\n",
        "                    print(\"âœ… ì´ë¯¸ì§€ê°€ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
        "                    display(self.image.resize((300, 300), Image.LANCZOS))\n",
        "            except Exception as e:\n",
        "                with self.output:\n",
        "                    print(f\"âŒ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
        "\n",
        "    def on_question_submit(self, change):\n",
        "        \"\"\"ì§ˆë¬¸ ì œì¶œ ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬\"\"\"\n",
        "        question = change['new']\n",
        "\n",
        "        if not question:\n",
        "            return\n",
        "\n",
        "        if self.image is None:\n",
        "            with self.output:\n",
        "                print(\"âš ï¸ ë¨¼ì € ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”!\")\n",
        "            self.question_input.value = \"\"\n",
        "            return\n",
        "\n",
        "        # ì§ˆë¬¸ ì²˜ë¦¬\n",
        "        with self.output:\n",
        "            display(HTML(f\"<b>ğŸ‘¤ ì‚¬ìš©ì:</b> {question}\"))\n",
        "            print(\"ğŸ¤” ë‹µë³€ ìƒì„± ì¤‘...\")\n",
        "\n",
        "        try:\n",
        "            # ë‹µë³€ ìƒì„±\n",
        "            answer = chat_with_image(self.image, question, self.chat_history)\n",
        "\n",
        "            # ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€\n",
        "            self.chat_history.append((question, answer))\n",
        "\n",
        "            # ê²°ê³¼ ì¶œë ¥\n",
        "            with self.output:\n",
        "                display(HTML(f\"<b>ğŸ¤– Qwen2-VL:</b> {answer}\"))\n",
        "                display(HTML(\"<br>\"))\n",
        "\n",
        "        except Exception as e:\n",
        "            with self.output:\n",
        "                print(f\"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
        "\n",
        "        # ì…ë ¥ì°½ ì´ˆê¸°í™”\n",
        "        self.question_input.value = \"\"\n",
        "\n",
        "    def on_reset(self, button):\n",
        "        \"\"\"ì´ˆê¸°í™” ë²„íŠ¼ ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬\"\"\"\n",
        "        self.chat_history = []\n",
        "        with self.output:\n",
        "            self.output.clear_output()\n",
        "            print(\"ğŸ”„ ëŒ€í™”ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
        "            if self.image:\n",
        "                display(self.image.resize((300, 300), Image.LANCZOS))\n",
        "\n",
        "    def display(self):\n",
        "        \"\"\"ìœ„ì ¯ í‘œì‹œ\"\"\"\n",
        "        url_box = widgets.HBox([self.image_url, self.url_button])\n",
        "\n",
        "        display(widgets.VBox([\n",
        "            widgets.HTML(\"<h2>ğŸ–¼ï¸ Qwen2-VL í•œêµ­ì–´ ì´ë¯¸ì§€ ì±„íŒ…ë´‡</h2>\"),\n",
        "            self.image_upload,\n",
        "            url_box,\n",
        "            widgets.HTML(\"<hr>\"),\n",
        "            self.question_input,\n",
        "            self.reset_button,\n",
        "            widgets.HTML(\"<hr>\"),\n",
        "            self.output\n",
        "        ]))\n",
        "\n",
        "# ==========================================\n",
        "# 5. ì±—ë´‡ ì‹¤í–‰\n",
        "# ==========================================\n",
        "chatbot = ImageChatbot()\n",
        "chatbot.display()"
      ],
      "metadata": {
        "id": "pDIkEdG9P-NJ"
      },
      "id": "pDIkEdG9P-NJ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}