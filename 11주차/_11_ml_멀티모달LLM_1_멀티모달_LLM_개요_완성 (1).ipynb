{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **ë©€í‹°ëª¨ë‹¬ LLM**"
      ],
      "metadata": {
        "id": "Hf71TfhJ3zW9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0mWB0gqO_43"
      },
      "source": [
        "---\n",
        "\n",
        "ğŸ’¡ **NOTE**: ì´ ë…¸íŠ¸ë¶ì˜ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë ¤ë©´ GPUë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. êµ¬ê¸€ ì½”ë©ì—ì„œëŠ” **ëŸ°íƒ€ì„ > ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½ > í•˜ë“œì›¨ì–´ ê°€ì†ê¸° > T4 GPU**ë¥¼ ì„ íƒí•˜ì„¸ìš”.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6N7d_LipEMFr"
      },
      "outputs": [],
      "source": [
        "# ê¹ƒí—ˆë¸Œì—ì„œ ìœ„ì ¯ ìƒíƒœ ì˜¤ë¥˜ë¥¼ í”¼í•˜ê¸° ìœ„í•´ ì§„í–‰ í‘œì‹œì¤„ì„ ë‚˜íƒ€ë‚´ì§€ ì•Šë„ë¡ ì„¤ì •í•©ë‹ˆë‹¤.\n",
        "import os\n",
        "import tqdm\n",
        "from transformers.utils import logging\n",
        "\n",
        "# tqdm ë¹„í™œì„±í™”\n",
        "os.environ[\"DISABLE_TQDM\"] = \"1\"\n",
        "\n",
        "logging.disable_progress_bar()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. ë©€í‹°ëª¨ë‹¬**"
      ],
      "metadata": {
        "id": "Vj1SHKs1W0U0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ë©€í‹°ëª¨ë‹¬ì´ë€?**\n",
        "\n",
        "- **ëª¨ë‹¬(Modal)** ì´ë€?\n",
        "    - **AIë‚˜ ì¸ì§€ê³¼í•™**ì—ì„œ â€œëª¨ë‹¬(modal)â€ì€ ì •ë³´ê°€ í‘œí˜„ë˜ê±°ë‚˜ ê°ì§€ë˜ëŠ” í˜•íƒœ/í˜•ì‹ì„ ì˜ë¯¸í•¨ - **ì •ë³´ì˜ ì¢…ë¥˜**/ **ë°ì´í„°ì˜ í˜•íƒœ**\n",
        "    - ì¦‰, **ì¸ê°„**ì´ ì„¸ìƒì„ ì¸ì‹í•  ë•Œ ì‚¬ìš©í•˜ëŠ” ê°ê° ì±„ë„(sensory modality) ê³¼ ê°™ì€ ê°œë… - **ì‹œê°(ëˆˆ), ì²­ê°(ê·€), ì´‰ê°(í”¼ë¶€) ë“±**\n",
        "\n",
        "- **ë©€í‹°ëª¨ë‹¬(Multimodal)** ì´ë€\n",
        "    - <mark>**AIê°€ ì—¬ëŸ¬ í˜•íƒœ(Modalities)ì˜ ì •ë³´ë¥¼ ë™ì‹œì— ì´í•´í•˜ê³  ì²˜ë¦¬í•˜ëŠ” ê¸°ìˆ **</mark>ì„ ì˜ë¯¸\n",
        "    - ì¦‰, í…ìŠ¤íŠ¸(text), ì´ë¯¸ì§€(image), ìŒì„±(audio), ì˜ìƒ(video), ì„¼ì„œ ë°ì´í„°(sensor data) ë“±ì„ í•œ ë²ˆì— ë‹¤ë£¨ì–´ ì„œë¡œ ë‹¤ë¥¸ í˜•íƒœì˜ ë°ì´í„°ë¥¼ í†µí•©ì ìœ¼ë¡œ ì¸ì‹í•˜ê³  ì¶”ë¡ í•  ìˆ˜ ìˆê²Œ í•˜ëŠ” ê²ƒ\n",
        "    \n"
      ],
      "metadata": {
        "id": "hglauUrDSl_l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ë°ì´í„° ëª¨ë‹¬ë¦¬í‹° ìœ í˜•**\n",
        "\n",
        "- **ë°ì´í„° ëª¨ë‹¬ë¦¬í‹° ìœ í˜•**\n",
        "|ëª¨ë‹¬ë¦¬í‹°|ì„¤ëª…|ì˜ˆì‹œ ì‘ìš©|\n",
        "|---|---|---|\n",
        "|**í…ìŠ¤íŠ¸(Text)**|ìì—°ì–´ ë¬¸ì¥, ë¬¸ì„œ, ì½”ë“œ ë“±|ë¬¸ì„œ ë¶„ì„, ì±—ë´‡, ë²ˆì—­|\n",
        "|**ì´ë¯¸ì§€(Image)**|ì‚¬ì§„, ê·¸ë¦¼, ì˜ë£Œ ì˜ìƒ ë“±|ê°ì²´ ì¸ì‹, ì˜ë£Œ ì§„ë‹¨, ì–¼êµ´ ì¸ì‹|\n",
        "|**ì˜¤ë””ì˜¤(Audio)**|ìŒì„±, ìŒì•…, í™˜ê²½ ì†ŒìŒ ë“±|ìŒì„± ì¸ì‹, ê°ì • ë¶„ì„, ìŒì•… ìƒì„±|\n",
        "|**ë¹„ë””ì˜¤(Video)**|ë™ì˜ìƒ, ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°|í–‰ë™ ì¸ì‹, ë¹„ë””ì˜¤ ìš”ì•½, ê°ì‹œ ì‹œìŠ¤í…œ|\n",
        "|**ì„¼ì„œ ë°ì´í„°**|GPS, LIDAR, IMU, ì˜¨ë„ ë“±|ììœ¨ì£¼í–‰, ë¡œë´‡ ì œì–´, IoT ì‹œìŠ¤í…œ|\n",
        "|**ê¹Šì´(Depth)**|3D ì •ë³´, ê±°ë¦¬ ì¸¡ì •|AR/VR, 3D ì¬êµ¬ì„±, ê³µê°„ ì¸ì‹|\n",
        "\n",
        "\n",
        "- **ëª¨ë‹¬ì˜ êµ¬ì„±**\n",
        "|êµ¬ë¶„|\të‚´ìš©|\tì˜ˆì‹œ|\n",
        "|---|---|---|\n",
        "|**ë‹¨ì¼ëª¨ë‹¬ (Unimodal)**|\tí•˜ë‚˜ì˜ ë°ì´í„° í˜•íƒœë§Œ ì²˜ë¦¬|\tGPT-2 â†’ í…ìŠ¤íŠ¸ë§Œ ì…ë ¥/ì¶œë ¥|\n",
        "|**ë©€í‹°ëª¨ë‹¬ (Multimodal)**|\tì—¬ëŸ¬ í˜•íƒœì˜ ë°ì´í„°ë¥¼ ë™ì‹œì— ì²˜ë¦¬|\tGPT-4V â†’ í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€ ì´í•´ ê°€ëŠ¥|\n",
        "|**í¬ë¡œìŠ¤ëª¨ë‹¬ (Cross-modal)**|\tí•œ í˜•íƒœì˜ ë°ì´í„°ë¥¼ ë‹¤ë¥¸ í˜•íƒœë¡œ ë³€í™˜|\tí…ìŠ¤íŠ¸ ì„¤ëª… â†’ ì´ë¯¸ì§€ ìƒì„± (ì˜ˆ: DALLÂ·E, Midjourney)|\n"
      ],
      "metadata": {
        "id": "pEeepF8ZXjAn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ë©€í‹°ëª¨ë‹¬ í•™ìŠµì˜ ì£¼ìš” ê³¼ì œ**\n",
        "\n",
        "|ê³¼ì œ|ì„¤ëª…|í•´ê²° ë°©ë²•|\n",
        "|---|---|---|\n",
        "|**í‘œí˜„(Representation)**|ì„œë¡œ ë‹¤ë¥¸ í˜•ì‹ì˜ ë°ì´í„°ë¥¼ ê³µí†µ ê³µê°„ì— í‘œí˜„|Joint Embedding Space, Shared Representations|\n",
        "|**ì •ë ¬(Alignment)**|ì„œë¡œ ë‹¤ë¥¸ ëª¨ë‹¬ë¦¬í‹° ê°„ì˜ ëŒ€ì‘ ê´€ê³„ ì°¾ê¸°|Cross-Modal Attention, Contrastive Learning|\n",
        "|**ìœµí•©(Fusion)** |ì—¬ëŸ¬ ëª¨ë‹¬ë¦¬í‹°ì˜ ì •ë³´ë¥¼ ê²°í•©|Early Fusion, Late Fusion, Hybrid Fusion|\n",
        "|**ë³€í™˜(Translation)**|í•œ ëª¨ë‹¬ë¦¬í‹°ì—ì„œ ë‹¤ë¥¸ ëª¨ë‹¬ë¦¬í‹°ë¡œ ë³€í™˜|Image Captioning, Text-to-Image Generation|\n",
        "|**ê³µë™í•™ìŠµ(Co-learning)**|í•œ ëª¨ë‹¬ë¦¬í‹°ì˜ ì§€ì‹ì„ ë‹¤ë¥¸ ëª¨ë‹¬ë¦¬í‹° í•™ìŠµì— í™œìš©|Transfer Learning, Knowledge Distillation|\n"
      ],
      "metadata": {
        "id": "lapExk5IZdBf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ë©€í‹°ëª¨ë‹¬ì˜ ë°œì „**\n",
        "- ì¸ê°„ì˜ ì¸ì§€ëŠ¥ë ¥ì€ ì‹œê°, ì²­ê°, ì–¸ì–´ë¥¼ í•¨ê»˜ ì‚¬ìš©í•¨.\n",
        "- AIë„ ì´ë¥¼ ëª¨ë°©í•˜ë ¤ë©´ â€œë©€í‹°ëª¨ë‹¬ í†µí•©â€ì´ í•„ìš”í–ˆìŒ\n",
        "\n",
        "|ì‹œëŒ€|\tì£¼ìš” ë°œì „|\tì„¤ëª…|\n",
        "|---|---|---|\n",
        "|**2012~2014**|\t**CNN** ê¸°ë°˜ ì´ë¯¸ì§€ ì¸ì‹ ë°œì „|\tì´ë¯¸ì§€ ë¶„ë¥˜ ì •í™•ë„ ê¸‰ìƒìŠ¹ (AlexNet, ResNet)|\n",
        "|**2014~2015**|ì´ˆê¸° ë©€í‹°ëª¨ë‹¬ ì—°êµ¬|**Image Captioning**(ì´ë¯¸ì§€ ì„¤ëª… ìƒì„±) ì—°êµ¬ê°€ í™œë°œí•´ì¡ŒìŠµë‹ˆë‹¤. Show and Tell(2015), Show, Attend and Tell(2016) ë“±ì˜ ëª¨ë¸ì´ ë“±ì¥|\n",
        "|**2017~2019**|\t**Transformer**ì™€ **NLP í˜ëª…**|\tBERT, GPT ë“±ìœ¼ë¡œ ì–¸ì–´ ì´í•´ ì„±ëŠ¥ í–¥ìƒ|\n",
        "|**2019~2021**|\t**Vision-Language ëª¨ë¸** ë°œì „|\t**CLIP**(2021), DALL-E(2021) ë“± ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ ì—°ê²°í•˜ëŠ” ëª¨ë¸ì´ ë“±ì¥í•¨. **CLIPì€ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ìŒì„ ëŒ€ì¡° í•™ìŠµ(Contrastive Learning)ìœ¼ë¡œ í•™ìŠµí–ˆìŒ**\n",
        "|**2022~2023**|**ìƒì„±í˜• ë©€í‹°ëª¨ë‹¬ AI**|GPT-4(2023)ê°€ ë©€í‹°ëª¨ë‹¬ ê¸°ëŠ¥ì„ ê°–ì¶”ë©° **ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ í•¨ê»˜ ì²˜ë¦¬í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆìŒ**. Stable Diffusion, Midjourney ë“± ì´ë¯¸ì§€ ìƒì„± ëª¨ë¸ë„ ëŒ€ì¤‘í™”ë˜ì—ˆìŒ|\n",
        "|**2024~**|í†µí•© ë©€í‹°ëª¨ë‹¬ ì‹œìŠ¤í…œ|Gemini 2.5, Claude 3.7, GPT-4o, Grok-4 ë“± **í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ì˜¤ë””ì˜¤, ë¹„ë””ì˜¤ë¥¼ ëª¨ë‘ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” í†µí•© ëª¨ë¸ë“¤ì´ ë“±ì¥**.2024ë…„ ë©€í‹°ëª¨ë‹¬ AI ì‹œì¥ ê·œëª¨ëŠ” 16ì–µ ë‹¬ëŸ¬ë¡œ í‰ê°€ë¨. ì´í›„ ê³„ì† ì„±ì¥ ì¤‘ |"
      ],
      "metadata": {
        "id": "9MNhy-HaUYJy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "4iv75O6UauYD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. ë¹„ì „ íŠ¸ëœìŠ¤í¬ë¨¸(ViT)**\n",
        "\n",
        "- **íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ëª¨ë¸ì˜ ì„±ê³µ**\n",
        "    - ë¶„ë¥˜ì™€ í´ëŸ¬ìŠ¤í„°ë§ë¶€í„° ê²€ìƒ‰ê³¼ ìƒì„± ëª¨ë¸ë§ì— ì´ë¥´ê¸°ê¹Œì§€ ë‹¤ì–‘í•œ ì–¸ì–´ ëª¨ë¸ë§ ì‘ì—…ì— ì„±ê³µì ìœ¼ë¡œ ì‚¬ìš©í•¨\n",
        "- **ë¹„ì „ íŠ¸ëœìŠ¤í¬ë¨¸**(**Vision Transformer, ViT**)\n",
        "    - íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì„±ê³µì„ ì»´í“¨í„° ë¹„ì „ ë¶„ì•¼ë¡œ ì¼ë°˜í™”í•˜ëŠ” ë°©ë²•ì„ ì°¾ìœ¼ëŸ¬ëŠ” ë…¸ë ¥(ë°©ë²•)\n",
        "    - ì¢…ì „ì˜ í•©ì„±ê³± ì‹ ê²½ë§(CNN)ì— ë¹„í•´ ì´ë¯¸ì§€ ì¸ì‹ ì‘ì—…ì—ì„œ ë§¤ìš° ì˜ ë™ì‘í•œë‹¤ëŠ” ê²ƒì´ ì…ì¦ë¨(An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, 2020.10,  https://arxiv.org/abs/2010.11929)\n",
        "    - **ViT**ëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ êµ¬ì¡°ì˜ í•µì‹¬ êµ¬ì„± ìš”ì†Œì¸ **ì¸ì½”ë”ì— ì˜ì¡´**í•¨\n",
        "    - **ViT**ëŠ” <mark>**ì´ë¯¸ì§€ë¥¼ ë‹¨ì–´ë¡œ í† í°í™” í•˜ëŠ” ë°©ë²•ì„ ë§Œë“¤ì—ˆìŒ**</mark>\n",
        "        - í…ìŠ¤íŠ¸ë¥¼ í† í°ìœ¼ë¡œ ë¶„í• í•˜ëŠ” ê²ƒì²˜ëŸ¼ ì›ë³¸ ì´ë¯¸ì§€ë¥¼ ì´ë¯¸ì§€ íŒ¨ì¹˜(í”½ì…€ì˜ íŒ¨ì¹˜ patch)ë¡œ ë³€í™˜í•¨\n",
        "        - ì´ë¯¸ì§€ì˜ ê° íŒ¨ì¹˜ëŠ” í† í°ê³¼ ë‹¬ë¦¬ ê³ ìœ í•œ IDë¥¼ í• ë‹¹í•  ìˆ˜ ì—†ìŒ(í…ìŠ¤íŠ¸ ì–´íœ˜ì‚¬ì „ì— ìˆëŠ” í† í°ê³¼ ë‹¬ë¦¬ ë‹¤ë¥¸ ì´ë¯¸ì§€ì— ë‚˜íƒ€ë‚˜ëŠ” ê²½ìš°ê°€ ê±°ì˜ ì—†ê¸°  ë•Œë¬¸)\n",
        "        - ê·¸ ëŒ€ì‹  **íŒ¨ì¹˜ë¥¼ ì„ í˜•ì ìœ¼ë¡œ ì„ë² ë”©í•˜ì—¬ ìˆ˜ì¹˜ í‘œí˜„ ì„ë² ë”©ì„ ë§Œë“ ë‹¤**.--> ì´ ì„ë² ë”©ì´ ì¸ì½”ë”ë¡œ ì „ë‹¬ë˜ë©´ í…ìŠ¤íŠ¸ í† í°ì²˜ëŸ¼ ì²˜ë¦¬ë˜ê¸° ë•Œë¬¸ì— **í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ë¥¼ í›ˆë ¨í•˜ëŠ” ë°©ì‹ì— ì°¨ì´ê°€ ì—†ìŒ**\n"
      ],
      "metadata": {
        "id": "IT1m1x5se9Xf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **ViT í•µì‹¬ ì•Œê³ ë¦¬ì¦˜**\n",
        "<div style=\"display: flex; justify-content: space-around;\">\n",
        "  \n",
        "  <img src=\"https://drive.google.com/uc?export=view&id=1W5eof7cmEJqqyeypnUOrski9TUwFIj31\" width=\"48%\" title=\"ì´ë¯¸ì§€: í•¸ì¦ˆì˜¨ LLM 1\">\n",
        "\n",
        "  <img src=\"https://drive.google.com/uc?export=view&id=1bDgXU5MQeMX95fnl5t4v_vFblg_Ezmmo\" width=\"48%\" title=\"ì´ë¯¸ì§€: í•¸ì¦ˆì˜¨ LLM\">\n",
        "\n",
        "</div>\n",
        "\n"
      ],
      "metadata": {
        "id": "mrKvej21kqBX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### âœ… **ì˜ˆì œ : ViT ëª¨ë¸ì„ ì´ìš©í•˜ì—¬ ì´ë¯¸ì§€ ë¶„ë¥˜í•˜ê¸°**"
      ],
      "metadata": {
        "id": "oTFWCXKi_IhK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ViTImageProcessor, ViTForImageClassification\n",
        "from PIL import Image\n",
        "import requests\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ëª¨ë¸ê³¼ ì „ì²˜ë¦¬ê¸° ë¡œë“œ (ViTImageProcessor ì‚¬ìš©)\n",
        "processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
        "model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224\")\n",
        "\n",
        "\n",
        "# ì´ë¯¸ì§€ ë¡œë“œ\n",
        "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/cats.png\"\n",
        "# url = \"https://img.sbs.co.kr/newsnet/etv/upload/2022/10/07/30000795252_1280.jpg\"\n",
        "# url = \"https://images.pexels.com/photos/34663263/pexels-photo-34663263.jpeg\"\n",
        "\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "# image = Image.open('dog.jpg')  # ì´ë¯¸ì§€ íŒŒì¼ ì§ì ‘ ì‚¬ìš©\n",
        "\n",
        "# ì´ë¯¸ì§€ë¥¼ RGB ëª¨ë“œë¡œ ëª…ì‹œì  ë³€í™˜ (ì¤‘ìš”!)\n",
        "image = image.convert(\"RGB\")\n",
        "plt.imshow(image)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# ì´ë¯¸ì§€ ì „ì²˜ë¦¬\n",
        "inputs = processor(images=image, return_tensors=\"pt\")\n",
        "\n",
        "# ì¶”ë¡ \n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    predicted_class = logits.argmax(-1).item()\n",
        "\n",
        "    # í™•ë¥ ê°’ë„ í•¨ê»˜ ì¶œë ¥\n",
        "    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
        "    confidence = probabilities[0][predicted_class].item()\n",
        "\n",
        "print(f\"ì˜ˆì¸¡ëœ í´ë˜ìŠ¤: {model.config.id2label[predicted_class]}\")\n",
        "print(f\"í™•ì‹ ë„: {confidence:.2%}\")\n"
      ],
      "metadata": {
        "id": "fuRim01Q_Ipf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **ì´ë¯¸ì§€ íŒŒì¼ ì½ì–´ì„œ ë¶„ë¥˜í•˜ê¸°**"
      ],
      "metadata": {
        "id": "YsngiF5ldReU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ì´ë¯¸ì§€ ë¡œë“œ\n",
        "file = 'dog.jpg'\n",
        "image = Image.open(file)\n",
        "\n",
        "# ì´ë¯¸ì§€ë¥¼ RGB ëª¨ë“œë¡œ ëª…ì‹œì  ë³€í™˜ (ì¤‘ìš”!)\n",
        "image = image.convert(\"RGB\")\n",
        "plt.imshow(image)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# ì´ë¯¸ì§€ ì „ì²˜ë¦¬\n",
        "inputs = processor(images=image, return_tensors=\"pt\")\n",
        "\n",
        "# ì¶”ë¡ \n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    predicted_class = logits.argmax(-1).item()\n",
        "\n",
        "    # í™•ë¥ ê°’ë„ í•¨ê»˜ ì¶œë ¥\n",
        "    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
        "    confidence = probabilities[0][predicted_class].item()\n",
        "\n",
        "print(f\"ì˜ˆì¸¡ëœ í´ë˜ìŠ¤: {model.config.id2label[predicted_class]}\")\n",
        "print(f\"í™•ì‹ ë„: {confidence:.2%}\")\n"
      ],
      "metadata": {
        "id": "rxkbWO6qCN3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### âœ… **ì‘ìš© : ViT ë¡œ ì´ë¯¸ì§€ ìë™ íƒœê¹…í•˜ê¸°**\n",
        "\n",
        "- ì½”ë“œíŒŒì¼ :  _11_ml_ë©€í‹°ëª¨ë‹¬LLM_2_ViTë¡œ ì´ë¯¸ì§€ ìë™ íƒœê¹…í•˜ê¸°_ì™„ì„±.ipynb\n"
      ],
      "metadata": {
        "id": "jRXZXA5SdlD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "N_nXGynse9u5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. ë©€í‹°ëª¨ë‹¬ ì„ë² ë”© ëª¨ë¸**"
      ],
      "metadata": {
        "id": "bsv2voY-le0K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **ì„ë² ë”©**\n",
        "    - ì„ë² ë”© ëª¨ë¸ì€ ì„ë² ë”©(ë˜ëŠ” ìˆ˜ì¹˜ í‘œí˜„)ì„ ì‚¬ìš©í•´ ë¹„ìŠ·í•œ ë¬¸ì„œë¥¼ ì°¾ê³ , ë¶„ë¥˜ ì‘ì—…ì„ ì ìš©í•˜ê³ , í† í”½ ëª¨ë¸ì¼ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆì—ˆìŒ\n",
        "    - ì„ë² ë”©ì€ LLM ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬ì„±í•˜ëŠ”ë° í•µì‹¬ ì—­í• ì„ ìˆ˜í–‰í•¨\n",
        "\n",
        "- **ë©€í‹°ëª¨ë‹¬ ì„ë² ë”© ëª¨ë¸**\n",
        "    - **í…ìŠ¤íŠ¸ì™€ ì‹œê° í‘œí˜„ì„ ëª¨ë‘ í¬ì°©í•  ìˆ˜ ìˆëŠ” ì„ë² ë”© ëª¨ë¸**\n",
        "    - ë§Œë“¤ì–´ì§„ ì„ë² ë”©ì´ ë™ì¼í•œ ë²¡í„° ê³µê°„ì— ë†“ì´ê¸° ë•Œë¬¸ì— ë©€í‹°ëª¨ë‹¬ í‘œí˜„ì„ ë¹„êµí•  ìˆ˜ ìˆë‹¤.\n",
        "    - ex: 'pictures of a puppy' í…ìŠ¤íŠ¸ì™€ ê°€ì¥ ë¹„ìŠ·í•œ ì´ë¯¸ì§€ëŠ”? --> **ì§ˆë¬¸ê³¼ ê´€ë ¨ì„±ì´ ê°€ì¥ ë†’ì€ ë¬¸ì„œë¥¼ ì°¾ëŠ” ì‘ì—…ê³¼ ë¹„ìŠ·**í•˜ë‹¤.\n",
        "    - **CLIP(Contrastive Language-Image Pre-training)**\n",
        "        - ë§ì€ ë©€í‹°ëª¨ë‹¬ ì„ë² ë”© ëª¨ë¸ì¤‘ ê°€ì¥ ìœ ëª…í•˜ê³  ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ëª¨ë¸\n"
      ],
      "metadata": {
        "id": "spUHwXpmlw0f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **ë©€í‹°ëª¨ë‹¬ ì„ë² ë”© ë²¡í„° ê³µê°„**\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1dlQtZ-ZblB2ZYYaylEkJ1jOIqGWp9Z8O\" width=\"60%\" title=\"ì´ë¯¸ì§€: í•¸ì¦ˆì˜¨ LLM\">\n"
      ],
      "metadata": {
        "id": "WHkXTS-Jn3j9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **CLIP ê¸°ë°˜ ëª¨ë¸**\n",
        "\n",
        "- **CLIP**(Contrastive Language-Image Pre-training, ëŒ€ì¡°í•™ìŠµ ì–¸ì–´-ì´ë¯¸ì§€ ì‚¬ì „ í›ˆë ¨ ëª¨ë¸)ì€ **ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ì˜ ì„ë² ë”©ì„ ëª¨ë‘ ê³„ì‚°í•  ìˆ˜ ìˆëŠ” ì„ë² ë”© ëª¨ë¸ì„**\n",
        "- **ë§Œë“¤ì–´ì§„ ì„ë² ë”©ì€ ë™ì¼í•œ ë²¡í„° ê³µê°„ì— ë†“ì„** --> ì´ë¯¸ì§€ ì„ë² ë”©ê³¼ í…ŒìŠ¤íŠ¸ ì„ë² ë”©ì„ ë¹„êµí•  ìˆ˜ ìˆë‹¤.\n",
        "\n",
        "- **CLIP ëª¨ë¸ì´ ìœ ìš©í•œ ì‘ì—…**\n",
        "    - <mark>**ì œë¡œìƒ· ë¶„ë¥˜**</mark> : ì œë¡œìƒ· ì´ë¯¸ì§€ ë¶„ë¥˜ëŠ” **ëª¨ë¸ì„ í›ˆë ¨í•˜ëŠ” ë™ì•ˆ ì´ì „ì— ë³¸ ì ì´ ì—†ëŠ” í´ë˜ìŠ¤ë¥¼ ë¶„ë¥˜í•˜ëŠ” ì‘ì—…**,\n",
        "    - ì´ë¯¸ì§€ ì„ë² ë”©ì„ í›„ë³´ í´ë˜ìŠ¤ ì„¤ëª…ì˜ ì„ë² ë”©ê³¼ ë¹„êµí•˜ì—¬ ê°€ì¥ ë¹„ìŠ·í•œ í´ë˜ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ìˆìŒ, https://huggingface.co/tasks/zero-shot-image-classification\n",
        "    - **í´ëŸ¬ìŠ¤í„°ë§** : í‚¤ì›Œë“œê°€ ì–´ë–¤ ì´ë¯¸ì§€ ì§‘í•©ì— ì†í•˜ëŠ”ì§€ ì°¾ê¸° ìœ„í•´ ì´ë¯¸ì§€ì™€ í‚¤ì›Œë“œ ì§‘í•©ì„ í´ëŸ¬ìŠ¤í„°ë¡œ ëª¨ìŒ\n",
        "    - **ê²€ìƒ‰** : ìˆ˜ì‹­ì–µ ê°œì˜ í…ìŠ¤íŠ¸ ë˜ëŠ” ì´ë¯¸ì§€ì—ì„œ ì…ë ¥ í…ìŠ¤íŠ¸ë‚˜ ì´ë¯¸ì§€ì— ê´€ë ¨ëœ ê²°ê³¼ë¥¼ ë¹ ë¥´ê²Œ ì°¾ì„ ìˆ˜ ìˆë‹¤.\n",
        "    - **ìƒì„±** : ì´ë¯¸ì§€ ìƒì„±(ex: ìŠ¤í…Œì´ë¸” ë””í“¨ì „)ì— ë©€í‹°ëª¨ë‹¬ ì„ë² ë”©ì„ ì‚¬ìš©í•¨"
      ],
      "metadata": {
        "id": "85RwwICenxqp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **CLIPì´ ë©€í‹°ëª¨ë‹¬ ì„ë² ë”©ì„ ìƒì„±í•˜ëŠ” ë°©ë²•**\n",
        "\n",
        "- ex: ìˆ˜ë°±ë§Œ ê°œì˜ **ì´ë¯¸ì§€ì™€ ìº¡ì…˜ìœ¼ë¡œ êµ¬ì„±ëœ ë°ì´í„°ì…‹ ì¤€ë¹„**\n",
        "1. **ì´ë¯¸ì§€ ì¸ì½”ë”ì™€ í…ìŠ¤íŠ¸ ì¸ì½”ë”ë¥¼ ì‚¬ìš©í•´ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ ê°ê° ì„ë² ë”©í•¨** -> ì„ë² ë”© ìƒì„±\n",
        "2. **ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ì‚¬ìš©í•´ ë¬¸ì¥ ì„ë² ë”©ê³¼ ì´ë¯¸ì§€ ì„ë² ë”© ì‚¬ì´ì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°**\n",
        "    - í›ˆë ¨í•˜ëŠ” ë™ì•ˆ ë‘ ì„ë² ë”© ì‚¬ì´ì˜ ìœ ì‚¬ë„ë¥¼ ìµœì í™”í•¨.\n",
        "    - ë¹„ìŠ·í•œ ì´ë¯¸ì§€/ìº¡ì…˜ ìŒì—ì„œëŠ” ìœ ì‚¬ë„ë¥¼ ìµœëŒ€í™”í•˜ê³  ë¹„ìŠ·í•˜ì§€ ì•Šì€ ì´ë¯¸ì§€/ìº¡ì…˜ ìŒì—ì„œëŠ” ìµœì†Œí™”í•¨\n",
        "3. **ê¸°ëŒ€í•˜ëŠ” ìœ ì‚¬ë„ì™€ ì¼ì¹˜í•˜ë„ë¡ í…ìŠ¤íŠ¸ ì¸ì½”ë”ì™€ ì´ë¯¸ì§€ ì¸ì½”ë”ê°€ ì—…ë°ì´íŠ¸ë¨**. ì…ë ¥ì´ ë¹„ìŠ·í•˜ë‹¤ë©´ ë²¡í„° ê³µê°„ì—ì„œ ê°€ê¹Œì´ ìœ„ì¹˜í•˜ë„ë¡ ì„ë² ë”©ì„ ì—…ë°ì´íŠ¸í•˜ê²Œ ë¨\n",
        "    - ì´ ê³¼ì •ì„ **ëŒ€ì¡°í•™ìŠµ(contrastrive learning**)ì´ë¼ê³  ë¶€ë¥¸ë‹¤.\n",
        "    - ê°€ëŠ¥í•œ í•œ ì •í™•í•œ í‘œí˜„ì„ ë§Œë“¤ê¸° ìœ„í•´ **ê´€ë ¨ ì—†ëŠ” ì´ë¯¸ì§€ì™€ ìº¡ì…˜ì„ ì‚¬ìš©í•˜ëŠ” ë„¤ì»¤í‹°ë¸Œ ìƒ˜í”Œì´ í›ˆë ¨ ê³¼ì •ì— í¬í•¨ë¨**\n",
        "    - ìœ ì‚¬ë„ ëª¨ë¸ì¼ì„ í†µí•´ **ë¬´ì—‡ì´ ì„œë¡œ ë¹„ìŠ·í•˜ê²Œ ë§Œë“œëŠ”ì§€ë¿ë§Œ ì•„ë‹ˆë¼ ë¬´ì—‡ì´ ì„œë¡œ ë‹¤ë¥´ê²Œ ë§Œë“œëŠ”ì§€ë„ ë°°ì›€**"
      ],
      "metadata": {
        "id": "n0bth1SUqiHM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **ë©€í‹°ëª¨ë‹¬ ì„ë² ë”© ëª¨ë¸ í›ˆë ¨ì„ ìœ„í•œ ë°ì´í„° ìœ í˜•**\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1MXRPynisuDNVzogq6rCBchZVhjWieLnb\" width=\"60%\" title=\"ì´ë¯¸ì§€: í•¸ì¦ˆì˜¨ LLM\">\n",
        "\n"
      ],
      "metadata": {
        "id": "6Em_NENUAHKR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **CLIPì´ ë©€í‹°ëª¨ë‹¬ ì„ë² ë”©ì„ ìƒì„±í•˜ëŠ” ë°©ë²•**\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1UYqqnJbV9WhT_aJqVvLvb9_WbwvqzH8p\" width=\"65%\" title=\"ì´ë¯¸ì§€: í•¸ì¦ˆì˜¨ LLM\">\n"
      ],
      "metadata": {
        "id": "igRPxvC4uEEX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMF9p8qK58Ou"
      },
      "source": [
        "#### âœ… **ì˜ˆì œ : OpenCLIP ëª¨ë¸ë¡œ ì´ë¯¸ì§€ì™€ ìº¡ì…˜ ìœ ì‚¬ë„ í™•ì¸í•˜ëŠ” ê³¼ì • ì‚´í´ë³´ê¸°**\n",
        "\n",
        "1. **ë°ì´í„° ì¤€ë¹„**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHUY1IBzZkgy"
      },
      "outputs": [],
      "source": [
        "from urllib.request import urlopen\n",
        "from PIL import Image\n",
        "\n",
        "# (ìŠ¤í…Œì´ë¸” ë””í“¨ì „)AIë¡œ ìƒì„±í•œ ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n",
        "puppy_path = \"https://bit.ly/4jYqmPu\"   # ê°•ì•„ì§€ ì´ë¯¸ì§€ path\n",
        "image = Image.open(urlopen(puppy_path)).convert(\"RGB\")\n",
        "caption = \"a puppy playing in the snow\"\n",
        "image.size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHOmvsRFZp4B"
      },
      "outputs": [],
      "source": [
        "image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpAwAij0at4o"
      },
      "source": [
        "2. **OpenCLIP ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€& í…ìŠ¤íŠ¸ ì„ë² ë”©í•˜ê¸°**\n",
        "    - CLIPì˜ ì˜¤í”ˆ ì†ŒìŠ¤ ë²„ì „ ëª¨ë¸ ì‚¬ìš©\n",
        "    - **OpenCLIP ëª¨ë¸**: **openai/clip-vit-base-patch32**   \n",
        "        - **base**: ëª¨ë¸ì˜ í¬ê¸°, ë³´í†µ 'Small', 'Base', 'Large' ë“±ìœ¼ë¡œ êµ¬ë¶„\n",
        "        - **patch32** : ì´ë¯¸ì§€ë¥¼ 32x32 í”½ì…€ í¬ê¸°ì˜ 'íŒ¨ì¹˜(Patch)' ì¡°ê°ë“¤ë¡œ ë‚˜ëˆ„ì–´ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©\n",
        "        - ì´ ëª¨ë¸ì€ **ì œë¡œìƒ· ì´ë¯¸ì§€ ë¶„ë¥˜(Zero-Shot Image Classification**)ì— ê°€ì¥ ë„ë¦¬ ì‚¬ìš©ë¨\n",
        "        - ëª¨ë¸ì—ê²Œ íŠ¹ì • ì´ë¯¸ì§€ í•œ ì¥ê³¼ [\"ê°•ì•„ì§€ ì‚¬ì§„\", \"ê³ ì–‘ì´ ì‚¬ì§„\", \"ìë™ì°¨ ì‚¬ì§„\"] ê°™ì€ í…ìŠ¤íŠ¸ í›„ë³´ë“¤ì„ ì£¼ë©´, ëª¨ë¸ì´ í•´ë‹¹ ì´ë¯¸ì§€ë¥¼ í•™ìŠµí•œ ì ì´ ì—†ì–´ë„ ì–´ë–¤ í…ìŠ¤íŠ¸ ì„¤ëª…ê³¼ ê°€ì¥ ìœ ì‚¬í•œì§€ ì•Œë ¤ì¤„ ìˆ˜ ìˆìŒ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLaZrGSxavbk"
      },
      "outputs": [],
      "source": [
        "from transformers import CLIPTokenizerFast, CLIPProcessor, CLIPModel\n",
        "\n",
        "model_id = \"openai/clip-vit-base-patch32\"\n",
        "\n",
        "# í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ë¥¼ ìœ„í•œ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n",
        "clip_tokenizer = CLIPTokenizerFast.from_pretrained(model_id)\n",
        "\n",
        "# ì´ë¯¸ì§€ ì „ì²˜ë¦¬ë¥¼ ìœ„í•œ ì „ì²˜ë¦¬ê¸°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n",
        "clip_processor = CLIPProcessor.from_pretrained(model_id)\n",
        "\n",
        "# í…ìŠ¤íŠ¸ ì„ë² ë”©ê³¼ ì´ë¯¸ì§€ ì„ë² ë”©ì„ ìƒì„±í•˜ê¸° ìœ„í•œ ë©”ì¸ ëª¨ë¸\n",
        "model = CLIPModel.from_pretrained(model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSx-X0-ecBv_"
      },
      "outputs": [],
      "source": [
        "# ì…ë ¥ì„ í† í°ìœ¼ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.\n",
        "inputs = clip_tokenizer(caption, return_tensors=\"pt\")\n",
        "inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVyHP2YwdUqB"
      },
      "outputs": [],
      "source": [
        "# ì…ë ¥ ì•„ì´ë””ë¥¼ í† í°ìœ¼ë¡œ ë˜ëŒë¦½ë‹ˆë‹¤.\n",
        "clip_tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEj4b22qeCm8"
      },
      "outputs": [],
      "source": [
        "# í…ìŠ¤íŠ¸ ì„ë² ë”©ì„ ë§Œë“­ë‹ˆë‹¤.\n",
        "text_embedding = model.get_text_features(**inputs)\n",
        "text_embedding.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwNQpueperGn"
      },
      "outputs": [],
      "source": [
        "# ì´ë¯¸ì§€ë¥¼ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
        "processed_image = clip_processor(\n",
        "    text=None, images=image, return_tensors='pt'\n",
        ")['pixel_values']\n",
        "\n",
        "# ëª¨ë¸ ì…ë ¥ì€ ë³´í†µ [Batch, Channel, Height, Width]\n",
        "# (e.g., [1, 3, 224, 224]) í˜•íƒœ\n",
        "processed_image.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4P9I7JsGf7Km"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ì‹œê°í™”ë¥¼ ì´í•´ ì´ë¯¸ì§€ë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤.\n",
        "img = processed_image.squeeze(0)    # ë§¨ ì•ì˜ 'Batch' ì°¨ì›(í¬ê¸°ê°€ 1)ì„ ì œê±°\n",
        "print(img.shape)\n",
        "img = img.permute(*torch.arange(img.ndim - 1, -1, -1)) # torch.arange(2, -1, -1)ê³¼ ê°™ìœ¼ë©°, [2, 1, 0] í…ì„œë¥¼ ìƒì„±\n",
        "print(img.shape)\n",
        "img = np.einsum('ijk->jik', img)    # NumPyì˜ ì•„ì¸ìŠˆíƒ€ì¸ í•©(einsum) í‘œê¸°ë²•ì„ ì‚¬ìš©í•´ ì°¨ì›ì„ ì¬ì •ë ¬\n",
        "print(img.shape)                    # Matplotlibì´ ìš”êµ¬í•˜ëŠ” (Height, Width, Channel) í˜•ì‹ì´ ì™„ì„±\n",
        "\n",
        "# ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
        "plt.imshow(img)\n",
        "plt.axis('off')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVFva16chlS3"
      },
      "outputs": [],
      "source": [
        "# ì´ë¯¸ì§€ ì„ë² ë”©ì„ ë§Œë“­ë‹ˆë‹¤.\n",
        "image_embedding = model.get_image_features(processed_image)\n",
        "image_embedding.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. PyTorch í…ì„œ ìƒíƒœì—ì„œ ì •ê·œí™”ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤. ---\n",
        "# (NumPy í•¨ìˆ˜ê°€ ì•„ë‹Œ, PyTorchì˜ .norm() í•¨ìˆ˜ë¡œ í†µì¼í•©ë‹ˆë‹¤)\n",
        "text_embedding /= text_embedding.norm(dim=-1, keepdim=True)\n",
        "image_embedding /= image_embedding.norm(dim=-1, keepdim=True)\n",
        "\n",
        "# --- 2. ìœ ì‚¬ë„ ê³„ì‚°ì„ ìœ„í•´ í…ì„œë¥¼ NumPy ë°°ì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. ---\n",
        "# ì´ ì‹œì ì— .detach().cpu().numpy()ë¥¼ ë”± í•œ ë²ˆë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "text_embedding_np = text_embedding.detach().cpu().numpy()\n",
        "image_embedding_np = image_embedding.detach().cpu().numpy()\n",
        "\n",
        "# --- 3. NumPy ë°°ì—´ë¡œ ìœ ì‚¬ë„(ë‚´ì )ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. ---\n",
        "score = text_embedding_np @ image_embedding_np.T\n",
        "score\n",
        "\n",
        "# array([[0.33149645]], dtype=float32)\n"
      ],
      "metadata": {
        "id": "r-AMX8J_KutS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rKItoQAkQwU"
      },
      "source": [
        "#### **[ì°¸ê³ ] ì—¬ëŸ¬ ì´ë¯¸ì§€ë¡œ ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚°í•˜ê¸°**\n",
        "- ì•ì—ì„œ ë‚˜ì˜¨ ìœ ì‚¬ë„ ì ìˆ˜(array([[0.33149645]], dtype=float32) )ì˜ ì˜ë¯¸ íŒŒì•…ì„ ìœ„í•´ ë‹¤ë¥¸ ì´ë¯¸ì§€ì™€ ìº¡ì…˜ìœ¼ë¡œ ë¹„êµí•´ë³¸ë‹¤.\n",
        "- ì‚¬ìš© ì´ë¯¸ì§€ 3ê°œ : puppy, cat, car"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMsrjH7xkSWH"
      },
      "outputs": [],
      "source": [
        "from urllib.request import urlopen\n",
        "from PIL import Image\n",
        "\n",
        "# AIë¡œ ìƒì„±í•œ ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n",
        "cat_path = \"https://bit.ly/42UJXJu\"     # ê³ ì–‘ì´ ì´ë¯¸ì§€ path\n",
        "car_path = \"https://bit.ly/4cR4rHs\"     # ìë™ì°¨ ì´ë¯¸ì§€ path\n",
        "paths = [puppy_path, cat_path, car_path]\n",
        "images = [Image.open(urlopen(path)).convert(\"RGBA\") for path in paths]\n",
        "captions = [\n",
        "    \"a puppy playing in the snow\",\n",
        "    \"a pixelated image of a cute cat\",\n",
        "    \"A supercar on the road \\nwith the sunset in the background\"\n",
        "]\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# ëª¨ë“  ì´ë¯¸ì§€ì˜ ì„ë² ë”©ì„ ë§Œë“­ë‹ˆë‹¤.\n",
        "image_embeddings = []\n",
        "for image in images:\n",
        "  image_processed = clip_processor(images=image, return_tensors='pt')['pixel_values']\n",
        "  image_embedding = model.get_image_features(image_processed).detach().cpu().numpy()[0]\n",
        "  image_embeddings.append(image_embedding)\n",
        "image_embeddings = np.array(image_embeddings)\n",
        "\n",
        "# ëª¨ë“  ìº¡ì…˜ì˜ ì„ë² ë”©ì„ ë§Œë“­ë‹ˆë‹¤.\n",
        "text_embeddings = []\n",
        "for caption in captions:\n",
        "  inputs = clip_tokenizer(caption, return_tensors=\"pt\")\n",
        "  text_emb = model.get_text_features(**inputs).detach().cpu().numpy()[0]\n",
        "  text_embeddings.append(text_emb)\n",
        "text_embeddings = np.array(text_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5ABSnFGlGeW"
      },
      "outputs": [],
      "source": [
        "# ì´ë¯¸ì§€ì™€ ìº¡ì…˜ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "sim_matrix = cosine_similarity(image_embeddings, text_embeddings)\n",
        "sim_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmIyzyTXlIRN"
      },
      "outputs": [],
      "source": [
        "# í”¼ê²¨ ê°ì²´ë¥¼ ë§Œë“­ë‹ˆë‹¤.\n",
        "def visualize_figure_matrix(sim_matrix, images, captions):\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.figure(figsize=(14, 10))\n",
        "    plt.imshow(sim_matrix, cmap='viridis')\n",
        "\n",
        "    # ì •ë‹µ ë ˆì´ë¸”ë¡œ í‹±ì„ ì„¤ì •í•©ë‹ˆë‹¤.\n",
        "    plt.yticks(range(len(captions)), captions, fontsize=18)\n",
        "    plt.xticks([])\n",
        "\n",
        "    # ì´ë¯¸ì§€ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
        "    for i, image in enumerate(images):\n",
        "        plt.imshow(image, extent=(i - 0.5, i + 0.5, -1.6, -0.6), origin=\"lower\")\n",
        "\n",
        "    # ìº¡ì…˜ì„ ì¶”ê°€í•©ë‹ˆë‹¤.\n",
        "    for x in range(sim_matrix.shape[1]):\n",
        "        for y in range(sim_matrix.shape[0]):\n",
        "            plt.text(x, y, f\"{sim_matrix[y, x]:.2f}\", ha=\"center\", va=\"center\", size=30)\n",
        "\n",
        "    # ë¶ˆí•„ìš”í•œ ìš”ì†Œë¥¼ ì œê±°í•©ë‹ˆë‹¤.\n",
        "    for side in [\"left\", \"top\", \"right\", \"bottom\"]:\n",
        "        plt.gca().spines[side].set_visible(False)\n",
        "\n",
        "        # í¬ê¸°ë¥¼ ì¡°ì •í•©ë‹ˆë‹¤.\n",
        "        plt.xlim([-0.5, len(captions) - 0.5])\n",
        "        plt.ylim([len(captions) + 0.5, -2])\n",
        "        # plt.title(\"Similarity Matrix\", size=20)\n",
        "        plt.savefig(\"sim_matrix.png\", dpi=300, bbox_inches='tight')\n",
        "\n",
        "visualize_figure_matrix(sim_matrix, images, captions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5it3Vqi0Bf5"
      },
      "source": [
        "#### âœ… **ì˜ˆì œ: SBERT ì„ë² ë”© ëª¨ë¸ ë°©ì‹ ì‚¬ìš©**\n",
        "\n",
        "- **ì„ë² ë”©ì„ ì‰½ê²Œ ë§Œë“¤ ìˆ˜ ìˆëŠ” CLIPê¸°ë°˜ ëª¨ë¸ ì‚¬ìš©í•´ë³´ê¸°**\n",
        "- **OpenCLIP ëª¨ë¸**: **clip-ViT-B-32**\n",
        "    - OpenAIê°€ 2021ë…„ ë…¼ë¬¸ \"Learning Transferable Visual Models From Natural Language Supervision\"ì—ì„œ ì‚¬ìš©í•œ ê³µì‹ ëª¨ë¸ ì•„í‚¤í…ì²˜ ëª…ì¹­\n",
        "    - <mark>'ë©€í‹°ëª¨ë‹¬ AI'ì™€ 'ì œë¡œìƒ· í•™ìŠµ'ì„ ì„¤ëª…í•  ë•Œ ê°€ì¥ ê¸°ë³¸ì´ ë˜ê³  ìƒì§•ì ì¸ ëª¨ë¸</mark>\n",
        "    - ì•ì—ì„œ ì‚¬ìš©í•œ **openai/clip-vit-base-patch32** ì™€ ë™ì¼í•œ ëª¨ë¸\n",
        "        - í—ˆê¹…í˜ì´ìŠ¤ì—ì„œ ì‚¬ìš©í•  ë•ŒëŠ” openai/clip-vit-base-patch32 ë¡œ ë“±ë¡ë˜ì–´ ìˆìŒ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XFEvSss0DQf"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# SBERT í˜¸í™˜ CLIP ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.\n",
        "model = SentenceTransformer('clip-ViT-B-32')\n",
        "\n",
        "# ì´ë¯¸ì§€ë¥¼ ì¸ì½”ë”©í•©ë‹ˆë‹¤.\n",
        "image_embeddings = model.encode(images)\n",
        "\n",
        "# ìº¡ì…˜ì„ ì¸ì½”ë”©í•©ë‹ˆë‹¤.\n",
        "text_embeddings = model.encode(captions)\n",
        "\n",
        "# ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
        "sim_matrix = util.cos_sim(image_embeddings, text_embeddings)\n",
        "print(sim_matrix)\n",
        "\n",
        "visualize_figure_matrix(sim_matrix, images, captions)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "_cB31WDwQex9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. í…ìŠ¤íŠ¸ ìƒì„± ëª¨ë¸ì„ ë©€í‹°ëª¨ë‹¬ë¡œ ë§Œë“¤ê¸°**"
      ],
      "metadata": {
        "id": "AlGsFR3lQf3i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **í…ìŠ¤íŠ¸ ìƒì„± ëª¨ë¸**\n",
        "    - <mark>í…ìŠ¤íŠ¸ ìƒì„± ëª¨ë¸ì˜ ê²½ìš° íŠ¹ì • ì…ë ¥ ì´ë¯¸ì§€ì— ëŒ€í•´ ì¶”ë¡ í•  ìˆ˜ ìˆë‹¤.</mark>\n",
        "        - ex: í”¼ì ì´ë¯¸ì§€ë¥¼ ì£¼ê³  ì–´ë–¤ ì¬ë£Œê°€ ë“¤ì–´ ìˆëŠ”ì§€ ë¬¼ì„ ìˆ˜ ìˆë‹¤\n",
        "        - ex: ì—í íƒ‘ ì‚¬ì§„ì„ ë³´ì—¬ ì£¼ê³  ì–¸ì œ ì§€ì–´ì¡ŒëŠ”ì§€ ì–´ë””ì— ìˆëŠ”ì§€ ë¬¼ì–´ë³¼ ìˆ˜ë„ ìˆë‹¤.\n",
        "        "
      ],
      "metadata": {
        "id": "yXa7yUsgf9Wz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **BLIP**\n",
        "\n",
        "- **BLIP**\n",
        "    - **BLIP** **Bootstrapping Language-Image Pre-training** for Unified Vision-Language Understanding and Generation\n",
        "    - BLIPì€ 2022ë…„ 1ì›” Salesforce AI Researchì—ì„œ ë°œí‘œí•œ ë…¼ë¬¸ìœ¼ë¡œ, ë‹¹ì‹œ Vision-Language Pre-training (VLP) ë¶„ì•¼ì˜ ë‘ ê°€ì§€ í° ë¬¸ì œë¥¼ í•´ê²°í•˜ë©° ë“±ì¥í•œ í˜ì‹ ì ì¸ ëª¨ë¸\n",
        "    - **[BLIPì˜ í•µì‹¬ ëª©í‘œ]**\n",
        "        1. **ëª¨ë¸ì˜ í•œê³„ (Understanding vs. Generation)**\n",
        "        2. **ë°ì´í„°ì˜ í•œê³„ (Noisy Web Data)**\n",
        "        - **CapFilt** (**Captioning and Filtering**)ë¼ëŠ” 2ë‹¨ê³„ í”„ë¡œì„¸ìŠ¤ë¡œ êµ¬í˜„\n",
        "            - Captioning : Captioner (ìº¡ì…˜ ìƒì„±ê¸°)\n",
        "            - Filtering :  Filter (ë…¸ì´ì¦ˆ ì œê±°ê¸°)\n",
        "    - \"í•˜ë‚˜ì˜ ëª¨ë¸ ì•„í‚¤í…ì²˜ë¡œ 'ì´í•´(Understanding)'ì™€ 'ìƒì„±(Generation)'ì„ ëª¨ë‘ ì˜í•˜ë©´ì„œ,\n",
        "    - ë™ì‹œì— 'ì§€ì €ë¶„í•œ ì›¹ ë°ì´í„°(Noisy Data)'ë¥¼ ìŠ¤ìŠ¤ë¡œ ì •ì œí•´ì„œ í•™ìŠµ íš¨ìœ¨ì„ ê·¹ëŒ€í™”í•˜ì!\"\n",
        "- **BLIP-1ì´ VLP ëª¨ë¸ì˜ 'ì²´ê¸‰(ë°ì´í„°+ì•„í‚¤í…ì²˜)' ìì²´ë¥¼ í‚¤ìš°ë ¤ í–ˆë‹¤ë©´**,\n",
        "- **BLIP-2ëŠ” 'íš¨ìœ¨ì„±'ì— ì´ˆì ì„ ë§ì¶° ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ê°•ë ¥í•œ ëª¨ë“ˆë“¤ì„ 'ì¡°ë¦½'í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ íŒ¨ëŸ¬ë‹¤ì„ì„ ì „í™˜**"
      ],
      "metadata": {
        "id": "65ZuV0Uu3Klq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### âœ… **ì˜ˆì œ : BLIPìœ¼ë¡œ ì´ë¯¸ì§€ ìë™ ìº¡ì…”ë‹í•˜ê¸°**"
      ],
      "metadata": {
        "id": "PdiKFrJi3WAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ëª¨ë¸ ë° í”„ë¡œì„¸ì„œ ë¡œë“œ\n",
        "model_name = \"Salesforce/blip-image-captioning-base\"\n",
        "blip_processor = BlipProcessor.from_pretrained(model_name)\n",
        "model = BlipForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# ì´ë¯¸ì§€ ì…ë ¥ : ì ì ˆí•œ ì´ë¯¸ì§€ íŒŒì¼ ì‚¬ìš©\n",
        "image = Image.open(\"dog.jpg\")\n",
        "# ì´ë¯¸ì§€ ì‹œê°í™”\n",
        "plt.imshow(image)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# ì´ë¯¸ì§€ ìº¡ì…˜ ìƒì„± (LM ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸)\n",
        "# ì¡°ê±´ ì—†ì´ ìƒì„± (Unconditional Captioning)\n",
        "# ì´ë¯¸ì§€ë§Œ ì…ë ¥ìœ¼ë¡œ ë„£ì–´ì¤ë‹ˆë‹¤.\n",
        "inputs_uncond = blip_processor(images=image, return_tensors=\"pt\")\n",
        "out_uncond = model.generate(**inputs_uncond, max_new_tokens=50)\n",
        "caption_uncond = blip_processor.decode(out_uncond[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"--- BLIP-1 ì´ë¯¸ì§€ ìº¡ì…”ë‹ (ì¡°ê±´ ì—†ìŒ) ---\")\n",
        "print(f\"ìƒì„±ëœ ìº¡ì…˜: {caption_uncond}\")\n"
      ],
      "metadata": {
        "id": "o0DclwDT3aDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì›¹ì—ì„œ ì´ë¯¸ì§€ë¥¼ ì°¾ì•„ì„œ ì…ë ¥í•´ì£¼ì„¸ìš”.\n",
        "image = Image.open(\"pexels-kylenehashimoto-1334441.jpg\")\n",
        "# ì´ë¯¸ì§€ ì‹œê°í™”\n",
        "plt.imshow(image)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# ì´ë¯¸ì§€ ìº¡ì…˜ ìƒì„± (LM ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸)\n",
        "# ì¡°ê±´ ì—†ì´ ìƒì„± (Unconditional Captioning)\n",
        "# ì´ë¯¸ì§€ë§Œ ì…ë ¥ìœ¼ë¡œ ë„£ì–´ì¤ë‹ˆë‹¤.\n",
        "inputs_uncond = blip_processor(images=image, return_tensors=\"pt\")\n",
        "out_uncond = model.generate(**inputs_uncond, max_new_tokens=50)\n",
        "caption_uncond = blip_processor.decode(out_uncond[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"--- BLIP-1 ì´ë¯¸ì§€ ìº¡ì…”ë‹ (ì¡°ê±´ ì—†ìŒ) ---\")\n",
        "print(f\"ìƒì„±ëœ ìº¡ì…˜: {caption_uncond}\")\n"
      ],
      "metadata": {
        "id": "uHIA2Dq-l4iR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **BLIP-2**\n",
        "\n",
        "- ë‘ ë„ë©”ì¸ ì‚¬ì´ì˜ ê°„ê·¹ì„ ë©”ê¾¸ê¸° ìœ„í•´ ê¸°ì¡´ ëª¨ë¸ì— ë©€í‹°ëª¨ë‹¬ ê¸°ëŠ¥ì„ ì¶”ê°€í•˜ë ¤ëŠ” ì‹œë„\n",
        "- í•µì‹¬ ì•„ì´ë””ì–´ëŠ” **ì´ë¯¸ ì˜ ë§Œë“¤ì–´ì§„ ê°•ë ¥í•œ ë¹„ì „ ëª¨ë¸ê³¼ LLMì„ 'ì–¼ë ¤ì„œ(frozen)' ì¬ì‚¬ìš©í•˜ê³ , ê·¸ ì‚¬ì´ë¥¼ ì—°ê²°í•˜ëŠ” 'ë‹¤ë¦¬'ë§Œ ì˜ í•™ìŠµì‹œí‚¤ì**ëŠ” ê²ƒ"
      ],
      "metadata": {
        "id": "vo04pQsgifux"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **ëª¨ë¸ ì§€ì •**"
      ],
      "metadata": {
        "id": "2SrbEXwgKBFL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnNrafWu6BSJ"
      },
      "outputs": [],
      "source": [
        "# ëª¨ë¸ ë‹¤ìš´ë¡œë“œí•˜ëŠ”ë° ì•½ê°„ ì‹œê°„ì´ ê±¸ë¦°ë‹¤.(T4 3~5ë¶„ )\n",
        "from transformers import AutoProcessor, Blip2ForConditionalGeneration\n",
        "import torch\n",
        "\n",
        "# ì „ì²˜ë¦¬ê¸°ì™€ ë©”ì¸ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.\n",
        "# # Choose specific model because of: https://huggingface.co/Salesforce/blip2-opt-2.7b/discussions/39\n",
        "model_name = \"Salesforce/blip2-opt-2.7b\"\n",
        "blip_processor = AutoProcessor.from_pretrained(model_name)\n",
        "model = Blip2ForConditionalGeneration.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "# ì¶”ë¡  ì†ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´ ëª¨ë¸ì„ GPUì— ì „ì†¡í•©ë‹ˆë‹¤.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xI_UWlR8c_Ey"
      },
      "source": [
        "#### **ì´ë¯¸ì§€ ì „ì²˜ë¦¬**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6YeV_TEQFAA"
      },
      "outputs": [],
      "source": [
        "from urllib.request import urlopen\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n",
        "car_path = \"https://bit.ly/4cR4rHs\" # ìˆ˜í¼ì¹´\n",
        "image = Image.open(urlopen(car_path)).convert(\"RGB\")\n",
        "\n",
        "# ì´ë¯¸ì§€ ì‹œê°í™”\n",
        "plt.imshow(image)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "flZeiDFmQIIg"
      },
      "outputs": [],
      "source": [
        "# ì´ë¯¸ì§€ë¥¼ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
        "inputs = blip_processor(image, return_tensors=\"pt\").to(device, torch.float16)\n",
        "inputs[\"pixel_values\"].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lHJ7LiKUhWf"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "def show_image(image_inputs):\n",
        "    # ë„˜íŒŒì´ ë°°ì—´ë¡œ ë³€í™˜í•˜ê³ , í¬ê¸°ë¥¼ (1, 3, 224, 224)ì—ì„œ (224, 224, 3)ë¡œ ë°”ê¿‰ë‹ˆë‹¤.\n",
        "    image_inputs = inputs[\"pixel_values\"][0].detach().cpu().numpy()\n",
        "    image_inputs = np.einsum('ijk->kji', image_inputs)\n",
        "    image_inputs = np.einsum('ijk->jik', image_inputs)\n",
        "\n",
        "    # RGB ê°’ì— í•´ë‹¹í•˜ëŠ” 0-255 ë²”ìœ„ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
        "    scaler = MinMaxScaler(feature_range=(0, 255))\n",
        "    image_inputs = scaler.fit_transform(image_inputs.reshape(-1, image_inputs.shape[-1])).reshape(image_inputs.shape)\n",
        "    image_inputs = np.array(image_inputs, dtype=np.uint8)\n",
        "\n",
        "    # ë„˜íŒŒì´ ë°°ì—´ì„ Image ê°ì²´ë¡œ ë°”ê¿‰ë‹ˆë‹¤.\n",
        "    return Image.fromarray(image_inputs)\n",
        "\n",
        "t_image = show_image(inputs)\n",
        "t_image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t_image.info"
      ],
      "metadata": {
        "id": "9bNcNCzcndUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDoBrN56dBTF"
      },
      "source": [
        "#### **í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬**(skip ê°€ëŠ¥)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHJkauW9dFhB"
      },
      "outputs": [],
      "source": [
        "blip_processor.tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21Zh5Rx8QLAy"
      },
      "outputs": [],
      "source": [
        "# í…ìŠ¤íŠ¸ë¥¼ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
        "text = \"Her vocalization was remarkably melodic\"\n",
        "token_ids = blip_processor(text=text, return_tensors=\"pt\")\n",
        "token_ids = token_ids.to(device, torch.float16)[\"input_ids\"][0]\n",
        "\n",
        "# ì…ë ¥ IDë¥¼ í† í°ìœ¼ë¡œ ë˜ëŒë¦½ë‹ˆë‹¤.\n",
        "tokens = blip_processor.tokenizer.convert_ids_to_tokens(token_ids)\n",
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFnL_Yt3elPX"
      },
      "outputs": [],
      "source": [
        "# íŠ¹ìˆ˜ í† í°ì„ ë°‘ì¤„ ë¬¸ìë¡œ ë°”ê¿‰ë‹ˆë‹¤.\n",
        "tokens = [token.replace(\"Ä \", \"_\") for token in tokens]\n",
        "tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDx5elnHeHnT"
      },
      "source": [
        "#### âœ… **ì˜ˆì œ : ì´ë¯¸ì§€ ìº¡ì…”ë‹**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFJ9-l8c8u3i"
      },
      "outputs": [],
      "source": [
        "# AIê°€ ìƒì„±í•œ ìˆ˜í¼ì¹´ ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n",
        "image = Image.open(urlopen(car_path)).convert(\"RGB\")\n",
        "\n",
        "# ì´ë¯¸ì§€ë¥¼ ì „ì²˜ë¦¬í•˜ì—¬ ì…ë ¥ì„ ì¤€ë¹„í•©ë‹ˆë‹¤.\n",
        "inputs = blip_processor(image, return_tensors=\"pt\").to(device, torch.float16)\n",
        "show_image(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHIoTZa6eEaR"
      },
      "outputs": [],
      "source": [
        "# ì´ë¯¸ì§€ë¥¼ ì„ë² ë”©ì„ ë§Œë“¤ê³  Q-í¬ë¨¸ì˜ ì¶œë ¥ì„ ë””ì½”ë”(LLM)ì— ì „ë‹¬í•´ í† í° IDë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "generated_ids = model.generate(**inputs, max_new_tokens=20)\n",
        "\n",
        "# í† í° IDë¥¼ ë°”íƒ•ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "generated_text = blip_processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "generated_text = generated_text[0].strip()\n",
        "generated_text\n",
        "print(f\"ìƒì„±ëœ ìº¡ì…˜: {generated_text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### âœ… **ì˜ˆì œ : ì´ë¯¸ì§€ ì„¤ëª…**(ë¡œë¥´ìƒ¤í í…ŒìŠ¤íŠ¸ë¥¼ ì´ìš©í•œ ì‹¬ë¦¬ë¶„ì„)\n",
        "- [ì°¸ê³ ] https://blog.naver.com/eehome/220365180030\n",
        "\n"
      ],
      "metadata": {
        "id": "aiGRhvhuLpW0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JbGnyiYhlfQi"
      },
      "outputs": [],
      "source": [
        "# ë¡œë¥´ìƒ¤í ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤\n",
        "url = 'https://postfiles.pstatic.net/20150520_195/eehome_1432100347266Kk6U5_JPEG/2009073100028_0.jpg?type=w3840'\n",
        "image = Image.open(urlopen(url)).convert(\"RGB\")\n",
        "image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bIxPEySrJW0"
      },
      "outputs": [],
      "source": [
        "# ë¡œë¥´ìƒ¤í ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n",
        "url = 'https://postfiles.pstatic.net/20150520_195/eehome_1432100347266Kk6U5_JPEG/2009073100028_0.jpg?type=w3840'\n",
        "image = Image.open(urlopen(url)).convert(\"RGB\")\n",
        "\n",
        "# ìº¡ì…˜ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "inputs = blip_processor(image, return_tensors=\"pt\").to(device, torch.float16)\n",
        "generated_ids = model.generate(**inputs, max_new_tokens=20)\n",
        "generated_text = blip_processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "generated_text = generated_text[0].strip()\n",
        "print(f\"ìƒì„±ëœ ìº¡ì…˜: {generated_text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjHfQxWGkVYF"
      },
      "source": [
        "### âœ… **ì˜ˆì œ : ì±„íŒ… ê¸°ë°˜ ë©€í‹°ëª¨ë‹¬ í”„ë¡¬í”„íŠ¸**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "269NtauNFIze"
      },
      "outputs": [],
      "source": [
        "# AIë¡œ ìƒì„±í•œ ìˆ˜í¼ì¹´ ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n",
        "image = Image.open(urlopen(car_path)).convert(\"RGB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_NsMF_hMCIIj"
      },
      "outputs": [],
      "source": [
        "# ì‹œê° ì§ˆë¬¸ ë‹µë³€\n",
        "prompt = \"Question: Write down what you see in this picture. Answer:\"\n",
        "\n",
        "# ì´ë¯¸ì§€ì™€ í”„ë¡¬í”„íŠ¸ë¥¼ ëª¨ë‘ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
        "inputs = blip_processor(image, text=prompt, return_tensors=\"pt\").to(device, torch.float16)\n",
        "\n",
        "# í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "generated_ids = model.generate(**inputs, max_new_tokens=30)\n",
        "generated_text = blip_processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "generated_text = generated_text[0].strip()\n",
        "print(f\"ìƒì„±ëœ ìº¡ì…˜: {generated_text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtpMIgUaScBD"
      },
      "outputs": [],
      "source": [
        "# ì±„íŒ… ìŠ¤íƒ€ì¼ì˜ í”„ë¡¬í”„íŠ¸\n",
        "prompt = \"Question: Write down what you see in this picture. Answer: A sports car driving on the road at sunset. Question: What would it cost me to drive that car? Answer:\"\n",
        "\n",
        "# ì¶œë ¥ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "inputs = blip_processor(image, text=prompt, return_tensors=\"pt\").to(device, torch.float16)\n",
        "generated_ids = model.generate(**inputs, max_new_tokens=30)\n",
        "generated_text = blip_processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "generated_text = generated_text[0].strip()\n",
        "generated_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtvYbcF-H_t6"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML, display\n",
        "import ipywidgets as widgets\n",
        "\n",
        "def text_eventhandler(*args):\n",
        "  question = args[0][\"new\"]\n",
        "  if question:\n",
        "    args[0][\"owner\"].value = \"\"\n",
        "\n",
        "    # í”„ë¡¬í”„íŠ¸ë¥¼ ë§Œë“­ë‹ˆë‹¤.\n",
        "    if not memory:\n",
        "      prompt = \" Question: \" + question + \" Answer:\"\n",
        "    else:\n",
        "      template = \"Question: {} Answer: {}.\"\n",
        "      prompt = \" \".join(\n",
        "          [\n",
        "              template.format(memory[i][0], memory[i][1])\n",
        "              for i in range(len(memory))\n",
        "          ]\n",
        "      ) + \" Question: \" + question + \" Answer:\"\n",
        "\n",
        "    # í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "    inputs = blip_processor(image, text=prompt, return_tensors=\"pt\")\n",
        "    inputs = inputs.to(device, torch.float16)\n",
        "    generated_ids = model.generate(**inputs, max_new_tokens=100)\n",
        "    generated_text = blip_processor.batch_decode(\n",
        "        generated_ids,\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "    generated_text = generated_text[0].strip().split(\"Answer: \")[-1]\n",
        "\n",
        "    # ë©”ëª¨ë¦¬ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.\n",
        "    memory.append((question, generated_text))\n",
        "\n",
        "    # ì¶œë ¥ì— í• ë‹¹í•©ë‹ˆë‹¤.\n",
        "    output.append_display_data(HTML(\"<b>USER:</b> \" + question))\n",
        "    output.append_display_data(HTML(\"<b>BLIP-2:</b> \" + generated_text))\n",
        "    output.append_display_data(HTML(\"<br>\"))\n",
        "\n",
        "# ìœ„ì ¯ì„ ì¤€ë¹„í•©ë‹ˆë‹¤.\n",
        "in_text = widgets.Text()\n",
        "in_text.continuous_update = False\n",
        "in_text.observe(text_eventhandler, \"value\")\n",
        "output = widgets.Output()\n",
        "memory = []\n",
        "\n",
        "# ì±„íŒ… ìƒìë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
        "display(\n",
        "    widgets.VBox(\n",
        "        children=[output, in_text],\n",
        "        layout=widgets.Layout(display=\"inline-flex\", flex_flow=\"column-reverse\"),\n",
        "    )\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}