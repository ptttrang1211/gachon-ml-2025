{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ì–¸ì–´ AIì˜ ìµœê·¼ ì—­ì‚¬"
      ],
      "metadata": {
        "id": "_intVJo1FXQF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "- ğŸ’¡**ì°¸ê³  êµì¬**\n",
        "    - í•¸ì¦ˆì˜¨LLM https://www.hanbit.co.kr/store/books/look.php?p_code=B2599445240\n",
        "- ğŸ’¡ **NOTE**\n",
        "    - ì´ ë…¸íŠ¸ë¶ì˜ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë ¤ë©´ GPUë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. êµ¬ê¸€ ì½”ë©ì—ì„œëŠ” **ëŸ°íƒ€ì„ > ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½ > í•˜ë“œì›¨ì–´ ê°€ì†ê¸° > T4 GPU**ë¥¼ ì„ íƒí•˜ì„¸ìš”.\n",
        "    - **T4 GPU | 16 VRAM(Video Random Access Memory)**\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "81_Ybs4LI7IX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[ì£¼ì˜!] (ì½”ë©)í•œê¸€ í°íŠ¸ ì§€ì •í•˜ëŠ” ë°©ë²•**"
      ],
      "metadata": {
        "id": "eLfwMhbVRg6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 0) í™˜ê²½ ì ê²€: ì„¤ì¹˜ëœ í°íŠ¸ ì‹¤ì œ íŒŒì¼ì´ ìˆëŠ”ì§€ ë¨¼ì € í™•ì¸\n",
        "!ls -R /usr/share/fonts | head -n 50\n",
        "\n",
        "# 1) (ì´ë¯¸ ì„¤ì¹˜í–ˆë‹¤ë©´ ìƒëµ ê°€ëŠ¥) í•œê¸€ í°íŠ¸ ì„¤ì¹˜\n",
        "!apt-get -y update\n",
        "!apt-get -y install fonts-nanum fonts-noto-cjk\n",
        "\n",
        "# 2) ì‹œìŠ¤í…œ í°íŠ¸ ìºì‹œ ì¬ìƒì„±\n",
        "!fc-cache -fv\n",
        "\n",
        "# 3) Matplotlib ë‚´ë¶€ í°íŠ¸ ìºì‹œ ì‚­ì œ (ì¤‘ìš”)\n",
        "import os, shutil\n",
        "import matplotlib\n",
        "cache_dir = matplotlib.get_cachedir()      # ë³´í†µ ~/.cache/matplotlib\n",
        "# print(\"matplotlib cache:\", cache_dir)\n",
        "shutil.rmtree(cache_dir, ignore_errors=True)\n",
        "\n",
        "# 4) ëŸ°íƒ€ì„ ì¬ì‹œì‘ í•„ìš” ì—†ì´, í°íŠ¸ ë§¤ë‹ˆì € ê°•ì œ ë¦¬ë¡œë“œ\n",
        "from matplotlib import font_manager\n",
        "_ = font_manager._load_fontmanager(try_read_cache=False)\n",
        "\n",
        "# 5) í°íŠ¸ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ (fc-listì™€ íŒŒì´ì¬ì—ì„œ ëª¨ë‘ í™•ì¸)\n",
        "# !fc-list | grep -i \"nanum\\|noto\" | head -n 40\n",
        "\n",
        "# from matplotlib import font_manager\n",
        "# fonts = font_manager.findSystemFonts()\n",
        "# [f for f in fonts if (\"Nanum\" in f or \"NotoSansCJK\" in f or \"Noto\" in f)][:10]\n",
        "\n",
        "\n",
        "# 6) í•œê¸€ í°íŠ¸ ì§€ì •\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from matplotlib import font_manager, pyplot as plt\n",
        "\n",
        "font_path = \"/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf\"\n",
        "font_manager.fontManager.addfont(font_path)\n",
        "family_name = font_manager.FontProperties(fname=font_path).get_name()\n",
        "# print(\"ì ìš©í•  íŒ¨ë°€ë¦¬ëª…:\", family_name)\n",
        "\n",
        "# plt.rcParams[\"font.family\"] = 'NanumBarunGothic'\n",
        "plt.rcParams[\"font.family\"] = family_name # NanumBarunGothic\n",
        "plt.rcParams[\"axes.unicode_minus\"] = False\n",
        "print(\"âœ… ë‚˜ëˆ”í°íŠ¸ ì„¤ì • ì™„ë£Œ\")# í°íŠ¸ ì§€ì •\n",
        "\n",
        "\n",
        "# 7) ì˜ˆì‹œ ë°ì´í„°\n",
        "documents = [\"ë¬¸ì„œ1\",\"ë¬¸ì„œ2\",\"ë¬¸ì„œ3\"]\n",
        "similarity_matrix = np.array([[1.0,0.7,0.2],\n",
        "                              [0.7,1.0,0.4],\n",
        "                              [0.2,0.4,1.0]])\n",
        "\n",
        "plt.figure(figsize=(3,4))\n",
        "sns.heatmap(similarity_matrix,\n",
        "            annot=True,\n",
        "            cmap='Blues',\n",
        "            xticklabels=[f'ë¬¸ì„œ{i+1}' for i in range(len(documents))],\n",
        "            yticklabels=[f'ë¬¸ì„œ{i+1}' for i in range(len(documents))])\n",
        "plt.title('ë¬¸ì„œ ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Rj_HPelGfns7",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "DJRiPmIyRePs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. BoW(Bag-of-Words)**"
      ],
      "metadata": {
        "id": "BHtmx0nuFlEa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **ë¹„êµ¬ì¡°ì ì¸ í…ìŠ¤íŠ¸ë¥¼ í‘œí˜„**í•˜ëŠ” í•œ ë°©ë²•\n",
        "- 1950ë…„ëŒ€ì— ì–¸ê¸‰ë˜ì—ˆì§€ë§Œ 2000ë…„ëŒ€ì— ì¸ê¸°ë¥¼ ì–»ìŒ\n",
        "- ê³µë°±ì„ ê¸°ì¤€ìœ¼ë¡œ ê°œë³„ ë‹¨ì–´ë¡œ ë¶„í• --> Token\n",
        "- **í† í°í™”(Tokenization)** ê³¼ì •:  \n",
        "    - ë¬¸ì¥ì„ ê°œë³„ ë‹¨ì–´ë‚˜ ë¶€ë¶„ë‹¨ì–´(subword)ë¡œ ë¶„í• í•˜ëŠ” ê³¼ì •\n",
        "- **BoWì˜ ëª©í‘œ**:  \n",
        "    - **ë²¡í„°(or ë²¡í„° í‘œí˜„)ë¼ê³  ë¶€ë¥´ëŠ” ìˆ˜ì¹˜ í˜•íƒœë¡œ í…ìŠ¤íŠ¸ë¥¼ í‘œí˜„**\n",
        "- ì´ëŸ° ëª¨ë¸ì„ **í‘œí˜„ ëª¨ë¸(representation model)** ì´ë¼ê³  ë¶€ë¦„"
      ],
      "metadata": {
        "id": "4c93FnL3PZp1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ì˜ˆì œ : BoW êµ¬í˜„í•˜ê³  í…ìŠ¤íŠ¸ ë¶„ë¥˜í•˜ê¸°**"
      ],
      "metadata": {
        "id": "GIW9EoDWR-u2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **BoW Basic Class(ì§ì ‘ êµ¬í˜„)**"
      ],
      "metadata": {
        "id": "V8gnwjtCbttx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import re\n",
        "from collections import Counter, defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "\n",
        "plt.rcParams['font.family'] = 'NanumBarunGothic'\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "\n",
        "class BagOfWords:\n",
        "    def __init__(self, min_freq=1, max_vocab_size=None, remove_stopwords=True):\n",
        "        \"\"\"\n",
        "        Bag of Words êµ¬í˜„ í´ë˜ìŠ¤\n",
        "\n",
        "        Args:\n",
        "            min_freq: ìµœì†Œ ë‹¨ì–´ ë¹ˆë„\n",
        "            max_vocab_size: ìµœëŒ€ ì–´íœ˜ í¬ê¸°\n",
        "            remove_stopwords: ë¶ˆìš©ì–´ ì œê±° ì—¬ë¶€\n",
        "        \"\"\"\n",
        "        self.min_freq = min_freq\n",
        "        self.max_vocab_size = max_vocab_size\n",
        "        self.remove_stopwords = remove_stopwords\n",
        "        self.vocab = {}\n",
        "        self.word_to_idx = {}\n",
        "        self.idx_to_word = {}\n",
        "        self.vocab_size = 0\n",
        "\n",
        "        # í•œêµ­ì–´ ë¶ˆìš©ì–´ (ê°„ë‹¨í•œ ì˜ˆì œ)\n",
        "        self.stopwords = {\n",
        "            'ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì—ì„œ', 'ë¡œ', 'ìœ¼ë¡œ',\n",
        "            'ì˜', 'ì™€', 'ê³¼', 'ë„', 'ë§Œ', 'ê¹Œì§€', 'ë¶€í„°', 'ì¡°ì°¨', 'ë§ˆì €',\n",
        "            'ê·¸ë¦¬ê³ ', 'ê·¸ëŸ¬ë‚˜', 'í•˜ì§€ë§Œ', 'ë˜í•œ', 'ê·¸ë˜ì„œ', 'ë”°ë¼ì„œ'\n",
        "        }\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬\"\"\"\n",
        "        # ì†Œë¬¸ì ë³€í™˜ ë° íŠ¹ìˆ˜ë¬¸ì ì œê±° (í•œê¸€ ë³´ì¡´)\n",
        "        text = re.sub(r'[^\\w\\sê°€-í£]', '', text.lower())\n",
        "        words = text.split()\n",
        "\n",
        "        # ë¶ˆìš©ì–´ ì œê±°\n",
        "        if self.remove_stopwords:\n",
        "            words = [word for word in words if word not in self.stopwords]\n",
        "\n",
        "        return [word for word in words if word.strip()]\n",
        "\n",
        "    def build_vocab(self, documents):\n",
        "        \"\"\"ì–´íœ˜ êµ¬ì¶•\"\"\"\n",
        "        word_counts = Counter()\n",
        "\n",
        "        # ëª¨ë“  ë¬¸ì„œì—ì„œ ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°\n",
        "        for doc in documents:\n",
        "            words = self.preprocess_text(doc)\n",
        "            word_counts.update(words)\n",
        "\n",
        "        # ìµœì†Œ ë¹ˆë„ í•„í„°ë§\n",
        "        filtered_words = {word: count for word, count in word_counts.items()\n",
        "                         if count >= self.min_freq}\n",
        "\n",
        "        # ë¹ˆë„ìˆœ ì •ë ¬\n",
        "        sorted_words = sorted(filtered_words.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # ìµœëŒ€ ì–´íœ˜ í¬ê¸° ì œí•œ\n",
        "        if self.max_vocab_size:\n",
        "            sorted_words = sorted_words[:self.max_vocab_size]\n",
        "\n",
        "        # ì–´íœ˜ ë§¤í•‘ ìƒì„±\n",
        "        self.vocab = dict(sorted_words)\n",
        "        self.word_to_idx = {word: idx for idx, (word, _) in enumerate(sorted_words)}\n",
        "        self.idx_to_word = {idx: word for word, idx in self.word_to_idx.items()}\n",
        "        self.vocab_size = len(self.vocab)\n",
        "\n",
        "        print(f\"êµ¬ì¶•ëœ ì–´íœ˜ í¬ê¸°: {self.vocab_size}\")\n",
        "        return self.vocab\n",
        "\n",
        "    def text_to_bow(self, text):\n",
        "        \"\"\"í…ìŠ¤íŠ¸ë¥¼ BoW ë²¡í„°ë¡œ ë³€í™˜\"\"\"\n",
        "        words = self.preprocess_text(text)\n",
        "        bow_vector = np.zeros(self.vocab_size)\n",
        "\n",
        "        for word in words:\n",
        "            if word in self.word_to_idx:\n",
        "                idx = self.word_to_idx[word]\n",
        "                bow_vector[idx] += 1\n",
        "\n",
        "        return bow_vector\n",
        "\n",
        "    def documents_to_bow_matrix(self, documents):\n",
        "        \"\"\"ë¬¸ì„œë“¤ì„ BoW í–‰ë ¬ë¡œ ë³€í™˜\"\"\"\n",
        "        bow_matrix = []\n",
        "        for doc in documents:\n",
        "            bow_vector = self.text_to_bow(doc)\n",
        "            bow_matrix.append(bow_vector)\n",
        "\n",
        "        return np.array(bow_matrix)\n",
        "\n",
        "    def get_top_words(self, bow_vector, top_n=5):\n",
        "        \"\"\"BoW ë²¡í„°ì—ì„œ ìƒìœ„ ë‹¨ì–´ë“¤ ì¶”ì¶œ\"\"\"\n",
        "        word_scores = [(self.idx_to_word[idx], score)\n",
        "                      for idx, score in enumerate(bow_vector) if score > 0]\n",
        "        word_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "        return word_scores[:top_n]\n",
        "\n",
        "    def calculate_similarity(self, doc1, doc2):\n",
        "        \"\"\"ë‘ ë¬¸ì„œ ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°\"\"\"\n",
        "        bow1 = self.text_to_bow(doc1)\n",
        "        bow2 = self.text_to_bow(doc2)\n",
        "\n",
        "        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„\n",
        "        dot_product = np.dot(bow1, bow2)\n",
        "        norm1 = np.linalg.norm(bow1)\n",
        "        norm2 = np.linalg.norm(bow2)\n",
        "\n",
        "        if norm1 == 0 or norm2 == 0:\n",
        "            return 0.0\n",
        "\n",
        "        return dot_product / (norm1 * norm2)\n"
      ],
      "metadata": {
        "id": "M8R7Gl3-byCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1. ê¸°ë³¸ BoW êµ¬í˜„ ì‹œì—°**"
      ],
      "metadata": {
        "id": "ObS9srkGb6LZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# ì˜ˆì œ ë¬¸ì„œë“¤\n",
        "# ----------------------------\n",
        "documents = [\n",
        "    \"ê°•ì•„ì§€ëŠ” ê·€ì—¬ìš´ ë™ë¬¼ì…ë‹ˆë‹¤\",\n",
        "    \"ê³ ì–‘ì´ë„ ê·€ì—¬ìš´ ë°˜ë ¤ë™ë¬¼ì…ë‹ˆë‹¤\",\n",
        "    \"ë°˜ë ¤ë™ë¬¼ì€ ê°€ì¡±ê³¼ ê°™ìŠµë‹ˆë‹¤\",\n",
        "    \"ì¸ê³µì§€ëŠ¥ì€ ë¯¸ë˜ ê¸°ìˆ ì…ë‹ˆë‹¤\",\n",
        "    \"ê¸°ìˆ  ë°œì „ì€ ë¹ ë¦…ë‹ˆë‹¤\",\n",
        "    \"ë¯¸ë˜ì—ëŠ” ë¡œë´‡ì´ ì¼ìƒì´ ë  ê²ƒì…ë‹ˆë‹¤\"\n",
        "] # 6 docs\n",
        "\n",
        "# documents = [\n",
        "#     \"ìì—°ì–¸ì–´ì²˜ë¦¬ëŠ” ì¸ê³µì§€ëŠ¥ì˜ ì¤‘ìš”í•œ ë¶„ì•¼ì…ë‹ˆë‹¤\",\n",
        "#     \"ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì´ ìì—°ì–¸ì–´ì²˜ë¦¬ë¥¼ ë°œì „ì‹œì¼°ìŠµë‹ˆë‹¤\",\n",
        "#     \"í…ìŠ¤íŠ¸ ë§ˆì´ë‹ì€ ë¹…ë°ì´í„° ë¶„ì„ì— í™œìš©ë©ë‹ˆë‹¤\",\n",
        "#     \"ê°ì •ë¶„ì„ê³¼ ë¬¸ì„œë¶„ë¥˜ëŠ” NLPì˜ ëŒ€í‘œì ì¸ ì‘ìš©ë¶„ì•¼ì…ë‹ˆë‹¤\",\n",
        "#     \"ì±—ë´‡ê³¼ ë²ˆì—­ì‹œìŠ¤í…œì´ ì¼ìƒìƒí™œì— ë„ë¦¬ ì‚¬ìš©ë©ë‹ˆë‹¤\"\n",
        "# ]"
      ],
      "metadata": {
        "id": "Da782x9ycyqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def demonstrate_bow_basic(documents):\n",
        "    \"\"\"ê¸°ë³¸ BoW ì‹œì—°\"\"\"\n",
        "    print(\"=\" * 50)\n",
        "    print(\"âœ… 1. ê¸°ë³¸ BoW êµ¬í˜„ ì‹œì—°\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # ----------------------------\n",
        "    # BoW ëª¨ë¸ ìƒì„± ë° í•™ìŠµ\n",
        "    # ----------------------------\n",
        "    bow_model = BagOfWords(min_freq=1, remove_stopwords=True)\n",
        "    vocab = bow_model.build_vocab(documents)\n",
        "\n",
        "    print(\"\\nâ¡ï¸ êµ¬ì¶•ëœ ì–´íœ˜:\")\n",
        "    for word, freq in list(vocab.items())[:10]:\n",
        "        print(f\"  {word}: {freq}\")\n",
        "\n",
        "    # ----------------------------\n",
        "    # ë¬¸ì„œë¥¼ BoW ë²¡í„°ë¡œ ë³€í™˜\n",
        "    # ----------------------------\n",
        "    print(\"\\nâ¡ï¸ ë¬¸ì„œë³„ BoW ë²¡í„°:\")\n",
        "    for i, doc in enumerate(documents):\n",
        "        bow_vector = bow_model.text_to_bow(doc)\n",
        "        top_words = bow_model.get_top_words(bow_vector, top_n=3)\n",
        "        print(f\"\\në¬¸ì„œ {i+1}: {doc}\")\n",
        "        print(f\"BoW ë²¡í„° í˜•íƒœ: {bow_vector.shape}\")\n",
        "        print(f\"ìƒìœ„ ë‹¨ì–´: {top_words}\")\n",
        "\n",
        "    # ----------------------------\n",
        "    # ë¬¸ì„œ ê°„ ìœ ì‚¬ë„ ê³„ì‚°\n",
        "    # ----------------------------\n",
        "    print(\"\\nâ¡ï¸ ë¬¸ì„œ ê°„ ìœ ì‚¬ë„:\")\n",
        "    for i in range(len(documents)):\n",
        "        for j in range(i+1, len(documents)):\n",
        "            similarity = bow_model.calculate_similarity(documents[i], documents[j])\n",
        "            print(f\"ë¬¸ì„œ{i+1} vs ë¬¸ì„œ{j+1}: {similarity:.4f}\")\n",
        "\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸\n",
        "demonstrate_bow_basic(documents)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "zyGE8uTfcA2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2. sklearnì„ í™œìš©í•œ ê³ ê¸‰ BoW**"
      ],
      "metadata": {
        "id": "lhUeOruicSmA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def demonstrate_bow_with_sklearn(documents):\n",
        "    \"\"\"sklearnì„ í™œìš©í•œ ê³ ê¸‰ BoW\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"âœ… 2. sklearnì„ í™œìš©í•œ ê³ ê¸‰ BoW\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # ----------------------------\n",
        "    # 1. CountVectorizer (ê¸°ë³¸ BoW)\n",
        "    # ----------------------------\n",
        "    count_vectorizer = CountVectorizer()\n",
        "    count_matrix = count_vectorizer.fit_transform(documents)\n",
        "\n",
        "    print(\"â¡ï¸ CountVectorizer ê²°ê³¼:\")\n",
        "    print(f\"íŠ¹ì„± ìˆ˜: {len(count_vectorizer.get_feature_names_out())}\")\n",
        "    print(f\"í–‰ë ¬ í˜•íƒœ: {count_matrix.shape}\")\n",
        "\n",
        "    # íŠ¹ì„± ì´ë¦„ë“¤\n",
        "    feature_names = count_vectorizer.get_feature_names_out()\n",
        "    print(f\"íŠ¹ì„± ì˜ˆì‹œ: {list(feature_names)[:10]}\")\n",
        "\n",
        "    # ----------------------------\n",
        "    # 2. TF-IDF\n",
        "    # ----------------------------\n",
        "    tfidf_vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
        "\n",
        "    print(f\"\\nâ¡ï¸ TF-IDF í–‰ë ¬ í˜•íƒœ: {tfidf_matrix.shape}\")\n",
        "\n",
        "    # ë¬¸ì„œë³„ ìƒìœ„ TF-IDF ë‹¨ì–´\n",
        "    tfidf_array = tfidf_matrix.toarray()\n",
        "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "    print(\"\\nâ¡ï¸ ê° ë¬¸ì„œì˜ ìƒìœ„ TF-IDF ë‹¨ì–´:\")\n",
        "    for i, doc in enumerate(documents):\n",
        "        # í•´ë‹¹ ë¬¸ì„œì˜ TF-IDF ì ìˆ˜\n",
        "        tfidf_scores = tfidf_array[i]\n",
        "        # ìƒìœ„ 3ê°œ ë‹¨ì–´ ì¸ë±ìŠ¤\n",
        "        top_indices = np.argsort(tfidf_scores)[-3:][::-1]\n",
        "        top_words = [(feature_names[idx], tfidf_scores[idx]) for idx in top_indices if tfidf_scores[idx] > 0]\n",
        "\n",
        "        print(f\"ë¬¸ì„œ {i+1}: {top_words}\")\n",
        "\n",
        "    # ë¬¸ì„œ ê°„ ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤\n",
        "    similarity_matrix = cosine_similarity(tfidf_matrix)\n",
        "\n",
        "    print(f\"\\nâ¡ï¸ ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤ í˜•íƒœ: {similarity_matrix.shape}\")\n",
        "\n",
        "    # ìœ ì‚¬ë„ íˆíŠ¸ë§µ ì‹œê°í™”\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.rcParams[\"font.family\"] = 'NanumBarunGothic'\n",
        "    sns.heatmap(similarity_matrix,\n",
        "                annot=True,\n",
        "                cmap='Blues',\n",
        "                xticklabels=[f'ë¬¸ì„œ{i+1}' for i in range(len(documents))],\n",
        "                yticklabels=[f'ë¬¸ì„œ{i+1}' for i in range(len(documents))])\n",
        "    plt.title('ë¬¸ì„œ ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸\n",
        "demonstrate_bow_with_sklearn(documents)"
      ],
      "metadata": {
        "id": "bp8xf_w7ch5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3. BoW ë¥¼ ì´ìš©í•œ í…ìŠ¤íŠ¸ ë¶„ë¥˜**"
      ],
      "metadata": {
        "id": "ZZGCdRpNcllO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def demonstrate_bow_applications():\n",
        "    \"\"\"BoW ì‘ìš© ì˜ˆì œ\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"âœ… 3. BoW ì‹¤ì „ ì‘ìš© - í…ìŠ¤íŠ¸ ë¶„ë¥˜\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # ì¹´í…Œê³ ë¦¬ë³„ ì˜ˆì œ ë°ì´í„°\n",
        "    tech_docs = [\n",
        "        \"ì¸ê³µì§€ëŠ¥ê³¼ ë¨¸ì‹ ëŸ¬ë‹ ê¸°ìˆ ì´ ê¸‰ì†íˆ ë°œì „í•˜ê³  ìˆìŠµë‹ˆë‹¤\",\n",
        "        \"ë”¥ëŸ¬ë‹ì€ ì‹ ê²½ë§ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ê¸°ê³„í•™ìŠµ ë°©ë²•ì…ë‹ˆë‹¤\",\n",
        "        \"ìì—°ì–¸ì–´ì²˜ë¦¬ ê¸°ìˆ ë¡œ í…ìŠ¤íŠ¸ë¥¼ ìë™ìœ¼ë¡œ ë¶„ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\",\n",
        "        \"ì»´í“¨í„° ë¹„ì „ì€ ì´ë¯¸ì§€ ì¸ì‹ê³¼ ì²˜ë¦¬ì— ì‚¬ìš©ë©ë‹ˆë‹¤\"\n",
        "    ]\n",
        "\n",
        "    sports_docs = [\n",
        "        \"ì¶•êµ¬ëŠ” ì „ ì„¸ê³„ì—ì„œ ê°€ì¥ ì¸ê¸° ìˆëŠ” ìŠ¤í¬ì¸ ì…ë‹ˆë‹¤\",\n",
        "        \"ì˜¬ë¦¼í”½ì—ì„œ ë‹¤ì–‘í•œ ì¢…ëª©ì˜ ê²½ê¸°ê°€ ì—´ë¦½ë‹ˆë‹¤\",\n",
        "        \"ë†êµ¬ëŠ” íŒ€ì›Œí¬ê°€ ì¤‘ìš”í•œ ìš´ë™ì…ë‹ˆë‹¤\",\n",
        "        \"ìˆ˜ì˜ì€ ì „ì‹ ì„ ì‚¬ìš©í•˜ëŠ” ìœ ì‚°ì†Œ ìš´ë™ì…ë‹ˆë‹¤\"\n",
        "    ]\n",
        "\n",
        "    food_docs = [\n",
        "        \"í•œì‹ì€ ë°œíš¨ìŒì‹ì´ ë§ì•„ ê±´ê°•ì— ì¢‹ìŠµë‹ˆë‹¤\",\n",
        "        \"ì´íƒˆë¦¬ì•„ íŒŒìŠ¤íƒ€ëŠ” ë‹¤ì–‘í•œ ì†ŒìŠ¤ì™€ í•¨ê»˜ ì¦ê¹ë‹ˆë‹¤\",\n",
        "        \"ì¼ë³¸ ì´ˆë°¥ì€ ì‹ ì„ í•œ ì¬ë£Œê°€ í•µì‹¬ì…ë‹ˆë‹¤\",\n",
        "        \"í”„ë‘ìŠ¤ ìš”ë¦¬ëŠ” ì •êµí•œ ì¡°ë¦¬ë²•ìœ¼ë¡œ ìœ ëª…í•©ë‹ˆë‹¤\"\n",
        "    ]\n",
        "\n",
        "    # ì „ì²´ ë¬¸ì„œì™€ ë¼ë²¨\n",
        "    all_docs = tech_docs + sports_docs + food_docs\n",
        "    labels = ['ê¸°ìˆ '] * len(tech_docs) + ['ìŠ¤í¬ì¸ '] * len(sports_docs) + ['ìŒì‹'] * len(food_docs)\n",
        "\n",
        "    # TF-IDF ë²¡í„°í™”\n",
        "    vectorizer = TfidfVectorizer(max_features=100)\n",
        "    doc_vectors = vectorizer.fit_transform(all_docs)\n",
        "\n",
        "    print(f\"â¡ï¸ ë¬¸ì„œ ìˆ˜: {len(all_docs)}\")\n",
        "    print(f\"â¡ï¸ íŠ¹ì„± ìˆ˜: {doc_vectors.shape[1]}\")\n",
        "\n",
        "    # ì¹´í…Œê³ ë¦¬ë³„ ëŒ€í‘œ ë‹¨ì–´ ì¶”ì¶œ\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "    # ì¹´í…Œê³ ë¦¬ë³„ í‰ê·  TF-IDF ê³„ì‚°\n",
        "    categories = ['ê¸°ìˆ ', 'ìŠ¤í¬ì¸ ', 'ìŒì‹']\n",
        "    category_vectors = {}\n",
        "\n",
        "    for category in categories:\n",
        "        # í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì˜ ë¬¸ì„œ ì¸ë±ìŠ¤\n",
        "        category_indices = [i for i, label in enumerate(labels) if label == category]\n",
        "\n",
        "        # í•´ë‹¹ ì¹´í…Œê³ ë¦¬ ë¬¸ì„œë“¤ì˜ í‰ê·  ë²¡í„°\n",
        "        category_matrix = doc_vectors[category_indices]\n",
        "        avg_vector = np.mean(category_matrix.toarray(), axis=0)\n",
        "        category_vectors[category] = avg_vector\n",
        "\n",
        "        # ìƒìœ„ ë‹¨ì–´ë“¤\n",
        "        top_indices = np.argsort(avg_vector)[-5:][::-1]\n",
        "        top_words = [feature_names[idx] for idx in top_indices]\n",
        "\n",
        "        print(f\"\\n{category} ì¹´í…Œê³ ë¦¬ ëŒ€í‘œ ë‹¨ì–´: {top_words}\")\n",
        "\n",
        "\n",
        "    # ìƒˆë¡œìš´ ë¬¸ì„œ ë¶„ë¥˜ ì˜ˆì œ\n",
        "    test_docs = [\n",
        "        \"ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ëŠ” ë°©ë²•ì„ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤\",\n",
        "        \"ì›”ë“œì»µ ì¶•êµ¬ ê²½ê¸°ê°€ ì—´ë ¸ìŠµë‹ˆë‹¤\",\n",
        "        \"ë§›ìˆëŠ” ê¹€ì¹˜ì°Œê°œ ë ˆì‹œí”¼ë¥¼ ì†Œê°œí•©ë‹ˆë‹¤\"\n",
        "    ]\n",
        "\n",
        "    print(f\"\\nâ¡ï¸ ìƒˆë¡œìš´ ë¬¸ì„œ ë¶„ë¥˜:\")\n",
        "    test_vectors = vectorizer.transform(test_docs)\n",
        "\n",
        "    for i, test_doc in enumerate(test_docs):\n",
        "        test_vector = test_vectors[i].toarray()[0]\n",
        "\n",
        "        # ê° ì¹´í…Œê³ ë¦¬ì™€ì˜ ìœ ì‚¬ë„ ê³„ì‚°\n",
        "        similarities = {}\n",
        "        for category, category_vector in category_vectors.items():\n",
        "            similarity = cosine_similarity([test_vector], [category_vector])[0][0]\n",
        "            similarities[category] = similarity\n",
        "\n",
        "        # ê°€ì¥ ìœ ì‚¬í•œ ì¹´í…Œê³ ë¦¬\n",
        "        predicted_category = max(similarities.items(), key=lambda x: x[1])\n",
        "\n",
        "        print(f\"\\n í…ŒìŠ¤íŠ¸ ë¬¸ì„œ: {test_doc}\")\n",
        "        print(f\" ìœ ì‚¬ë„ - {similarities}\")\n",
        "        print(f\" ì˜ˆì¸¡ ì¹´í…Œê³ ë¦¬: {predicted_category[0]} (ìœ ì‚¬ë„: {predicted_category[1]:.4f})\")\n",
        "\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸\n",
        "demonstrate_bow_applications()"
      ],
      "metadata": {
        "id": "m4nikP8vovr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **4. BoW íŠ¹ì„± ë¶„ì„**"
      ],
      "metadata": {
        "id": "eNWwP0D1o9OI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWRq3G5oE3EE"
      },
      "outputs": [],
      "source": [
        "def analyze_bow_characteristics():\n",
        "    \"\"\"BoW íŠ¹ì„± ë¶„ì„\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"âœ… 4. BoW íŠ¹ì„± ë¶„ì„\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    documents = [\n",
        "        \"ê³ ì–‘ì´ê°€ ì§‘ì—ì„œ ì ì„ ì¡ë‹ˆë‹¤\",\n",
        "        \"ì§‘ì—ì„œ ê³ ì–‘ì´ê°€ ì ì„ ì¡ë‹ˆë‹¤\",  # ì–´ìˆœì´ ë‹¤ë¥¸ ê°™ì€ ì˜ë¯¸\n",
        "        \"ê°•ì•„ì§€ê°€ ê³µì›ì—ì„œ ë›°ì–´ë†‰ë‹ˆë‹¤\",\n",
        "        \"ê³µì›ì—ì„œ ê°•ì•„ì§€ê°€ ë›°ì–´ë†‰ë‹ˆë‹¤\"   # ì–´ìˆœì´ ë‹¤ë¥¸ ê°™ì€ ì˜ë¯¸\n",
        "    ]\n",
        "\n",
        "    vectorizer = CountVectorizer()\n",
        "    bow_matrix = vectorizer.fit_transform(documents)\n",
        "    print('bow_matrix type: ', type(bow_matrix))   # <class 'scipy.sparse._csr.csr_matrix'>\n",
        "\n",
        "    print(\"â¡ï¸ BoWì˜ íŠ¹ì„± ë¶„ì„:\")\n",
        "\n",
        "    print(\"\\n1. ì–´ìˆœ ë¬´ì‹œ íŠ¹ì„±:\")\n",
        "    # ì–´ìˆœì´ ë‹¤ë¥¸ ë¬¸ì„œë“¤ì˜ ë²¡í„° ë¹„êµ\n",
        "    bow_array = bow_matrix.toarray()\n",
        "    for i in range(0, len(documents), 2): # 2ê°œì”© ë¹„êµ\n",
        "        vec1 = bow_array[i]\n",
        "        vec2 = bow_array[i+1]\n",
        "        similarity = cosine_similarity([vec1], [vec2])[0][0]\n",
        "\n",
        "        print(f\"ë¬¸ì„œ {i+1}: {documents[i]}\")\n",
        "        print(f\"ë¬¸ì„œ {i+2}: {documents[i+1]}\")\n",
        "        print(f\"ìœ ì‚¬ë„: {similarity:.4f} (ì™„ì „íˆ ê°™ìŒ)\" if similarity == 1.0 else f\"ìœ ì‚¬ë„: {similarity:.4f}\")\n",
        "        print()\n",
        "\n",
        "    # í¬ì†Œì„± ë¶„ì„\n",
        "    total_elements = bow_matrix.shape[0] * bow_matrix.shape[1]\n",
        "    non_zero_elements = bow_matrix.nnz\n",
        "    sparsity = 1 - (non_zero_elements / total_elements)\n",
        "\n",
        "    print(f\"2. í¬ì†Œì„±(Sparsity) ë¶„ì„:\")\n",
        "    print(f\"ì „ì²´ ì›ì†Œ ìˆ˜: {total_elements}\")\n",
        "    print(f\"0ì´ ì•„ë‹Œ ì›ì†Œ ìˆ˜: {non_zero_elements}\")\n",
        "    print(f\"í¬ì†Œì„±: {sparsity:.4f} ({sparsity*100:.2f}%)\")\n",
        "\n",
        "    # ì°¨ì› ë¶„ì„\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    print(f\"\\n3. ì°¨ì› ë¶„ì„:\")\n",
        "    print(f\"ë¬¸ì„œ ìˆ˜: {bow_matrix.shape[0]}\")\n",
        "    print(f\"íŠ¹ì„±(ë‹¨ì–´) ìˆ˜: {bow_matrix.shape[1]}\")\n",
        "    print(f\"íŠ¹ì„±ë“¤: {list(feature_names)}\")\n",
        "\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸\n",
        "analyze_bow_characteristics()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "mvkW2Wb_HoWj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. TF-IDF(Term Frequency-Inverse Document Frequency)**\n",
        "- TF-IDF :\n",
        "    - ë¬¸ì„œ ë‚´ì—ì„œ ë‹¨ì–´ì˜ ì¤‘ìš”ë„ë¥¼ ì¸¡ì •í•˜ëŠ” ê°€ì¤‘ì¹˜ ê¸°ë²•ìœ¼ë¡œ, ë‹¨ì–´ ë¹ˆë„(TF)ì™€ ì—­ë¬¸ì„œ ë¹ˆë„(IDF)ë¥¼ ê²°í•©í•˜ì—¬ ê° ë‹¨ì–´ê°€ íŠ¹ì • ë¬¸ì„œì—ì„œ ì–¼ë§ˆë‚˜ ì¤‘ìš”í•œì§€ë¥¼ ìˆ˜ì¹˜í™”í•¨\n",
        "    - TF-IDF ê°’ì´ ë‚®ìœ¼ë©´ ì¤‘ìš”ë„ê°€ ë‚®ì€ ê²ƒ, TF-IDF ê°’ì´ í¬ë©´ ì¤‘ìš”ë„ê°€ í° ê²ƒ\n",
        "    - ì°¸ê³  : https://wikidocs.net/31698\n",
        "- ë“±ì¥ ë°°ê²½\n",
        "    - ë‹¨ìˆœí•œ ë‹¨ì–´ ë¹ˆë„ë§Œìœ¼ë¡œëŠ” ë¬¸ì„œì˜ ì£¼ì œë¥¼ ì •í™•íˆ íŒŒì•…í•˜ê¸° ì–´ë ¤ì› ìŒ\n",
        "    - ì˜ˆë¥¼ ë“¤ì–´ \"ê·¸ë¦¬ê³ \", \"ê·¸ëŸ¬ë‚˜\" ê°™ì€ ë‹¨ì–´ëŠ” ìì£¼ ë‚˜íƒ€ë‚˜ì§€ë§Œ ë¬¸ì„œì˜ ë‚´ìš©ê³¼ëŠ” ê´€ë ¨ì´ ì ìŒ. --> TF-IDFëŠ” ì´ëŸ° ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ê°œë°œ"
      ],
      "metadata": {
        "id": "psOBzjYiM5AD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ê¸°ë³¸ TF-IDF ê³„ì‚° ê³¼ì •:**\n",
        "\n",
        "1. **ë‹¨ì–´ ë¹ˆë„ (TF) ê³„ì‚°**\n",
        "   - Raw TF: $tf(t,d) = f(t,d)$\n",
        "   - Relative TF: $tf(t,d) = f(t,d) / Î£f(w,d)$\n",
        "   - Log TF: $tf(t,d) = 1 + log(f(t,d))$ (f(t,d) > 0ì¸ ê²½ìš°)\n",
        "\n",
        "2. **ì—­ë¬¸ì„œ ë¹ˆë„ (IDF) ê³„ì‚°**\n",
        "   - Basic IDF: $idf(t) = log(N / df(t))$\n",
        "   - Smooth IDF: $idf(t) = log(N / (1 + df(t)))$\n",
        "   - Probabilistic IDF: $idf(t) = log((N - df(t)) / df(t))$\n",
        "\n",
        "3. **ìµœì¢… TF-IDF ê°€ì¤‘ì¹˜**\n",
        "   -   $w(t,d) = tf(t,d) Ã— idf(t)$\n",
        "\n",
        "ì—¬ê¸°ì„œ:\n",
        "- t: ë‹¨ì–´ (term)\n",
        "- d: ë¬¸ì„œ (document)\n",
        "- f(t,d): ë¬¸ì„œ dì—ì„œ ë‹¨ì–´ tì˜ ë¹ˆë„\n",
        "- N: ì „ì²´ ë¬¸ì„œ ìˆ˜\n",
        "- df(t): ë‹¨ì–´ tê°€ í¬í•¨ëœ ë¬¸ì„œ ìˆ˜"
      ],
      "metadata": {
        "id": "bdb83S7QNkus"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ì •ê·œí™” ê³µì‹**\n",
        "|ì •ê·œí™” ë°©ë²•|ê³µì‹|ëª©ì |\n",
        "|------|---|---|\n",
        "|L1 ì •ê·œí™” | `w'(t,d) = w(t,d) / Î£ | w(i,d)  |\n",
        "|L2 ì •ê·œí™”| w'(t,d) = w(t,d) / âˆš(Î£w(i,d)Â²)| ë²¡í„°ì˜ í¬ê¸°ê°€ 1ì´ ë˜ë„ë¡ ì¡°ì •|\n",
        "|ì½”ì‚¬ì¸ ì •ê·œí™”| `sim(d1,d2) = (d1Â·d2) / (| d1 |"
      ],
      "metadata": {
        "id": "hqrlC2fEPDE_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ì˜ˆì œ : TF-IDF ì‹¤ì „ ì‘ìš© - ë¬¸ì„œ ë¶„ë¥˜ ë° í´ëŸ¬ìŠ¤í„°ë§**"
      ],
      "metadata": {
        "id": "R0jhrNRFSKZ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **TF-IDF Basic Class(ì§ì ‘ êµ¬í˜„)**"
      ],
      "metadata": {
        "id": "VHwtaSi8GetF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter, defaultdict\n",
        "import re\n",
        "import math\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cluster import KMeans\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# TF-IDF(ìˆ˜ì‹êµ¬í˜„ í´ë˜ìŠ¤)\n",
        "# ----------------------------\n",
        "class TFIDFImplementation:\n",
        "    def __init__(self, use_log_tf=True, smooth_idf=True, normalize=True):\n",
        "        \"\"\"\n",
        "        TF-IDF ì™„ì „ êµ¬í˜„ í´ë˜ìŠ¤\n",
        "\n",
        "        Args:\n",
        "            use_log_tf: ë¡œê·¸ TF ì‚¬ìš© ì—¬ë¶€\n",
        "            smooth_idf: ìŠ¤ë¬´ìŠ¤ IDF ì‚¬ìš© ì—¬ë¶€\n",
        "            normalize: L2 ì •ê·œí™” ì‚¬ìš© ì—¬ë¶€\n",
        "        \"\"\"\n",
        "        self.use_log_tf = use_log_tf\n",
        "        self.smooth_idf = smooth_idf\n",
        "        self.normalize = normalize\n",
        "        self.vocab = {}\n",
        "        self.idf_values = {}\n",
        "        self.documents = []\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬\"\"\"\n",
        "        # ì†Œë¬¸ì ë³€í™˜ ë° íŠ¹ìˆ˜ë¬¸ì ì œê±° (í•œê¸€ ë³´ì¡´)\n",
        "        text = re.sub(r'[^\\w\\sê°€-í£]', '', text.lower())\n",
        "        return [word for word in text.split() if word.strip()]\n",
        "\n",
        "    def build_vocabulary(self, documents):\n",
        "        \"\"\"ì–´íœ˜ êµ¬ì¶• ë° IDF ê³„ì‚°\"\"\"\n",
        "        self.documents = documents\n",
        "\n",
        "        # ì „ì²´ ì–´íœ˜ ìˆ˜ì§‘\n",
        "        all_words = set()\n",
        "        processed_docs = []\n",
        "\n",
        "        for doc in documents:\n",
        "            words = self.preprocess_text(doc)\n",
        "            processed_docs.append(words)\n",
        "            all_words.update(words)\n",
        "\n",
        "        # ì–´íœ˜ ì¸ë±ì‹±\n",
        "        self.vocab = {word: idx for idx, word in enumerate(sorted(all_words))}\n",
        "        vocab_size = len(self.vocab)\n",
        "\n",
        "        # ê° ë‹¨ì–´ê°€ í¬í•¨ëœ ë¬¸ì„œ ìˆ˜ ê³„ì‚°\n",
        "        doc_freq = defaultdict(int)\n",
        "        for words in processed_docs:\n",
        "            unique_words = set(words)\n",
        "            for word in unique_words:\n",
        "                doc_freq[word] += 1\n",
        "\n",
        "        # IDF ê³„ì‚°\n",
        "        total_docs = len(documents)\n",
        "        for word in self.vocab:\n",
        "            if self.smooth_idf:\n",
        "                self.idf_values[word] = math.log(total_docs / (1 + doc_freq[word]))\n",
        "            else:\n",
        "                self.idf_values[word] = math.log(total_docs / doc_freq[word]) if doc_freq[word] > 0 else 0\n",
        "\n",
        "        print(f\"ì–´íœ˜ í¬ê¸°: {vocab_size}\")\n",
        "        print(f\"ë¬¸ì„œ ìˆ˜: {total_docs}\")\n",
        "\n",
        "        return processed_docs\n",
        "\n",
        "    def calculate_tf(self, words):\n",
        "        \"\"\"TF ê³„ì‚°\"\"\"\n",
        "        word_count = Counter(words)\n",
        "        total_words = len(words)\n",
        "\n",
        "        tf_values = {}\n",
        "        for word in word_count:\n",
        "            if self.use_log_tf:\n",
        "                tf_values[word] = 1 + math.log(word_count[word])\n",
        "            else:\n",
        "                tf_values[word] = word_count[word] / total_words if total_words > 0 else 0\n",
        "\n",
        "        return tf_values\n",
        "\n",
        "    def document_to_vector(self, document):\n",
        "        \"\"\"ë¬¸ì„œë¥¼ TF-IDF ë²¡í„°ë¡œ ë³€í™˜\"\"\"\n",
        "        words = self.preprocess_text(document)\n",
        "        tf_values = self.calculate_tf(words)\n",
        "\n",
        "        # TF-IDF ë²¡í„° ìƒì„±\n",
        "        vector = np.zeros(len(self.vocab))\n",
        "\n",
        "        for word, tf in tf_values.items():\n",
        "            if word in self.vocab:\n",
        "                idx = self.vocab[word]\n",
        "                tfidf_value = tf * self.idf_values[word]\n",
        "                vector[idx] = tfidf_value\n",
        "\n",
        "        # L2 ì •ê·œí™”\n",
        "        if self.normalize:\n",
        "            norm = np.linalg.norm(vector)\n",
        "            if norm > 0:\n",
        "                vector = vector / norm\n",
        "\n",
        "        return vector\n",
        "\n",
        "    def fit_transform(self, documents):\n",
        "        \"\"\"ë¬¸ì„œë“¤ì„ TF-IDF í–‰ë ¬ë¡œ ë³€í™˜\"\"\"\n",
        "        processed_docs = self.build_vocabulary(documents)\n",
        "\n",
        "        # ëª¨ë“  ë¬¸ì„œë¥¼ ë²¡í„°ë¡œ ë³€í™˜\n",
        "        tfidf_matrix = []\n",
        "        for doc in documents:\n",
        "            vector = self.document_to_vector(doc)\n",
        "            tfidf_matrix.append(vector)\n",
        "\n",
        "        return np.array(tfidf_matrix)\n",
        "\n",
        "    def get_top_words(self, document, top_n=10):\n",
        "        \"\"\"ë¬¸ì„œì—ì„œ TF-IDF ê°’ì´ ë†’ì€ ë‹¨ì–´ë“¤ ì¶”ì¶œ\"\"\"\n",
        "        vector = self.document_to_vector(document)\n",
        "        word_scores = []\n",
        "\n",
        "        for word, idx in self.vocab.items():\n",
        "            if vector[idx] > 0:\n",
        "                word_scores.append((word, vector[idx]))\n",
        "\n",
        "        word_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "        return word_scores[:top_n]\n",
        "\n",
        "    def get_feature_names(self):\n",
        "        \"\"\"íŠ¹ì„±(ë‹¨ì–´) ì´ë¦„ ë°˜í™˜\"\"\"\n",
        "        return [word for word, _ in sorted(self.vocab.items(), key=lambda x: x[1])]\n"
      ],
      "metadata": {
        "id": "nT9xJkmXPdnC",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì˜ˆì œ ë¬¸ì„œë“¤\n",
        "documents = [\n",
        "    \"ì¸ê³µì§€ëŠ¥ì€ ì»´í“¨í„° ê³¼í•™ì˜ í•œ ë¶„ì•¼ë¡œ ê¸°ê³„ê°€ ì¸ê°„ì˜ ì§€ëŠ¥ì„ ëª¨ë°©í•˜ë„ë¡ í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤\",\n",
        "    \"ë¨¸ì‹ ëŸ¬ë‹ì€ ì¸ê³µì§€ëŠ¥ì˜ í•˜ìœ„ ë¶„ì•¼ë¡œ ë°ì´í„°ë¡œë¶€í„° í•™ìŠµí•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì„ ë‹¤ë£¹ë‹ˆë‹¤\",\n",
        "    \"ë”¥ëŸ¬ë‹ì€ ë¨¸ì‹ ëŸ¬ë‹ì˜ í•œ ë°©ë²•ìœ¼ë¡œ ì‹ ê²½ë§ì„ ì´ìš©í•˜ì—¬ ë³µì¡í•œ íŒ¨í„´ì„ í•™ìŠµí•©ë‹ˆë‹¤\",\n",
        "    \"ìì—°ì–¸ì–´ì²˜ë¦¬ëŠ” ì»´í“¨í„°ê°€ ì¸ê°„ì˜ ì–¸ì–´ë¥¼ ì´í•´í•˜ê³  ìƒì„±í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤\",\n",
        "    \"ì»´í“¨í„° ë¹„ì „ì€ ì»´í“¨í„°ê°€ ì´ë¯¸ì§€ì™€ ë¹„ë””ì˜¤ë¥¼ ë¶„ì„í•˜ê³  ì´í•´í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤\"\n",
        "]\n",
        "\n",
        "# documents = [\n",
        "#     \"ìì—°ì–¸ì–´ì²˜ë¦¬ ê¸°ìˆ ì´ ë¹ ë¥´ê²Œ ë°œì „í•˜ê³  ìˆìŠµë‹ˆë‹¤\",\n",
        "#     \"ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì´ AI ë°œì „ì„ ì´ëŒê³  ìˆìŠµë‹ˆë‹¤\",\n",
        "#     \"ë¹…ë°ì´í„° ë¶„ì„ì— ë‹¤ì–‘í•œ ê¸°ìˆ ì´ í™œìš©ë©ë‹ˆë‹¤\",\n",
        "#     \"í´ë¼ìš°ë“œ ì»´í“¨íŒ…ì´ IT ì¸í”„ë¼ë¥¼ í˜ì‹ í•˜ê³  ìˆìŠµë‹ˆë‹¤\",\n",
        "#     \"ì‚¬ë¬¼ì¸í„°ë„·ê³¼ ìŠ¤ë§ˆíŠ¸ì‹œí‹°ê°€ ì£¼ëª©ë°›ê³  ìˆìŠµë‹ˆë‹¤\"\n",
        "# ]"
      ],
      "metadata": {
        "id": "B_LXWLeh4tXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1. Basic TF-IDF êµ¬í˜„**"
      ],
      "metadata": {
        "id": "DI-sVtmXG72g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 1. ê¸°ë³¸ êµ¬í˜„ : TF-IDF(ìˆ˜ì‹êµ¬í˜„ í´ë˜ìŠ¤) í–‰ë ¬ ìƒì„±\n",
        "# ----------------------------\n",
        "def demonstrate_basic_tfidf():\n",
        "    \"\"\"ê¸°ë³¸ TF-IDF êµ¬í˜„ ì‹œì—°\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"âœ… 1. ê¸°ë³¸ TF-IDF êµ¬í˜„ ì‹œì—°\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # TF-IDF ëª¨ë¸ ìƒì„± ë° í•™ìŠµ\n",
        "    tfidf_model = TFIDFImplementation()\n",
        "    tfidf_matrix = tfidf_model.fit_transform(documents)\n",
        "\n",
        "    print(f\"TF-IDF í–‰ë ¬ í˜•íƒœ: {tfidf_matrix.shape}\")\n",
        "\n",
        "    # ê° ë¬¸ì„œë³„ ìƒìœ„ ë‹¨ì–´ë“¤\n",
        "    print(\"\\nê° ë¬¸ì„œë³„ ìƒìœ„ TF-IDF ë‹¨ì–´:\")\n",
        "    for i, doc in enumerate(documents):\n",
        "        top_words = tfidf_model.get_top_words(doc, top_n=5)\n",
        "        print(f\"\\në¬¸ì„œ {i+1}: {doc[:30]}...\")\n",
        "        for word, score in top_words:\n",
        "            print(f\"  {word}: {score:.4f}\")\n",
        "\n",
        "    # IDF ê°’ ë¶„ì„\n",
        "    print(f\"\\nìƒìœ„ IDF ê°’ (í¬ê·€í•œ ë‹¨ì–´ë“¤):\")\n",
        "    idf_sorted = sorted(tfidf_model.idf_values.items(), key=lambda x: x[1], reverse=True)\n",
        "    for word, idf in idf_sorted[:10]:\n",
        "        print(f\"  {word}: {idf:.4f}\")\n",
        "\n",
        "    return tfidf_model, tfidf_matrix\n",
        "\n",
        "\n",
        "model1, matrix1 = demonstrate_basic_tfidf()"
      ],
      "metadata": {
        "id": "bWJr_NiA2swd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2. sklearn ì œê³µ TF-IDF êµ¬í˜„**"
      ],
      "metadata": {
        "id": "-fbgqwtWHS_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 2. sklearn ë¹„êµ : TF-IDF(sklearn) í–‰ë ¬ ìƒì„±\n",
        "# ----------------------------\n",
        "def demonstrate_sklearn_tfidf():\n",
        "    \"\"\"sklearn TF-IDFì™€ ë¹„êµ ì‹œì—°\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"âœ… 2. sklearn TF-IDFì™€ ì„±ëŠ¥ ë¹„êµ\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # sklearn TfidfVectorizer\n",
        "    sklearn_tfidf = TfidfVectorizer()\n",
        "    sklearn_matrix = sklearn_tfidf.fit_transform(documents)\n",
        "\n",
        "    print(\"sklearn TF-IDF:\")\n",
        "    print(f\"í–‰ë ¬ í˜•íƒœ: {sklearn_matrix.shape}\")\n",
        "    print(f\"íŠ¹ì„± ìˆ˜: {len(sklearn_tfidf.get_feature_names_out())}\")\n",
        "\n",
        "    # ë¬¸ì„œë³„ ìƒìœ„ ë‹¨ì–´ (sklearn)\n",
        "    feature_names = sklearn_tfidf.get_feature_names_out()\n",
        "    sklearn_array = sklearn_matrix.toarray()\n",
        "\n",
        "    print(f\"\\nsklearn ìƒìœ„ TF-IDF ë‹¨ì–´ë“¤:\")\n",
        "    for i in range(len(documents)):\n",
        "        tfidf_scores = sklearn_array[i]\n",
        "        top_indices = np.argsort(tfidf_scores)[-5:][::-1]\n",
        "        top_words = [(feature_names[idx], tfidf_scores[idx]) for idx in top_indices if tfidf_scores[idx] > 0]\n",
        "        print(f\"ë¬¸ì„œ {i+1}: {top_words}\")\n",
        "\n",
        "    return sklearn_tfidf, sklearn_matrix\n",
        "\n",
        "model2, matrix2 = demonstrate_sklearn_tfidf()"
      ],
      "metadata": {
        "id": "A4GNl-B42s0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3 .Basic TF-IDF vs sklearn TF-IDF ë¹„êµ**"
      ],
      "metadata": {
        "id": "YdLor_wNFM4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 3. Basic TF-IDF vs sklearn TF-IDF ë¹„êµ\n",
        "# ----------------------------\n",
        "import numpy as np\n",
        "from scipy.sparse import issparse\n",
        "\n",
        "def _dense_row(mat, i):\n",
        "    \"\"\"ië²ˆì§¸ ë¬¸ì„œì˜ TF-IDF ë²¡í„°ë¥¼ 1D numpy arrayë¡œ ë°˜í™˜ (sparse/dense ëª¨ë‘ ì§€ì›).\"\"\"\n",
        "    if issparse(mat):\n",
        "        return mat.getrow(i).toarray().ravel()\n",
        "    arr = np.asarray(mat)\n",
        "    return arr[i].ravel()\n",
        "\n",
        "def _idf_dict_from_sklearn(model2):\n",
        "    \"\"\"sklearn TfidfVectorizerì—ì„œ {term: idf} ì‚¬ì „ ì–»ê¸°.\"\"\"\n",
        "    terms = model2.get_feature_names_out().tolist()\n",
        "    idfs = model2.idf_.tolist()\n",
        "    return dict(zip(terms, idfs))\n",
        "\n",
        "def _weights_dict_from_sklearn_doc(model2, matrix2, doc_idx):\n",
        "    \"\"\"sklearnì—ì„œ íŠ¹ì • ë¬¸ì„œì˜ TF-IDFë¥¼ {term: weight} ì‚¬ì „ìœ¼ë¡œ ë³€í™˜.\"\"\"\n",
        "    terms = model2.get_feature_names_out().tolist()\n",
        "    row = _dense_row(matrix2, doc_idx)\n",
        "    return {t: float(w) for t, w in zip(terms, row) if w != 0.0}\n",
        "\n",
        "def _weights_dict_from_basic_doc(model1, doc_text, max_terms=1_000_000):\n",
        "    \"\"\"\n",
        "    ì‚¬ìš©ì êµ¬í˜„ TFIDFImplementationì—ì„œ ë¬¸ì„œë³„ TF-IDFë¥¼ {term: weight}ë¡œ ì¶”ì¶œ.\n",
        "    get_top_words(doc, top_n=...)ê°€ ìˆëŠ” ì „í˜•ì  êµ¬í˜„ì„ ê°€ì •.\n",
        "    \"\"\"\n",
        "    if hasattr(model1, \"get_top_words\"):\n",
        "        pairs = model1.get_top_words(doc_text, top_n=max_terms)  # [(term, score), ...]\n",
        "        return {t: float(s) for t, s in pairs if s != 0.0}\n",
        "    # fallback: ì‚¬ìš©ìê°€ feature_names_ ê°™ì€ ì†ì„±ì„ ì œê³µí•œ ê²½ìš° í™•ì¥ ê°€ëŠ¥\n",
        "    raise AttributeError(\"model1ì—ì„œ ë¬¸ì„œë³„ ê°€ì¤‘ì¹˜ë¥¼ ì–»ëŠ” ë°©ë²•(get_top_words)ì´ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
        "\n",
        "def _idf_stats_from_basic(model1):\n",
        "    \"\"\"ì‚¬ìš©ì êµ¬í˜„ ëª¨ë¸ì—ì„œ IDF í†µê³„ë¥¼ ê³„ì‚° (idf_values: {term: idf} ê°€ì •).\"\"\"\n",
        "    if hasattr(model1, \"idf_values\") and isinstance(model1.idf_values, dict):\n",
        "        vals = np.array(list(model1.idf_values.values()), dtype=float)\n",
        "        return {\n",
        "            \"count\": int(vals.size),\n",
        "            \"mean\": float(vals.mean()) if vals.size else np.nan,\n",
        "            \"std\": float(vals.std(ddof=0)) if vals.size else np.nan,\n",
        "            \"min\": float(vals.min()) if vals.size else np.nan,\n",
        "            \"max\": float(vals.max()) if vals.size else np.nan,\n",
        "        }\n",
        "    return {\"count\": 0, \"mean\": np.nan, \"std\": np.nan, \"min\": np.nan, \"max\": np.nan}\n",
        "\n",
        "def _idf_stats_from_sklearn(model2):\n",
        "    vals = np.array(model2.idf_, dtype=float)\n",
        "    return {\n",
        "        \"count\": int(vals.size),\n",
        "        \"mean\": float(vals.mean()),\n",
        "        \"std\": float(vals.std(ddof=0)),\n",
        "        \"min\": float(vals.min()),\n",
        "        \"max\": float(vals.max()),\n",
        "    }\n",
        "\n",
        "def _pearson_on_common_terms(idf_map1, idf_map2):\n",
        "    \"\"\"ê³µí†µ ìš©ì–´ì˜ IDF í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜ (ìš©ì–´ ë¶ˆì¼ì¹˜ ë§ì•„ë„ ì•ˆì „).\"\"\"\n",
        "    common = set(idf_map1.keys()) & set(idf_map2.keys())\n",
        "    if not common:\n",
        "        return {\"n_common\": 0, \"pearson\": np.nan}\n",
        "    v1 = np.array([idf_map1[t] for t in common], dtype=float)\n",
        "    v2 = np.array([idf_map2[t] for t in common], dtype=float)\n",
        "    if v1.std() == 0 or v2.std() == 0:\n",
        "        return {\"n_common\": int(len(common)), \"pearson\": np.nan}\n",
        "    pearson = float(np.corrcoef(v1, v2)[0, 1])\n",
        "    return {\"n_common\": int(len(common)), \"pearson\": pearson}\n",
        "\n",
        "def _topk_overlap_and_jaccard(weights1, weights2, k):\n",
        "    \"\"\"ê° ë¬¸ì„œì—ì„œ Top-K ë‹¨ì–´ ê²¹ì¹¨ê³¼ ìì¹´ë“œ.\"\"\"\n",
        "    top1 = [t for t, _ in sorted(weights1.items(), key=lambda x: x[1], reverse=True)[:k]]\n",
        "    top2 = [t for t, _ in sorted(weights2.items(), key=lambda x: x[1], reverse=True)[:k]]\n",
        "    s1, s2 = set(top1), set(top2)\n",
        "    inter = s1 & s2\n",
        "    union = s1 | s2\n",
        "    jaccard = (len(inter) / len(union)) if union else np.nan\n",
        "    return {\n",
        "        \"top1\": top1,\n",
        "        \"top2\": top2,\n",
        "        \"overlap\": sorted(inter),\n",
        "        \"jaccard\": float(jaccard),\n",
        "    }\n",
        "\n",
        "def _cosine_from_weight_dicts(w1, w2):\n",
        "    \"\"\"ë‘ ê°€ì¤‘ì¹˜ ì‚¬ì „({term: weight})ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„.\"\"\"\n",
        "    if not w1 and not w2:\n",
        "        return np.nan\n",
        "    keys = set(w1.keys()) | set(w2.keys())\n",
        "    a = np.array([w1.get(k, 0.0) for k in keys], dtype=float)\n",
        "    b = np.array([w2.get(k, 0.0) for k in keys], dtype=float)\n",
        "    na = np.linalg.norm(a)\n",
        "    nb = np.linalg.norm(b)\n",
        "    if na == 0.0 or nb == 0.0:\n",
        "        return 0.0\n",
        "    return float(a.dot(b) / (na * nb))\n",
        "\n",
        "def compare_tfidf_models(model1, matrix1, model2, matrix2, documents, top_k=5):\n",
        "    \"\"\"\n",
        "    ì‚¬ìš©ì êµ¬í˜„ TF-IDF(model1/matrix1)ê³¼ sklearn TF-IDF(model2/matrix2)ë¥¼ ë¹„êµ.\n",
        "    - shapes, vocab size\n",
        "    - IDF í†µê³„ / ê³µí†µ ìš©ì–´ IDF ìƒê´€\n",
        "    - ë¬¸ì„œë³„ Top-K ê²¹ì¹¨(Jaccard)\n",
        "    - ë¬¸ì„œë³„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„(ì‚¬ì „ ê¸°ë°˜)\n",
        "    \"\"\"\n",
        "    \"\"\"sklearn TF-IDFì™€ ë¹„êµ ì‹œì—°\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"âœ… 3. ì‚¬ìš©ì êµ¬í˜„ TF-IDF(model1/matrix1)ê³¼ sklearn TF-IDF(model2/matrix2)ë¥¼ ë¹„êµ\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "\n",
        "    # 1) ê¸°ë³¸ ë©”íƒ€ ë¹„êµ\n",
        "    shape1 = getattr(matrix1, \"shape\", None)\n",
        "    shape2 = getattr(matrix2, \"shape\", None)\n",
        "\n",
        "    # vocab size ì¶”ì •\n",
        "    # model1: idf_valuesê°€ ìˆìœ¼ë©´ ê·¸ í¬ê¸°ë¥¼ ì‚¬ìš©, ì—†ìœ¼ë©´ í–‰ë ¬ ì—´ í¬ê¸° ì‚¬ìš©\n",
        "    if hasattr(model1, \"idf_values\") and isinstance(model1.idf_values, dict):\n",
        "        vocab1_size = len(model1.idf_values)\n",
        "        idf_map1 = dict(model1.idf_values)\n",
        "    else:\n",
        "        vocab1_size = shape1[1] if shape1 else None\n",
        "        idf_map1 = {}\n",
        "\n",
        "    vocab2_size = len(model2.get_feature_names_out())\n",
        "    idf_map2 = _idf_dict_from_sklearn(model2)\n",
        "\n",
        "    # 2) IDF í†µê³„ ë° ìƒê´€\n",
        "    idf_stats1 = _idf_stats_from_basic(model1)\n",
        "    idf_stats2 = _idf_stats_from_sklearn(model2)\n",
        "    idf_corr = _pearson_on_common_terms(idf_map1, idf_map2) if idf_map1 else {\"n_common\": 0, \"pearson\": np.nan}\n",
        "\n",
        "    # 3) ë¬¸ì„œë³„ ë¹„êµ\n",
        "    docwise = []\n",
        "    for i, doc in enumerate(documents):\n",
        "        try:\n",
        "            w1 = _weights_dict_from_basic_doc(model1, doc, max_terms=1_000_000)\n",
        "        except Exception:\n",
        "            # fallback: í–‰ë ¬ë§Œìœ¼ë¡œëŠ” ìš©ì–´-ì—´ ë§¤í•‘ì„ ëª¨ë¥´ë©´ ì‚¬ì „ êµ¬ì„± ë¶ˆê°€\n",
        "            w1 = {}\n",
        "\n",
        "        w2 = _weights_dict_from_sklearn_doc(model2, matrix2, i)\n",
        "\n",
        "        overlap = _topk_overlap_and_jaccard(w1, w2, top_k)\n",
        "        cosine = _cosine_from_weight_dicts(w1, w2)\n",
        "\n",
        "        docwise.append({\n",
        "            \"doc_index\": i,\n",
        "            \"doc_preview\": doc[:40] + (\"...\" if len(doc) > 40 else \"\"),\n",
        "            \"topk_overlap\": overlap,\n",
        "            \"cosine_similarity\": cosine,\n",
        "            \"nonzero_terms_basic\": len(w1),\n",
        "            \"nonzero_terms_sklearn\": len(w2),\n",
        "        })\n",
        "\n",
        "    return {\n",
        "        \"matrix_shapes\": {\"basic\": shape1, \"sklearn\": shape2},\n",
        "        \"vocab_sizes\": {\"basic\": vocab1_size, \"sklearn\": vocab2_size},\n",
        "        \"idf_stats\": {\"basic\": idf_stats1, \"sklearn\": idf_stats2},\n",
        "        \"idf_common_correlation\": idf_corr,\n",
        "        \"docwise\": docwise,\n",
        "    }\n",
        "\n",
        "def print_comparison_report(result, top_k=5):\n",
        "    \"\"\"compare_tfidf_models ê²°ê³¼ë¥¼ ê°„ë‹¨íˆ ì¶œë ¥.\"\"\"\n",
        "    print(\"=\"*70)\n",
        "    print(\"ğŸ” TF-IDF ëª¨ë¸ ë¹„êµ ë¦¬í¬íŠ¸\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"- í–‰ë ¬ í¬ê¸°: basic={result['matrix_shapes']['basic']}, sklearn={result['matrix_shapes']['sklearn']}\")\n",
        "    print(f\"- ì–´íœ˜ìˆ˜:    basic={result['vocab_sizes']['basic']}, sklearn={result['vocab_sizes']['sklearn']}\")\n",
        "    b = result[\"idf_stats\"][\"basic\"]; s = result[\"idf_stats\"][\"sklearn\"]\n",
        "    print(\"\\n[IDF ìš”ì•½ í†µê³„]\")\n",
        "    print(f\"  â€¢ basic   | count={b['count']}, mean={b['mean']:.4f}, std={b['std']:.4f}, min={b['min']:.4f}, max={b['max']:.4f}\")\n",
        "    print(f\"  â€¢ sklearn | count={s['count']}, mean={s['mean']:.4f}, std={s['std']:.4f}, min={s['min']:.4f}, max={s['max']:.4f}\")\n",
        "    corr = result[\"idf_common_correlation\"]\n",
        "    print(f\"  â€¢ ê³µí†µ ìš©ì–´ IDF ìƒê´€ (Pearson): n={corr['n_common']}, r={corr['pearson']:.4f}\" if corr[\"n_common\"] else\n",
        "          \"  â€¢ ê³µí†µ ìš©ì–´ IDF ìƒê´€: ê³µí†µ ìš©ì–´ê°€ ì—†ì–´ ê³„ì‚° ë¶ˆê°€\")\n",
        "\n",
        "    print(\"\\n[ë¬¸ì„œë³„ Top-{} & ì½”ì‚¬ì¸ ìœ ì‚¬ë„]\".format(top_k))\n",
        "    for item in result[\"docwise\"]:\n",
        "        ov = item[\"topk_overlap\"]\n",
        "        print(\"-\"*70)\n",
        "        print(f\"ë¬¸ì„œ {item['doc_index']+1}: {item['doc_preview']}\")\n",
        "        print(f\"  Â· nonzero terms | basic={item['nonzero_terms_basic']}, sklearn={item['nonzero_terms_sklearn']}\")\n",
        "        print(f\"  Â· cosine(sim)   | {item['cosine_similarity']:.4f}\")\n",
        "        print(f\"  Â· top{top_k} overlap({len(ov['overlap'])}ê°œ, jaccard={ov['jaccard']:.3f}): {ov['overlap']}\")\n"
      ],
      "metadata": {
        "id": "GIZThDXiFdau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì•ì„œ ë§Œë“  model1, matrix1, model2, matrix2, documents ê°€ ì¤€ë¹„ë˜ì–´ ìˆë‹¤ê³  ê°€ì •\n",
        "result = compare_tfidf_models(model1, matrix1, model2, matrix2, documents, top_k=5)\n",
        "print_comparison_report(result, top_k=5)\n"
      ],
      "metadata": {
        "id": "yDjw4MIQFeq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **4. TF-IDFë¡œ ë¬¸ì„œ ë¶„ë¥˜ ë° í´ëŸ¬ìŠ¤í„°ë§**"
      ],
      "metadata": {
        "id": "WrXqxgjBH3Tj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 4. TF-IDFë¡œ ë¬¸ì„œ ë¶„ë¥˜ ë° í´ëŸ¬ìŠ¤í„°ë§\n",
        "# ----------------------------\n",
        "def demonstrate_tfidf_applications():\n",
        "    \"\"\"TF-IDF ì‹¤ì „ ì‘ìš© ì˜ˆì œ\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"âœ… 4. TF-IDF ì‹¤ì „ ì‘ìš© - ë¬¸ì„œ ë¶„ë¥˜ ë° í´ëŸ¬ìŠ¤í„°ë§\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # ì¹´í…Œê³ ë¦¬ë³„ ë¬¸ì„œ ë°ì´í„°\n",
        "    tech_docs = [\n",
        "        \"ì¸ê³µì§€ëŠ¥ê³¼ ë¨¸ì‹ ëŸ¬ë‹ ê¸°ìˆ ì´ ê¸‰ì†íˆ ë°œì „í•˜ê³  ìˆì–´ ë‹¤ì–‘í•œ ì‚°ì—…ì— í˜ì‹ ì„ ê°€ì ¸ì˜¤ê³  ìˆìŠµë‹ˆë‹¤\",\n",
        "        \"ë”¥ëŸ¬ë‹ì€ ì‹ ê²½ë§ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ê¸°ê³„í•™ìŠµ ë°©ë²•ìœ¼ë¡œ ì´ë¯¸ì§€ ì¸ì‹ê³¼ ìì—°ì–¸ì–´ì²˜ë¦¬ì— ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤\",\n",
        "        \"ë¹…ë°ì´í„° ë¶„ì„ ê¸°ìˆ ì„ í†µí•´ ê¸°ì—…ë“¤ì€ ê³ ê°ì˜ í–‰ë™ íŒ¨í„´ì„ íŒŒì•…í•˜ê³  ë§ì¶¤í˜• ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤\",\n",
        "        \"í´ë¼ìš°ë“œ ì»´í“¨íŒ… ê¸°ìˆ ë¡œ ê¸°ì—…ë“¤ì€ IT ì¸í”„ë¼ ë¹„ìš©ì„ ì ˆê°í•˜ë©´ì„œë„ í™•ì¥ì„±ì„ í™•ë³´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\"\n",
        "    ]\n",
        "\n",
        "    business_docs = [\n",
        "        \"ìŠ¤íƒ€íŠ¸ì—… ê¸°ì—…ë“¤ì´ í˜ì‹ ì ì¸ ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ë¡œ ì‹œì¥ì— ì§„ì…í•˜ì—¬ í° ì„±ê³¼ë¥¼ ê±°ë‘ê³  ìˆìŠµë‹ˆë‹¤\",\n",
        "        \"ë””ì§€í„¸ ë§ˆì¼€íŒ… ì „ëµì„ í†µí•´ ê¸°ì—…ë“¤ì€ ê³ ê°ê³¼ì˜ ì ‘ì ì„ ëŠ˜ë¦¬ê³  ë¸Œëœë“œ ì¸ì§€ë„ë¥¼ ë†’ì´ê³  ìˆìŠµë‹ˆë‹¤\",\n",
        "        \"ê¸€ë¡œë²Œ ê²½ì œ ë¶ˆí™•ì‹¤ì„± ì†ì—ì„œë„ ì¼ë¶€ ê¸°ì—…ë“¤ì€ ìƒˆë¡œìš´ ì‹œì¥ ê¸°íšŒë¥¼ ì°¾ì•„ ì„±ì¥í•˜ê³  ìˆìŠµë‹ˆë‹¤\",\n",
        "        \"íˆ¬ììë“¤ì€ ì§€ì†ê°€ëŠ¥í•œ ê²½ì˜ê³¼ ESG ìš”ì†Œë¥¼ ê³ ë ¤í•˜ì—¬ íˆ¬ì ê²°ì •ì„ ë‚´ë¦¬ëŠ” ì¶”ì„¸ì…ë‹ˆë‹¤\"\n",
        "    ]\n",
        "\n",
        "    health_docs = [\n",
        "        \"ê°œì¸ ë§ì¶¤í˜• ì˜ë£Œ ì„œë¹„ìŠ¤ê°€ ë°œì „í•˜ë©´ì„œ ì§ˆë³‘ ì˜ˆë°©ê³¼ ì¹˜ë£Œ íš¨ê³¼ê°€ í¬ê²Œ í–¥ìƒë˜ê³  ìˆìŠµë‹ˆë‹¤\",\n",
        "        \"í—¬ìŠ¤ì¼€ì–´ ë¶„ì•¼ì— AI ê¸°ìˆ ì´ ë„ì…ë˜ì–´ ì§„ë‹¨ ì •í™•ë„ê°€ ë†’ì•„ì§€ê³  ìˆìŠµë‹ˆë‹¤\",\n",
        "        \"ì •ì‹ ê±´ê°•ì— ëŒ€í•œ ê´€ì‹¬ì´ ë†’ì•„ì§€ë©´ì„œ ë‹¤ì–‘í•œ ì¹˜ë£Œ ë°©ë²•ê³¼ ì˜ˆë°© í”„ë¡œê·¸ë¨ì´ ê°œë°œë˜ê³  ìˆìŠµë‹ˆë‹¤\",\n",
        "        \"ê³ ë ¹í™” ì‚¬íšŒì— ëŒ€ì‘í•˜ì—¬ ì‹¤ë²„ì¼€ì–´ ì‚°ì—…ì´ ìƒˆë¡œìš´ ì„±ì¥ ë™ë ¥ìœ¼ë¡œ ì£¼ëª©ë°›ê³  ìˆìŠµë‹ˆë‹¤\"\n",
        "    ]\n",
        "\n",
        "    # ì „ì²´ ë¬¸ì„œì™€ ë¼ë²¨\n",
        "    all_docs = tech_docs + business_docs + health_docs\n",
        "    labels = ['ê¸°ìˆ '] * len(tech_docs) + ['ë¹„ì¦ˆë‹ˆìŠ¤'] * len(business_docs) + ['ê±´ê°•'] * len(health_docs)\n",
        "\n",
        "    # TF-IDF ë²¡í„°í™”\n",
        "    vectorizer = TfidfVectorizer(max_features=200, ngram_range=(1, 2))\n",
        "    tfidf_matrix = vectorizer.fit_transform(all_docs)\n",
        "\n",
        "    print(f\"ì „ì²´ ë¬¸ì„œ ìˆ˜: {len(all_docs)}\")\n",
        "    print(f\"TF-IDF í–‰ë ¬ í˜•íƒœ: {tfidf_matrix.shape}\")\n",
        "\n",
        "    # 1. ë¬¸ì„œ ìœ ì‚¬ë„ ë¶„ì„\n",
        "    similarity_matrix = cosine_similarity(tfidf_matrix)\n",
        "\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(similarity_matrix,\n",
        "                annot=False,\n",
        "                cmap='Blues',\n",
        "                xticklabels=[f'{labels[i]}{i%4+1}' for i in range(len(all_docs))],\n",
        "                yticklabels=[f'{labels[i]}{i%4+1}' for i in range(len(all_docs))])\n",
        "    plt.title('Document Similarity Heatmap (TF-IDF + Cosine)', fontsize=16, pad=20)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # 2. í´ëŸ¬ìŠ¤í„°ë§\n",
        "    kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
        "    clusters = kmeans.fit_predict(tfidf_matrix.toarray())\n",
        "\n",
        "    print(f\"\\ní´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼:\")\n",
        "    cluster_labels = ['í´ëŸ¬ìŠ¤í„° A', 'í´ëŸ¬ìŠ¤í„° B', 'í´ëŸ¬ìŠ¤í„° C']\n",
        "    for i, (doc, true_label, pred_cluster) in enumerate(zip(all_docs, labels, clusters)):\n",
        "        print(f\"ë¬¸ì„œ {i+1} ({true_label}): {cluster_labels[pred_cluster]}\")\n",
        "\n",
        "    # 3. ì¹´í…Œê³ ë¦¬ë³„ ëŒ€í‘œ ë‹¨ì–´ ì¶”ì¶œ\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "    print(f\"\\nì¹´í…Œê³ ë¦¬ë³„ ëŒ€í‘œ ë‹¨ì–´ (TF-IDF ê¸°ì¤€):\")\n",
        "    categories = ['ê¸°ìˆ ', 'ë¹„ì¦ˆë‹ˆìŠ¤', 'ê±´ê°•']\n",
        "    category_vectors = {}\n",
        "\n",
        "    for category in categories:\n",
        "        # í•´ë‹¹ ì¹´í…Œê³ ë¦¬ ë¬¸ì„œë“¤ì˜ í‰ê·  ë²¡í„°\n",
        "        category_indices = [i for i, label in enumerate(labels) if label == category]\n",
        "        category_matrix = tfidf_matrix[category_indices]\n",
        "        avg_vector = np.mean(category_matrix.toarray(), axis=0)\n",
        "        category_vectors[category] = avg_vector\n",
        "\n",
        "        # ìƒìœ„ ë‹¨ì–´ë“¤\n",
        "        top_indices = np.argsort(avg_vector)[-8:][::-1]\n",
        "        top_words = [(feature_names[idx], avg_vector[idx]) for idx in top_indices if avg_vector[idx] > 0]\n",
        "\n",
        "        print(f\"\\n{category} ì¹´í…Œê³ ë¦¬:\")\n",
        "        for word, score in top_words:\n",
        "            print(f\"  {word}: {score:.4f}\")\n",
        "\n",
        "    return vectorizer, tfidf_matrix, category_vectors\n",
        "\n",
        "vectorizer, tfidf_matrix, category_vectors = demonstrate_tfidf_applications()"
      ],
      "metadata": {
        "id": "0NtIZfRF3Ts_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **5. TF-IDF ê¸°ë°˜ ê²€ìƒ‰ ì—”ì§„ ì˜ˆì œ**"
      ],
      "metadata": {
        "id": "SGJoNBCtIfCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 5. TF-IDF ê¸°ë°˜ ê²€ìƒ‰ ì—”ì§„\n",
        "# ----------------------------\n",
        "def demonstrate_tfidf_search_engine():\n",
        "    \"\"\"TF-IDF ê¸°ë°˜ ê²€ìƒ‰ ì—”ì§„ ì˜ˆì œ\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"âœ… 5. TF-IDF ê¸°ë°˜ ê²€ìƒ‰ ì—”ì§„\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # ë‰´ìŠ¤ ê¸°ì‚¬ ì˜ˆì œ ë°ì´í„°\n",
        "    news_articles = [\n",
        "        {\n",
        "            'title': 'AI ê¸°ìˆ  ë°œì „ í˜„í™©',\n",
        "            'content': 'ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì´ ë¹ ë¥´ê²Œ ë°œì „í•˜ë©´ì„œ ë‹¤ì–‘í•œ ì‚°ì—… ë¶„ì•¼ì— í˜ì‹ ì„ ê°€ì ¸ì˜¤ê³  ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ ìì—°ì–¸ì–´ì²˜ë¦¬ì™€ ì»´í“¨í„° ë¹„ì „ ë¶„ì•¼ì—ì„œ ê´„ëª©í•  ë§Œí•œ ì„±ê³¼ë¥¼ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤.',\n",
        "            'category': 'ê¸°ìˆ '\n",
        "        },\n",
        "        {\n",
        "            'title': 'ìŠ¤íƒ€íŠ¸ì—… íˆ¬ì ë™í–¥',\n",
        "            'content': 'ì˜¬í•´ ìŠ¤íƒ€íŠ¸ì—…ì— ëŒ€í•œ íˆ¬ìê°€ ì „ë…„ ëŒ€ë¹„ í¬ê²Œ ì¦ê°€í–ˆìŠµë‹ˆë‹¤. íŠ¹íˆ í•€í…Œí¬ì™€ í—¬ìŠ¤ì¼€ì–´ ë¶„ì•¼ì— ëŒ€í•œ íˆ¬ììë“¤ì˜ ê´€ì‹¬ì´ ë†’ìŠµë‹ˆë‹¤.',\n",
        "            'category': 'ë¹„ì¦ˆë‹ˆìŠ¤'\n",
        "        },\n",
        "        {\n",
        "            'title': 'ê±´ê°•í•œ ì‹ë‹¨ ê´€ë¦¬ë²•',\n",
        "            'content': 'ê· í˜•ì¡íŒ ì˜ì–‘ì†Œ ì„­ì·¨ì™€ ê·œì¹™ì ì¸ ìš´ë™ì´ ê±´ê°• ê´€ë¦¬ì˜ í•µì‹¬ì…ë‹ˆë‹¤. íŠ¹íˆ í˜„ëŒ€ì¸ë“¤ì—ê²Œ ë¶€ì¡±í•œ ë¹„íƒ€ë¯¼ê³¼ ë¯¸ë„¤ë„ ì„­ì·¨ì— ì£¼ì˜í•´ì•¼ í•©ë‹ˆë‹¤.',\n",
        "            'category': 'ê±´ê°•'\n",
        "        },\n",
        "        {\n",
        "            'title': 'ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ ì†Œê°œ',\n",
        "            'content': 'ë¨¸ì‹ ëŸ¬ë‹ì€ ë°ì´í„°ë¡œë¶€í„° íŒ¨í„´ì„ í•™ìŠµí•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤. ì§€ë„í•™ìŠµ, ë¹„ì§€ë„í•™ìŠµ, ê°•í™”í•™ìŠµ ë“± ë‹¤ì–‘í•œ ë°©ë²•ë¡ ì´ ìˆìŠµë‹ˆë‹¤.',\n",
        "            'category': 'ê¸°ìˆ '\n",
        "        },\n",
        "        {\n",
        "            'title': 'ë””ì§€í„¸ ë§ˆì¼€íŒ… ì „ëµ',\n",
        "            'content': 'ì†Œì…œë¯¸ë””ì–´ì™€ ê²€ìƒ‰ì—”ì§„ ë§ˆì¼€íŒ…ì„ í†µí•´ ê³ ê°ë“¤ê³¼ì˜ ì ‘ì ì„ ëŠ˜ë¦¬ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ë°ì´í„° ë¶„ì„ì„ í†µí•œ íƒ€ê²Ÿ ë§ˆì¼€íŒ…ì´ íš¨ê³¼ì ì…ë‹ˆë‹¤.',\n",
        "            'category': 'ë¹„ì¦ˆë‹ˆìŠ¤'\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # ë¬¸ì„œ ë‚´ìš© ì¶”ì¶œ\n",
        "    documents = [article['content'] for article in news_articles]\n",
        "    titles = [article['title'] for article in news_articles]\n",
        "\n",
        "    # TF-IDF ë²¡í„°í™”\n",
        "    vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=100)\n",
        "    doc_vectors = vectorizer.fit_transform(documents)\n",
        "\n",
        "    def search_documents(query, top_k=3):\n",
        "        \"\"\"ê²€ìƒ‰ ì¿¼ë¦¬ì— ëŒ€í•œ ê´€ë ¨ ë¬¸ì„œ ë°˜í™˜\"\"\"\n",
        "        # ì¿¼ë¦¬ ë²¡í„°í™”\n",
        "        query_vector = vectorizer.transform([query])\n",
        "\n",
        "        # ìœ ì‚¬ë„ ê³„ì‚°\n",
        "        similarities = cosine_similarity(query_vector, doc_vectors)[0]\n",
        "\n",
        "        # ìƒìœ„ kê°œ ë¬¸ì„œ\n",
        "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
        "\n",
        "        results = []\n",
        "        for idx in top_indices:\n",
        "            if similarities[idx] > 0:\n",
        "                results.append({\n",
        "                    'title': titles[idx],\n",
        "                    'content': documents[idx],\n",
        "                    'category': news_articles[idx]['category'],\n",
        "                    'similarity': similarities[idx]\n",
        "                })\n",
        "\n",
        "        return results\n",
        "\n",
        "\n",
        "    # ê²€ìƒ‰ ì˜ˆì œ\n",
        "    queries = [\n",
        "        \"ì¸ê³µì§€ëŠ¥ê³¼ ë¨¸ì‹ ëŸ¬ë‹\",\n",
        "        \"íˆ¬ìì™€ ìŠ¤íƒ€íŠ¸ì—…\",\n",
        "        \"ê±´ê°•ê³¼ ì˜ì–‘\",\n",
        "        \"ë°ì´í„° ë¶„ì„\"\n",
        "    ]\n",
        "\n",
        "    print(\"ê²€ìƒ‰ ê²°ê³¼:\")\n",
        "    for query in queries:\n",
        "        print(f\"\\nğŸ” ê²€ìƒ‰ì–´: '{query}'\")\n",
        "        results = search_documents(query, top_k=2)\n",
        "\n",
        "        if not results:\n",
        "            print(\" âŒ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "            continue\n",
        "\n",
        "\n",
        "        for i, result in enumerate(results):\n",
        "            print(f\"  {i+1}. {result['title']} (ìœ ì‚¬ë„: {result['similarity']:.4f})\")\n",
        "            print(f\"     ì¹´í…Œê³ ë¦¬: {result['category']}\")\n",
        "            print(f\"     ë‚´ìš©: {result['content'][:50]}...\")\n",
        "\n",
        "demonstrate_tfidf_search_engine()\n"
      ],
      "metadata": {
        "id": "0uaxq4cH3Tw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **6. TF-IDF íŠ¹ì„± ë¶„ì„**"
      ],
      "metadata": {
        "id": "x-mU78ugJzPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 5. TF-IDF íŠ¹ì„± ë¶„ì„\n",
        "# ----------------------------\n",
        "def analyze_tfidf_properties():\n",
        "    \"\"\"TF-IDF ì†ì„± ë¶„ì„\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"âœ… 6. TF-IDF íŠ¹ì„± ë¶„ì„\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # ë¶„ì„ìš© ë¬¸ì„œë“¤\n",
        "    documents = [\n",
        "        \"ì»´í“¨í„° ì»´í“¨í„° ì»´í“¨í„° ê³¼í•™ ê¸°ìˆ \",  # ë†’ì€ TF\n",
        "        \"ì–‘ì ì•”í˜¸í•™ ë¸”ë¡ì²´ì¸\",  # í¬ê·€ ë‹¨ì–´ë“¤\n",
        "        \"ê·¸ë¦¬ê³  ê·¸ëŸ¬ë‚˜ ë˜í•œ í•˜ì§€ë§Œ ê·¸ë˜ì„œ\",  # ì¼ë°˜ì ì¸ ë‹¨ì–´ë“¤\n",
        "        \"ì¸ê³µì§€ëŠ¥ ë¨¸ì‹ ëŸ¬ë‹ ë”¥ëŸ¬ë‹ ìì—°ì–¸ì–´ì²˜ë¦¬\"  # ê¸°ìˆ  ìš©ì–´ë“¤\n",
        "    ]\n",
        "\n",
        "    # ì»¤ìŠ¤í…€ TF-IDFì™€ sklearn ë¹„êµ\n",
        "    custom_tfidf = TFIDFImplementation()\n",
        "    custom_matrix = custom_tfidf.fit_transform(documents)\n",
        "\n",
        "    sklearn_tfidf = TfidfVectorizer()\n",
        "    sklearn_matrix = sklearn_tfidf.fit_transform(documents)\n",
        "\n",
        "    print(\"ë¬¸ì„œë³„ TF-IDF ë¶„ì„:\")\n",
        "\n",
        "    for i, doc in enumerate(documents):\n",
        "        print(f\"\\në¬¸ì„œ {i+1}: {doc}\")\n",
        "\n",
        "        # ì»¤ìŠ¤í…€ êµ¬í˜„ ê²°ê³¼\n",
        "        top_words_custom = custom_tfidf.get_top_words(doc, top_n=3)\n",
        "        print(f\"ì»¤ìŠ¤í…€ êµ¬í˜„ ìƒìœ„ ë‹¨ì–´: {top_words_custom}\")\n",
        "\n",
        "        # sklearn ê²°ê³¼\n",
        "        sklearn_vector = sklearn_matrix[i].toarray()[0]\n",
        "        feature_names = sklearn_tfidf.get_feature_names_out()\n",
        "        sklearn_scores = [(feature_names[idx], sklearn_vector[idx])\n",
        "                         for idx in np.argsort(sklearn_vector)[-3:][::-1]\n",
        "                         if sklearn_vector[idx] > 0]\n",
        "        print(f\"sklearn ìƒìœ„ ë‹¨ì–´: {sklearn_scores}\")\n",
        "\n",
        "    # TF vs IDF ë¶„ì„ ì‹œê°í™”\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # TF ë¶„í¬\n",
        "    plt.subplot(2, 2, 1)\n",
        "    sample_doc = \"ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì´ ë°œì „í•˜ë©´ì„œ ì¸ê³µì§€ëŠ¥ ì‘ìš© ë¶„ì•¼ê°€ í™•ëŒ€ë˜ê³  ìˆìŠµë‹ˆë‹¤ ì¸ê³µì§€ëŠ¥\"\n",
        "    words = custom_tfidf.preprocess_text(sample_doc)\n",
        "    word_freq = Counter(words)\n",
        "\n",
        "    plt.bar(word_freq.keys(), word_freq.values(), color='skyblue')\n",
        "    plt.title('Word Frequency (TF)', fontsize=14)\n",
        "    plt.xlabel('Words')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    # IDF ë¶„í¬\n",
        "    plt.subplot(2, 2, 2)\n",
        "    idf_words = list(custom_tfidf.idf_values.keys())[:10]\n",
        "    idf_vals = [custom_tfidf.idf_values[word] for word in idf_words]\n",
        "\n",
        "    plt.bar(idf_words, idf_vals, color='lightcoral')\n",
        "    plt.title('Inverse Document Frequency (IDF)', fontsize=14)\n",
        "    plt.xlabel('Words')\n",
        "    plt.ylabel('IDF Values')\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    # TF-IDF ìµœì¢… ì ìˆ˜\n",
        "    plt.subplot(2, 2, 3)\n",
        "    tfidf_vector = custom_tfidf.document_to_vector(sample_doc)\n",
        "    top_indices = np.argsort(tfidf_vector)[-5:][::-1]\n",
        "    top_words = [list(custom_tfidf.vocab.keys())[i] for i in top_indices if tfidf_vector[i] > 0]\n",
        "    top_scores = [tfidf_vector[i] for i in top_indices if tfidf_vector[i] > 0]\n",
        "\n",
        "    plt.bar(top_words, top_scores, color='lightgreen')\n",
        "    plt.title('TF-IDF Scores', fontsize=14)\n",
        "    plt.xlabel('Words')\n",
        "    plt.ylabel('TF-IDF Values')\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    # ë¬¸ì„œ ê¸¸ì´ë³„ TF-IDF ì˜í–¥ ë¶„ì„\n",
        "    plt.subplot(2, 2, 4)\n",
        "    doc_lengths = [len(doc.split()) for doc in documents]\n",
        "    max_tfidf_scores = [np.max(custom_matrix[i]) for i in range(len(documents))]\n",
        "\n",
        "    plt.scatter(doc_lengths, max_tfidf_scores, s=100, alpha=0.7)\n",
        "    plt.title('Document Length vs Max TF-IDF Score', fontsize=14)\n",
        "    plt.xlabel('Document Length (words)')\n",
        "    plt.ylabel('Max TF-IDF Score')\n",
        "\n",
        "    for i, (x, y) in enumerate(zip(doc_lengths, max_tfidf_scores)):\n",
        "        plt.annotate(f'Doc{i+1}', (x, y), xytext=(5, 5), textcoords='offset points')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "analyze_tfidf_properties()\n"
      ],
      "metadata": {
        "id": "dfwzxdCw3T7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ì˜ˆì œ: TF-IDFì™€ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ì´ìš©í•œ ì˜í™” ì¶”ì²œ ì‹œìŠ¤í…œ**"
      ],
      "metadata": {
        "id": "4epk_JtzIL1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy"
      ],
      "metadata": {
        "id": "9cZ00ARZI5PU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from konlpy.tag import Okt\n",
        "import re\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class KoreanMovieRecommendationSystem:\n",
        "    def __init__(self):\n",
        "        self.movies_df = None\n",
        "        self.tfidf_matrix = None\n",
        "        self.cosine_sim = None\n",
        "        self.okt = Okt()\n",
        "\n",
        "    def download_movie_data(self):\n",
        "\n",
        "        # ì˜ˆì‹œ í•œê¸€ ì˜í™” ë°ì´í„° ìƒì„±\n",
        "        sample_movies = [\n",
        "            {\n",
        "                'title': 'ê¸°ìƒì¶©',\n",
        "                'plot': 'ë°˜ì§€í•˜ì— ì‚´ë˜ ê¸°íƒ ê°€ì¡±ì´ ë¶€ìœ í•œ ë°• ì‚¬ì¥ ê°€ì¡±ì˜ ì§‘ì— ì·¨ì—…í•˜ë©´ì„œ ë²Œì–´ì§€ëŠ” ê³„ê¸‰ ê°ˆë“±ê³¼ ì‚¬íšŒì  ëª¨ìˆœì„ ê·¸ë¦° ì‘í’ˆ. ê°€ë‚œí•œ ê°€ì¡±ê³¼ ë¶€ìœ í•œ ê°€ì¡± ì‚¬ì´ì˜ ì²¨ì˜ˆí•œ ëŒ€ë¦½ê³¼ ì˜ˆìƒì¹˜ ëª»í•œ ì‚¬ê±´ë“¤ì´ ì—°ì‡„ì ìœ¼ë¡œ ì¼ì–´ë‚œë‹¤.',\n",
        "                'genre': 'ë“œë¼ë§ˆ, ìŠ¤ë¦´ëŸ¬',\n",
        "                'year': 2019\n",
        "            },\n",
        "            {\n",
        "                'title': 'ì˜¬ë“œë³´ì´',\n",
        "                'plot': '15ë…„ê°„ ê°ê¸ˆë‹¹í•œ ì˜¤ëŒ€ìˆ˜ê°€ ë³µìˆ˜ë¥¼ ìœ„í•´ ìì‹ ì„ ê°€ë‘” ìë¥¼ ì°¾ì•„ë‚˜ì„œëŠ” ë³µìˆ˜ê·¹. ë¯¸ìŠ¤í„°ë¦¬í•œ ê°ê¸ˆì˜ ì´ìœ ì™€ ì¶©ê²©ì ì¸ ì§„ì‹¤ì´ ë°í˜€ì§€ëŠ” ê°•ë ¬í•œ ìŠ¤ë¦´ëŸ¬ ì‘í’ˆì´ë‹¤.',\n",
        "                'genre': 'ìŠ¤ë¦´ëŸ¬, ë¯¸ìŠ¤í„°ë¦¬',\n",
        "                'year': 2003\n",
        "            },\n",
        "            {\n",
        "                'title': 'ì•„ê°€ì”¨',\n",
        "                'plot': 'ì¼ì œê°•ì ê¸° ì¡°ì„ ì„ ë°°ê²½ìœ¼ë¡œ í•œ ê·€ì¡± ì•„ê°€ì”¨ì™€ í•˜ë…€, ê·¸ë¦¬ê³  ì‚¬ê¸°ê¾¼ì´ ì–½íŒ ë³µì¡í•œ ì‚¬ê¸°ê·¹ê³¼ ì‚¬ë‘ ì´ì•¼ê¸°. ë°˜ì „ì— ë°˜ì „ì„ ê±°ë“­í•˜ëŠ” ì •êµí•œ í”Œë¡¯ê³¼ ì•„ë¦„ë‹¤ìš´ ì˜ìƒë¯¸ê°€ ë‹ë³´ì¸ë‹¤.',\n",
        "                'genre': 'ë“œë¼ë§ˆ, ìŠ¤ë¦´ëŸ¬',\n",
        "                'year': 2016\n",
        "            },\n",
        "            {\n",
        "                'title': 'ê³¡ì„±',\n",
        "                'plot': 'í‰í™”ë¡œìš´ ì‹œê³¨ ë§ˆì„ì— ì¼ë³¸ì¸ ë‚¨ìê°€ ë‚˜íƒ€ë‚œ í›„ ê¸°ì´í•œ ì§ˆë³‘ê³¼ ì‚´ì¸ì‚¬ê±´ì´ ì—°ì´ì–´ ë°œìƒí•œë‹¤. ê²½ì°°ê´€ ì¢…êµ¬ê°€ ë”¸ì„ êµ¬í•˜ê¸° ìœ„í•´ ë¯¸ìŠ¤í„°ë¦¬ì˜ ì§„ìƒì„ íŒŒí—¤ì¹˜ëŠ” ê³µí¬ ìŠ¤ë¦´ëŸ¬.',\n",
        "                'genre': 'ê³µí¬, ë¯¸ìŠ¤í„°ë¦¬',\n",
        "                'year': 2016\n",
        "            },\n",
        "            {\n",
        "                'title': 'íƒ€ì§œ',\n",
        "                'plot': 'í™”íˆ¬ê³„ì˜ ì „ì„¤ì ì¸ ë„ë°•ì‚¬ë“¤ê³¼ ì‹ ì¶œë‚´ê¸° ê³°ì´ ë§Œë‚˜ ë²Œì´ëŠ” ì†ì„ìˆ˜ì™€ ë°°ì‹ ì˜ ì´ì•¼ê¸°. ì¹˜ë°€í•œ ì‹¬ë¦¬ì „ê³¼ í™”íˆ¬ ê²Œì„ì˜ ì„¸ê³„ë¥¼ ìƒìƒí•˜ê²Œ ê·¸ë¦° ì‘í’ˆ.',\n",
        "                'genre': 'ë²”ì£„, ë“œë¼ë§ˆ',\n",
        "                'year': 2006\n",
        "            },\n",
        "            {\n",
        "                'title': 'ì‚´ì¸ì˜ ì¶”ì–µ',\n",
        "                'plot': '1980ë…„ëŒ€ ê²½ê¸°ë„ í™”ì„±ì—ì„œ ì—°ì‡„ì‚´ì¸ì‚¬ê±´ì´ ì¼ì–´ë‚˜ê³ , ë¬´ëŠ¥í•œ ê²½ì°°ë“¤ì´ ë²”ì¸ì„ ì°¾ê¸° ìœ„í•´ ê³ êµ°ë¶„íˆ¬í•˜ëŠ” ê³¼ì •ì„ ê·¸ë¦° ë¸”ë™ ì½”ë¯¸ë”” ìŠ¤ë¦´ëŸ¬.',\n",
        "                'genre': 'ìŠ¤ë¦´ëŸ¬, ë²”ì£„',\n",
        "                'year': 2003\n",
        "            },\n",
        "            {\n",
        "                'title': 'ë¶€ì‚°í–‰',\n",
        "                'plot': 'ì¢€ë¹„ ë°”ì´ëŸ¬ìŠ¤ê°€ ì „êµ­ìœ¼ë¡œ í™•ì‚°ë˜ëŠ” ìƒí™©ì—ì„œ KTXë¥¼ íƒ€ê³  ë¶€ì‚°ìœ¼ë¡œ í–¥í•˜ëŠ” ì‚¬ëŒë“¤ì˜ ìƒì¡´ê¸°. ê°€ì¡±ì• ì™€ ì¸ê°„ì„±ì„ ê·¸ë¦° ì¢€ë¹„ ì•¡ì…˜ ì˜í™”.',\n",
        "                'genre': 'ì•¡ì…˜, ê³µí¬',\n",
        "                'year': 2016\n",
        "            },\n",
        "            {\n",
        "                'title': 'ì‹ ê³¼í•¨ê»˜-ì£„ì™€ë²Œ',\n",
        "                'plot': 'ì‚¬í›„ì„¸ê³„ë¥¼ ë°°ê²½ìœ¼ë¡œ ì£½ì€ ìê°€ 7ê°œì˜ ì§€ì˜¥ì—ì„œ ì¬íŒì„ ë°›ìœ¼ë©° í™˜ìƒì„ ìœ„í•´ ë…¸ë ¥í•˜ëŠ” ì´ì•¼ê¸°. í•œêµ­ì  íŒíƒ€ì§€ì™€ ê°€ì¡±ì• ë¥¼ ë‹´ì€ ì‘í’ˆ.',\n",
        "                'genre': 'íŒíƒ€ì§€, ë“œë¼ë§ˆ',\n",
        "                'year': 2017\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        self.movies_df = pd.DataFrame(sample_movies)\n",
        "        print(\"ì˜í™” ë°ì´í„° ë¡œë“œ ì™„ë£Œ!\")\n",
        "        print(f\"ì´ {len(self.movies_df)}í¸ì˜ ì˜í™” ë°ì´í„°\")\n",
        "        return self.movies_df\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"\n",
        "        í•œê¸€ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬\n",
        "        \"\"\"\n",
        "        # íŠ¹ìˆ˜ë¬¸ì ì œê±°\n",
        "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "\n",
        "        # í˜•íƒœì†Œ ë¶„ì„ ë° ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬ë§Œ ì¶”ì¶œ\n",
        "        tokens = self.okt.pos(text)\n",
        "        meaningful_words = []\n",
        "\n",
        "        for word, pos in tokens:\n",
        "            if pos in ['Noun', 'Verb', 'Adjective'] and len(word) > 1:\n",
        "                meaningful_words.append(word)\n",
        "\n",
        "        return ' '.join(meaningful_words)\n",
        "\n",
        "    def create_tfidf_matrix(self):\n",
        "        \"\"\"\n",
        "        TF-IDF í–‰ë ¬ ìƒì„±\n",
        "        \"\"\"\n",
        "        # ì¤„ê±°ë¦¬ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬\n",
        "        processed_plots = [self.preprocess_text(plot) for plot in self.movies_df['plot']]\n",
        "\n",
        "        # TF-IDF ë²¡í„°í™”\n",
        "        self.tfidf_vectorizer = TfidfVectorizer(\n",
        "            max_features=1000,  # ìµœëŒ€ íŠ¹ì„± ìˆ˜\n",
        "            stop_words=None,    # í•œê¸€ì€ ë³„ë„ì˜ ë¶ˆìš©ì–´ ì²˜ë¦¬\n",
        "            ngram_range=(1, 2)  # 1-gram, 2-gram ì‚¬ìš©\n",
        "        )\n",
        "\n",
        "        self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(processed_plots)\n",
        "        print(f\"TF-IDF í–‰ë ¬ í¬ê¸°: {self.tfidf_matrix.shape}\")\n",
        "\n",
        "    def calculate_similarity(self):\n",
        "        \"\"\"\n",
        "        ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°\n",
        "        \"\"\"\n",
        "        self.cosine_sim = cosine_similarity(self.tfidf_matrix, self.tfidf_matrix)\n",
        "        print(\"ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° ì™„ë£Œ!\")\n",
        "\n",
        "    def get_recommendations(self, movie_title, num_recommendations=3):\n",
        "        \"\"\"\n",
        "        ì˜í™” ì¶”ì²œ í•¨ìˆ˜\n",
        "        \"\"\"\n",
        "        # ì˜í™” ì œëª©ìœ¼ë¡œ ì¸ë±ìŠ¤ ì°¾ê¸°\n",
        "        try:\n",
        "            movie_idx = self.movies_df[self.movies_df['title'] == movie_title].index[0]\n",
        "        except IndexError:\n",
        "            print(f\"'{movie_title}' ì˜í™”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "            return None\n",
        "\n",
        "        # í•´ë‹¹ ì˜í™”ì™€ ë‹¤ë¥¸ ì˜í™”ë“¤ ê°„ì˜ ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚°\n",
        "        sim_scores = list(enumerate(self.cosine_sim[movie_idx]))\n",
        "\n",
        "        # ìœ ì‚¬ë„ ì ìˆ˜ë¡œ ì •ë ¬ (ìê¸° ìì‹  ì œì™¸)\n",
        "        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:]\n",
        "\n",
        "        # ìƒìœ„ Nê°œ ì˜í™”ì˜ ì¸ë±ìŠ¤ ì¶”ì¶œ\n",
        "        top_movies_indices = [i[0] for i in sim_scores[:num_recommendations]]\n",
        "\n",
        "        # ì¶”ì²œ ì˜í™” ì •ë³´ ë°˜í™˜\n",
        "        recommendations = self.movies_df.iloc[top_movies_indices][['title', 'plot', 'genre', 'year']].copy()\n",
        "        recommendations['similarity_score'] = [sim_scores[i][1] for i in range(num_recommendations)]\n",
        "\n",
        "        return recommendations\n",
        "\n",
        "    def run_recommendation_system(self):\n",
        "        \"\"\"\n",
        "        ì¶”ì²œ ì‹œìŠ¤í…œ ì‹¤í–‰\n",
        "        \"\"\"\n",
        "        print(\"=== í•œê¸€ ì˜í™” ì¶”ì²œ ì‹œìŠ¤í…œ ===\\n\")\n",
        "\n",
        "        # 1. ë°ì´í„° ë¡œë“œ\n",
        "        self.download_movie_data()\n",
        "\n",
        "        # 2. TF-IDF í–‰ë ¬ ìƒì„±\n",
        "        print(\"\\ní…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° TF-IDF í–‰ë ¬ ìƒì„± ì¤‘...\")\n",
        "        self.create_tfidf_matrix()\n",
        "\n",
        "        # 3. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°\n",
        "        print(\"\\nì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° ì¤‘...\")\n",
        "        self.calculate_similarity()\n",
        "\n",
        "        print(\"\\n=== ì¶”ì²œ ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ ===\")\n",
        "        return True\n",
        "\n",
        "# ì‹œìŠ¤í…œ ì‹¤í–‰ ë° í…ŒìŠ¤íŠ¸\n",
        "def main():\n",
        "    # ì¶”ì²œ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ë° ì‹¤í–‰\n",
        "    recommender = KoreanMovieRecommendationSystem()\n",
        "    recommender.run_recommendation_system()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"ì˜í™” ì¶”ì²œ í…ŒìŠ¤íŠ¸\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # í…ŒìŠ¤íŠ¸ìš© ì˜í™”ë“¤\n",
        "    test_movies = ['ê¸°ìƒì¶©', 'ì˜¬ë“œë³´ì´', 'ë¶€ì‚°í–‰']\n",
        "\n",
        "    for movie in test_movies:\n",
        "        print(f\"\\nâœ… '{movie}'ì™€ ìœ ì‚¬í•œ ì˜í™” ì¶”ì²œ:\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "        recommendations = recommender.get_recommendations(movie, 3)\n",
        "\n",
        "        if recommendations is not None:\n",
        "            for idx, row in recommendations.iterrows():\n",
        "                print(f\"ğŸ¬ {row['title']} ({row['year']})\")\n",
        "                print(f\"   ì¥ë¥´: {row['genre']}\")\n",
        "                print(f\"   ìœ ì‚¬ë„: {row['similarity_score']:.4f}\")\n",
        "                print(f\"   ì¤„ê±°ë¦¬: {row['plot'][:100]}...\")\n",
        "                print()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "GQgj0APBIP4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "cv61nvoEM5tS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Word2Vec**"
      ],
      "metadata": {
        "id": "103amdw1GdG7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **ì„ë² ë”©(Embedding)** :\n",
        "    - ë°ì´í„°ì˜ ì˜ë¯¸ë¥¼ í¬ì°©í•˜ê¸° ìœ„í•œ **ë²¡í„° í‘œí˜„**\n",
        "- **ë‹¨ì–´ ì„ë² ë”©(Word Embedding)**\n",
        "    - ë‹¨ì–´ í•˜ë‚˜ë¥¼ ê³ ì°¨ì› ë²¡í„° ê³µê°„ì˜ ì ìœ¼ë¡œ í‘œí˜„, ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ ë‹¨ì–´ë“¤ì´ ê°€ê¹ê²Œ ìœ„ì¹˜í•¨\n",
        "- **Word2Vec** :\n",
        "    - 2013ë…„ Googleì˜ Tomas Mikolovê°€ ê°œë°œí•œ ë‹¨ì–´ ì„ë² ë”©(Word Embedding) ê¸°ë²•\n",
        "    - ë‹¨ì–´ë¥¼ ê³ ì°¨ì› ë²¡í„° ê³µê°„ì—ì„œ ë°€ì§‘ëœ ì‹¤ìˆ˜ ë²¡í„°ë¡œ í‘œí˜„í•˜ëŠ” ë°©ë²•\n",
        "    - **Word2Vec ëŠ” í…ìŠ¤íŠ¸ì˜ ì˜ë¯¸ë¥¼ í¬ì°©í•˜ëŠ” ë° ì„±ê³µí•œ ì²« ë²ˆì§¸ ì‹œë„**\n",
        "    - **ì‹ ê²½ë§ì„ ì‚¬ìš©**í•´ ì£¼ì–´ì§„ **ë¬¸ì¥ì—ì„œ ë‹¤ìŒì— ì–´ë–¤ ë‹¨ì–´ê°€ ë“±ì¥í•˜ëŠ”ì§€** ì‚´í´ë´„ìœ¼ë¡œì¨ ë‹¨ì–´ ì„ë² ë”©ì„ ìƒì„±í•¨ --> **ì˜ë¯¸ í‘œí˜„ì„ ìƒì„±í•¨**\n",
        "- **Word2Vec ë°©ë²•**:\n",
        "    - ì–´íœ˜ì‚¬ì „ì— ìˆëŠ” ëª¨ë“  ë‹¨ì–´ì— ëŒ€í•´ ëœë¤í•˜ê²Œ ì´ˆê¸°í™”ëœ ì¼ë ¨ì˜ ê°’ì„ ë‹¨ì–´ ì„ë² ë”©ìœ¼ë¡œ í• ë‹¹\n",
        "    - í›ˆë ¨ ìŠ¤í…ë§ˆë‹¤ í›ˆë ¨ ë°ì´í„°ì—ì„œ ë‹¨ì–´ ìŒì„ ê°€ì ¸ì™€ ëª¨ë¸ì´ ë¬¸ì¥ ì•ˆì—ì„œ ë‹¨ì–´ ìŒì´ ì´ì›ƒì— ë‚˜íƒ€ë‚  ê°€ëŠ¥ì„±ì´ ìˆëŠ”ì§€ ì˜ˆì¸¡\n",
        "    - í›ˆë ¨ ê³¼ì • ë™ì•ˆ Word2Vec ëŠ” ë‹¨ì–´ ì‚¬ì´ ê´€ê³„ë¥¼ í•™ìŠµí•˜ê³  ì´ ì •ë³´ë¥¼ ì„ë² ë”©ì— ì €ì¥\n",
        "    - ë‘ ë‹¨ì–´ê°€ ì´ì›ƒì— ë‚˜íƒ€ë‚  ê°€ëŠ¥ì„±ì´ ë†’ë‹¤ë©´ ë‘ ë‹¨ì–´ì˜ ì„ë² ë”©ì€ ì„œë¡œ ë§¤ìš° ê°€ê¹Œì›Œì§"
      ],
      "metadata": {
        "id": "39IocrbFHJGq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ì˜ˆì œ : word2vecë¡œ ë¬¸ì¥ ìœ ì‚¬ë„ í™•ì¸**\n",
        "\n",
        "- **ì‘ì—… ìˆœì„œ**\n",
        "  1) ë§ë­‰ì¹˜ êµ¬ì„± & ì „ì²˜ë¦¬\n",
        "  2) Word2Vec (CBOW vs Skip-gram) í•™ìŠµ\n",
        "  3) ìœ ì‚¬ì–´ íƒìƒ‰, ìœ ì‚¬ë„ ê³„ì‚°, ê°„ë‹¨ ìœ ì¶”(analogy)\n",
        "  4) ë¬¸ì„œ ì„ë² ë”©(í‰ê· )ìœ¼ë¡œ ìœ ì‚¬ë„ íˆíŠ¸ë§µ\n",
        "  5) t-SNEë¡œ ë‹¨ì–´ ë²¡í„° 2D ì‹œê°í™”\n",
        "- **gensim** ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
        "    - ëŒ€ê·œëª¨ í…ìŠ¤íŠ¸ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ê³  Word2Vec, LDA ê°™ì€ í† í”½ ëª¨ë¸ë§ê³¼ ì„ë² ë”©ì„ ì‰½ê²Œ êµ¬í˜„í•  ìˆ˜ ìˆëŠ” Python ë¼ì´ë¸ŒëŸ¬ë¦¬"
      ],
      "metadata": {
        "id": "zxibfPJDSyWh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1. ë§ë­‰ì¹˜ êµ¬ì„± & ì „ì²˜ë¦¬**\n",
        "- í† í°í™”(Tokenization): ë¬¸ì¥ì„ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ìª¼ê°œëŠ” ê³¼ì •\n",
        "- ì†Œë¬¸ì ë³€í™˜: ëŒ€ì†Œë¬¸ìë¥¼ í†µì¼í•˜ì—¬ 'AI'ì™€ 'ai'ë¥¼ ê°™ì€ ë‹¨ì–´ë¡œ ì¸ì‹í•˜ê²Œí•¨"
      ],
      "metadata": {
        "id": "OK_EeYrQKSlT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "OOwxJZgFGgwY",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# NLTK ë¦¬ì†ŒìŠ¤ ë‹¤ìš´ë¡œë“œ\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# 1. ë§ë­‰ì¹˜ êµ¬ì„±\n",
        "corpus = [\n",
        "    \"I like to eat pizza and pasta.\",\n",
        "    \"I enjoy eating sushi and noodles.\",\n",
        "    \"Cats and dogs are common pets.\",\n",
        "    \"Tigers and lions are wild animals.\",\n",
        "    \"AI is a revolutionary technology.\",\n",
        "    \"Machine learning is a subfield of AI.\",\n",
        "    \"Pizza is a popular dish.\",\n",
        "    \"Dogs are loyal companions.\",\n",
        "    \"Technology is advancing rapidly.\"\n",
        "]\n",
        "\n",
        "# 2. ì „ì²˜ë¦¬: í† í°í™” ë° ì†Œë¬¸ì ë³€í™˜\n",
        "tokenized_corpus = [word_tokenize(sentence.lower()) for sentence in corpus]\n",
        "print(\"--- ì „ì²˜ë¦¬ëœ ë§ë­‰ì¹˜(ì¼ë¶€) ---\")\n",
        "print(tokenized_corpus[0])\n",
        "print(tokenized_corpus[2])\n"
      ],
      "metadata": {
        "id": "4nSI_sPcKS0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2. Word2Vec (CBOW vs Skip-gram) í•™ìŠµ**\n",
        "- Word2Vec ëª¨ë¸ì„ í•™ìŠµì‹œí‚´\n",
        "- **CBOW** (Continuous Bag of Words): ì£¼ë³€ ë‹¨ì–´ë“¤ì„ ì´ìš©í•´ ì¤‘ê°„ì— ìˆëŠ” ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë°©ì‹\n",
        "- **Skip-gram**: ì–´ë–¤ ë‹¨ì–´(ì¤‘ì‹¬ ë‹¨ì–´)ë¥¼ ì´ìš©í•´ ì£¼ë³€ ë‹¨ì–´ë“¤ì„ ì˜ˆì¸¡í•˜ëŠ” ë°©ì‹, ë³´í†µ Skip-gramì´ ë” ì‘ì€ ë°ì´í„°ì…‹ì—ì„œ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì„"
      ],
      "metadata": {
        "id": "AlSWMbvEKS-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Word2Vec ëª¨ë¸ í•™ìŠµ\n",
        "# CBOW ëª¨ë¸\n",
        "cbow_model = Word2Vec(\n",
        "    sentences=tokenized_corpus, # í•™ìŠµí•  ë§ë­‰ì¹˜ (í† í°í™”ëœ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸)\n",
        "    vector_size=100, # ë‹¨ì–´ ë²¡í„°ì˜ ì°¨ì›\n",
        "    window=5,        # í•™ìŠµ ì‹œ ê³ ë ¤í•  ì£¼ë³€ ë‹¨ì–´ì˜ ë²”ìœ„\n",
        "    min_count=1,     # ìµœì†Œ ë“±ì¥ ë¹ˆë„ (ì´ ë¯¸ë§Œ ë‹¨ì–´ëŠ” ë¬´ì‹œ)\n",
        "    sg=0,            # í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ ì„ íƒ (0=CBOW, 1=Skip-gram)\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# Skip-gram ëª¨ë¸\n",
        "skipgram_model = Word2Vec(\n",
        "    sentences=tokenized_corpus,\n",
        "    vector_size=100,\n",
        "    window=5,\n",
        "    min_count=1,\n",
        "    sg=1,          # 1: Skip-gram\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "print(\"\\n--- CBOW ëª¨ë¸ í•™ìŠµ ì™„ë£Œ ---\")\n",
        "print(\"ë²¡í„° í¬ê¸°:\", cbow_model.vector_size)\n",
        "print(\"ë‹¨ì–´ ìˆ˜:\", len(cbow_model.wv))\n",
        "\n",
        "print(\"\\n--- Skip-gram ëª¨ë¸ í•™ìŠµ ì™„ë£Œ ---\")\n",
        "print(\"ë²¡í„° í¬ê¸°:\", skipgram_model.vector_size)\n",
        "print(\"ë‹¨ì–´ ìˆ˜:\", len(skipgram_model.wv))"
      ],
      "metadata": {
        "id": "roKR2GpfKTHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **tokenized_corpusì— ë§¤í•‘ëœ skipgram_model.wv ì¶œë ¥**\n"
      ],
      "metadata": {
        "id": "uwH5zKQCTTMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, sentence in enumerate(tokenized_corpus):\n",
        "    print(f\"\\n--- ë¬¸ì¥ {i+1} ---\")\n",
        "    for word in sentence:\n",
        "        if word in skipgram_model.wv:\n",
        "            print(f\"{word:10s} -> {skipgram_model.wv[word][:5]}\")  # ì• 5ê°œ ê°’ë§Œ í‘œì‹œ\n"
      ],
      "metadata": {
        "id": "FaYof-hLST9-",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3. ìœ ì‚¬ì–´ íƒìƒ‰, ìœ ì‚¬ë„ ê³„ì‚°, ê°„ë‹¨ ìœ ì¶”(analogy)**\n",
        "- í•™ìŠµëœ ëª¨ë¸ì„ í™œìš©í•˜ì—¬ ë‹¨ì–´ ê°„ì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ê³ , í¥ë¯¸ë¡œìš´ ë‹¨ì–´ ìœ ì¶”(Analogy) ì‹¤í—˜\n",
        "- most_similar(): ì£¼ì–´ì§„ ë‹¨ì–´ì™€ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ê°€ ê°€ì¥ ë†’ì€ ë‹¨ì–´ë“¤ì„ ì°¾ëŠ”ë‹¤.\n",
        "- similarity(): ë‘ ë‹¨ì–´ì˜ ë²¡í„° ê°„ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ì—¬ ì˜ë¯¸ì  ìœ ì‚¬ë„ë¥¼ ì¸¡ì •í•¨\n",
        "- ìœ ì¶”: ë²¡í„° ì—°ì‚°(vector('king') - vector('man') + vector('woman'))ì„ í†µí•´ ë‹¨ì–´ ê°„ì˜ ê´€ê³„ë¥¼ ì¶”ë¡ í•œë‹¤."
      ],
      "metadata": {
        "id": "6tcgLGN1KTOG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. ìœ ì‚¬ì–´ íƒìƒ‰ (ê°€ì¥ ë¹„ìŠ·í•œ ë‹¨ì–´ ì°¾ê¸°)\n",
        "print(\"\\nâœ… 'pizza'ì™€ ê°€ì¥ ë¹„ìŠ·í•œ ë‹¨ì–´ (Skip-gram) ---\")\n",
        "similar_words = skipgram_model.wv.most_similar(\"pizza\", topn=5)\n",
        "for word, similarity in similar_words:\n",
        "    print(f\"  {word}: {similarity:.4f}\")\n",
        "\n",
        "\n",
        "# 5. ìœ ì‚¬ë„ ê³„ì‚° (ë‘ ë‹¨ì–´ ê°„ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„)\n",
        "print(\"\\nâœ… 'dogs'ì™€ 'cats'ì˜ ìœ ì‚¬ë„ ---\")\n",
        "similarity_score = skipgram_model.wv.similarity('dogs', 'cats')\n",
        "print(f\"  ìœ ì‚¬ë„ ì ìˆ˜: {similarity_score:.4f}\")\n",
        "\n",
        "\n",
        "print(\"\\nâœ… 'pizza'ì™€ 'dogs'ì˜ ìœ ì‚¬ë„ ---\")\n",
        "similarity_score = skipgram_model.wv.similarity('pizza', 'dogs')\n",
        "print(f\"  ìœ ì‚¬ë„ ì ìˆ˜: {similarity_score:.4f}\")\n",
        "\n",
        "\n",
        "# 6. ê°„ë‹¨í•œ ìœ ì¶”(analogy): \"King - Man + Woman = ?\"\n",
        "\n",
        "# ì•ˆì „ ìœ ì‚¬/ìœ ì¶” í—¬í¼\n",
        "print(\"\\nâœ… King - Man + Woman ìœ ì¶” ---\")\n",
        "def check_tokens(kv, tokens):\n",
        "    missing = [t for t in tokens if t not in kv.key_to_index]\n",
        "    return missing\n",
        "\n",
        "def safe_most_similar(kv, positive, negative=None, topn=1):\n",
        "    negative = negative or []\n",
        "    missing = check_tokens(kv, positive + negative)\n",
        "    if missing:\n",
        "        raise KeyError(f\"ì‚¬ì „ì— ì—†ëŠ” ë‹¨ì–´: {missing}\")\n",
        "    return kv.most_similar(positive=positive, negative=negative, topn=topn)\n",
        "\n",
        "# ë¨¼ì € í˜„ì¬ skip-gram ëª¨ë¸ë¡œ ì‹œë„\n",
        "try:\n",
        "    analogy_result = safe_most_similar(\n",
        "        skipgram_model.wv, positive=['woman','king'], negative=['man'], topn=1\n",
        "    )\n",
        "    print(\" (ë‚´ ë§ë­‰ì¹˜) ìœ ì¶”:\", analogy_result[0])\n",
        "except KeyError as e:\n",
        "    print(\"âš ï¸ í˜„ì¬ Word2Vec ì–´íœ˜ì— ë‹¨ì–´ê°€ ì—†ìŠµë‹ˆë‹¤ -> GloVeë¡œ í´ë°±í•©ë‹ˆë‹¤.\", e)\n",
        "\n",
        "    # GloVe 100d ë¡œ í´ë°±\n",
        "    import gensim.downloader as api\n",
        "    glove = api.load(\"glove-wiki-gigaword-100\")  # ì•½ 128MB\n",
        "    analogy_result = glove.most_similar(positive=['woman','king'], negative=['man'], topn=1)\n",
        "    print(\"\\nâœ… (GloVe) ìœ ì¶”:\", analogy_result[0])\n"
      ],
      "metadata": {
        "id": "ZbTuXdheKTWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **4. ë¬¸ì„œ ì„ë² ë”©(í‰ê· ) ì´í›„ ìœ ì‚¬ë„ë¥¼ íˆíŠ¸ë§µìœ¼ë¡œ ì‹œê°í™”**\n",
        "    \n",
        "- ë‹¨ì–´ ì„ë² ë”©ì„ ì´ìš©í•´ ë¬¸ì¥(ë¬¸ì„œ)ì˜ ì„ë² ë”©ì„ ë§Œë“¤ê³ , ë¬¸ì¥ ê°„ì˜ ìœ ì‚¬ë„ë¥¼ íˆíŠ¸ë§µìœ¼ë¡œ ì‹œê°í™”í•¨. --> ì—¬ê¸°ì„œëŠ” ê° ë¬¸ì¥ì˜ ë‹¨ì–´ ë²¡í„°ë“¤ì˜ í‰ê· ì„ ì‚¬ìš©í•¨\n",
        "- ìŒì‹ì— ëŒ€í•œ ë¬¸ì¥, ë™ë¬¼ì— ëŒ€í•œ ë¬¸ì¥ë“¤ì´ ë†’ì€ ìœ ì‚¬ë„ë¥¼ ë³´"
      ],
      "metadata": {
        "id": "QLg7nSC6KTcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. ë¬¸ì„œ ì„ë² ë”©(í‰ê· ) ê³„ì‚°\n",
        "def get_sentence_vector(sentence, model):\n",
        "    vectors = [model.wv[word] for word in sentence if word in model.wv]\n",
        "    if not vectors:\n",
        "        return np.zeros(model.vector_size)\n",
        "    return np.mean(vectors, axis=0)\n",
        "\n",
        "sentence_vectors = np.array([get_sentence_vector(s, skipgram_model) for s in tokenized_corpus])\n",
        "print(\"\\n--- ë¬¸ì¥ ë²¡í„° í¬ê¸° ---\")\n",
        "print(sentence_vectors.shape)\n",
        "\n",
        "\n",
        "# 8. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°\n",
        "similarity_matrix = cosine_similarity(sentence_vectors)\n",
        "\n",
        "\n",
        "# 9. ìœ ì‚¬ë„ íˆíŠ¸ë§µ ì‹œê°í™”(ë¬¸ì¥ ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ íˆíŠ¸ë§µ)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(similarity_matrix, annot=True, cmap=\"YlGnBu\",\n",
        "            xticklabels=[f\"Doc {i+1}\" for i in range(len(corpus))],\n",
        "            yticklabels=[f\"Doc {i+1}\" for i in range(len(corpus))])\n",
        "plt.title(\"Cosine similarity heatmap between sentences\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nsKSS983KTkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **5. t-SNEë¡œ ë‹¨ì–´ ë²¡í„° 2D ì‹œê°í™”**\n",
        "    \n",
        "- **t-SNE**(t-Distributed Stochastic Neighbor Embedding) ëŠ” 2008ë…„ Geoffrey Hinton ì—°êµ¬íŒ€ì´ ì œì•ˆ.\n",
        "- **t-SNE**ëŠ” ê³ ì°¨ì› ë°ì´í„°ë¥¼ ì €ì°¨ì›(ë³´í†µ 2Dë‚˜ 3D)ìœ¼ë¡œ ì‹œê°í™”í•˜ê¸° ìœ„í•´ ìì£¼ ì‚¬ìš©í•˜ëŠ” ë¹„ì„ í˜• ì°¨ì› ì¶•ì†Œ  --> **í™•ë¥ ì  ë¶„í¬ê°„ ìœ ì‚¬ë„**\n",
        "- ê³ ì°¨ì›ì˜ ë‹¨ì–´ ë²¡í„°ë¥¼ 2ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œí•˜ì—¬ ì‹œê°í™”í•¨ìœ¼ë¡œì¨, **ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ ë‹¨ì–´ë“¤ì´ ì„œë¡œ ê°€ê¹ê²Œ ìœ„ì¹˜**í•˜ëŠ” ê²ƒì„ ì§ê´€ì ìœ¼ë¡œ ë³¼ ìˆ˜ ìˆìŒ\n",
        "- 'pizza'ì™€ 'pasta', 'cats'ì™€ 'dogs' ê°™ì€ ìœ ì‚¬í•œ ë‹¨ì–´ë“¤ì´ ì„œë¡œ ê°€ê¹Œìš´ ê³³ì— ìœ„ì¹˜í•˜ëŠ”ê°€???"
      ],
      "metadata": {
        "id": "pybl_aXVKTt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. t-SNEë¥¼ ì´ìš©í•œ ë‹¨ì–´ ë²¡í„° 2D ì‹œê°í™”\n",
        "\n",
        "# ë‹¨ì–´ì™€ ë²¡í„° ì¤€ë¹„\n",
        "words = [word for word in skipgram_model.wv.key_to_index if len(word) > 2]\n",
        "vectors = np.array([skipgram_model.wv[word] for word in words])  # ë¦¬ìŠ¤íŠ¸ â†’ ë„˜íŒŒì´ ë°°ì—´\n",
        "\n",
        "# t-SNE ë³€í™˜\n",
        "# tsne = TSNE(n_components=2, random_state=42, perplexity=5)\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=min(5, len(words)-1))\n",
        "\n",
        "vectors_2d = tsne.fit_transform(vectors)\n",
        "\n",
        "# ì‹œê°í™”\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1])\n",
        "\n",
        "for i, word in enumerate(words):\n",
        "    plt.annotate(word, (vectors_2d[i, 0], vectors_2d[i, 1]), ha='center', va='center')\n",
        "\n",
        "plt.title(\"Word2Vec t-SNE visualization\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8D5FftF_WlOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "GJ5vOUjDYJCO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ì˜ˆì œ: gensim ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì´ìš©í•œ Word2Vecë¡œ ë¬¸ì¥ ìœ ì‚¬ë„ í™•ì¸**\n",
        "\n",
        "- **gensim** (generate + similarity)\n",
        "    - ëŒ€ê·œëª¨ í…ìŠ¤íŠ¸ ë§ë­‰ì¹˜(Corpus)ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ê³ , í† í”½ ëª¨ë¸ë§Â·ë¬¸ì„œ ìœ ì‚¬ë„ ë¶„ì„Â·ë‹¨ì–´ ì„ë² ë”© ë“±ì„ ì†ì‰½ê²Œ ìˆ˜í–‰\n",
        "- ëŒ€í‘œê¸°ëŠ¥\n",
        "    - Word2Vec, FastText: ë‹¨ì–´ ë²¡í„° í•™ìŠµ ë° ìœ ì‚¬ì–´ ì°¾ê¸°\n",
        "    - Doc2Vec: ë¬¸ì„œ ì„ë² ë”©\n",
        "    - Topic Modeling: LDA, LSI\n",
        "    - ìœ ì‚¬ë„ ê²€ìƒ‰: ë¬¸ì„œ ê°„ ìœ ì‚¬ë„ ê³„ì‚°, ê²€ìƒ‰ ì—”ì§„ êµ¬í˜„"
      ],
      "metadata": {
        "id": "aXA7UflTYtxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install gensim==4.3.2 scipy==1.12.0"
      ],
      "metadata": {
        "id": "0zyO9cmzXu5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# ìƒ˜í”Œ ë§ë­‰ì¹˜\n",
        "sentences = [\n",
        "    [\"ì¸ê³µì§€ëŠ¥\", \"ë¨¸ì‹ ëŸ¬ë‹\", \"ë”¥ëŸ¬ë‹\"],\n",
        "    [\"ìì—°ì–´ì²˜ë¦¬\", \"ì»´í“¨í„°\", \"ì–¸ì–´\"],\n",
        "    [\"íˆ¬ì\", \"ìŠ¤íƒ€íŠ¸ì—…\", \"ë¹„ì¦ˆë‹ˆìŠ¤\"]\n",
        "]\n",
        "\n",
        "# ëª¨ë¸ í•™ìŠµ (Skip-gram)\n",
        "model = Word2Vec(\n",
        "    sentences,      # í•™ìŠµí•  ë§ë­‰ì¹˜ (í† í°í™”ëœ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸)\n",
        "    vector_size=50, # ë‹¨ì–´ ë²¡í„°ì˜ ì°¨ì›\n",
        "    window=3,       # í•™ìŠµ ì‹œ ê³ ë ¤í•  ì£¼ë³€ ë‹¨ì–´ì˜ ë²”ìœ„\n",
        "    sg=1,           # í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ ì„ íƒ (0=CBOW, 1=Skip-gram)\n",
        "    min_count=1     # ìµœì†Œ ë“±ì¥ ë¹ˆë„ (ì´ ë¯¸ë§Œ ë‹¨ì–´ëŠ” ë¬´ì‹œ)\n",
        ")\n",
        "\n",
        "\n",
        "# ìœ ì‚¬ì–´ íƒìƒ‰\n",
        "model.wv.most_similar(\"ì¸ê³µì§€ëŠ¥\")\n"
      ],
      "metadata": {
        "id": "BiQWcXsmZ71S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1. ë§ì¤‘ì¹˜ êµ¬ì„± & ì •ì²˜ë¦¬**"
      ],
      "metadata": {
        "id": "c7Fe4Dyhuha0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import math\n",
        "from collections import Counter\n",
        "from typing import List, Iterable\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 1) ë§ë­‰ì¹˜ êµ¬ì„±: TF-IDF ì˜ˆì œì˜ ë¬¸ì¥ë“¤ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš© (í•œêµ­ì–´ ìœ ì§€)\n",
        "# ---------------------------------------------------------------------\n",
        "\n",
        "documents_basic = [\n",
        "    \"ì¸ê³µì§€ëŠ¥ì€ ì»´í“¨í„° ê³¼í•™ì˜ í•œ ë¶„ì•¼ë¡œ ê¸°ê³„ê°€ ì¸ê°„ì˜ ì§€ëŠ¥ì„ ëª¨ë°©í•˜ë„ë¡ í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤\",\n",
        "    \"ë¨¸ì‹ ëŸ¬ë‹ì€ ì¸ê³µì§€ëŠ¥ì˜ í•˜ìœ„ ë¶„ì•¼ë¡œ ë°ì´í„°ë¡œë¶€í„° í•™ìŠµí•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì„ ë‹¤ë£¹ë‹ˆë‹¤\",\n",
        "    \"ë”¥ëŸ¬ë‹ì€ ë¨¸ì‹ ëŸ¬ë‹ì˜ í•œ ë°©ë²•ìœ¼ë¡œ ì‹ ê²½ë§ì„ ì´ìš©í•˜ì—¬ ë³µì¡í•œ íŒ¨í„´ì„ í•™ìŠµí•©ë‹ˆë‹¤\",\n",
        "    \"ìì—°ì–¸ì–´ì²˜ë¦¬ëŠ” ì»´í“¨í„°ê°€ ì¸ê°„ì˜ ì–¸ì–´ë¥¼ ì´í•´í•˜ê³  ìƒì„±í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤\",\n",
        "    \"ì»´í“¨í„° ë¹„ì „ì€ ì»´í“¨í„°ê°€ ì´ë¯¸ì§€ì™€ ë¹„ë””ì˜¤ë¥¼ ë¶„ì„í•˜ê³  ì´í•´í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤\"\n",
        "]\n",
        "\n",
        "documents_compare = [\n",
        "    \"ìì—°ì–¸ì–´ì²˜ë¦¬ ê¸°ìˆ ì´ ë¹ ë¥´ê²Œ ë°œì „í•˜ê³  ìˆìŠµë‹ˆë‹¤\",\n",
        "    \"ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì´ AI ë°œì „ì„ ì´ëŒê³  ìˆìŠµë‹ˆë‹¤\",\n",
        "    \"ë¹…ë°ì´í„° ë¶„ì„ì— ë‹¤ì–‘í•œ ê¸°ìˆ ì´ í™œìš©ë©ë‹ˆë‹¤\",\n",
        "    \"í´ë¼ìš°ë“œ ì»´í“¨íŒ…ì´ IT ì¸í”„ë¼ë¥¼ í˜ì‹ í•˜ê³  ìˆìŠµë‹ˆë‹¤\",\n",
        "    \"ì‚¬ë¬¼ì¸í„°ë„·ê³¼ ìŠ¤ë§ˆíŠ¸ì‹œí‹°ê°€ ì£¼ëª©ë°›ê³  ìˆìŠµë‹ˆë‹¤\"\n",
        "]\n",
        "\n",
        "tech_docs = [\n",
        "    \"ì¸ê³µì§€ëŠ¥ê³¼ ë¨¸ì‹ ëŸ¬ë‹ ê¸°ìˆ ì´ ê¸‰ì†íˆ ë°œì „í•˜ê³  ìˆì–´ ë‹¤ì–‘í•œ ì‚°ì—…ì— í˜ì‹ ì„ ê°€ì ¸ì˜¤ê³  ìˆìŠµë‹ˆë‹¤\",\n",
        "    \"ë”¥ëŸ¬ë‹ì€ ì‹ ê²½ë§ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ê¸°ê³„í•™ìŠµ ë°©ë²•ìœ¼ë¡œ ì´ë¯¸ì§€ ì¸ì‹ê³¼ ìì—°ì–¸ì–´ì²˜ë¦¬ì— ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤\",\n",
        "    \"ë¹…ë°ì´í„° ë¶„ì„ ê¸°ìˆ ì„ í†µí•´ ê¸°ì—…ë“¤ì€ ê³ ê°ì˜ í–‰ë™ íŒ¨í„´ì„ íŒŒì•…í•˜ê³  ë§ì¶¤í˜• ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤\",\n",
        "    \"í´ë¼ìš°ë“œ ì»´í“¨íŒ… ê¸°ìˆ ë¡œ ê¸°ì—…ë“¤ì€ IT ì¸í”„ë¼ ë¹„ìš©ì„ ì ˆê°í•˜ë©´ì„œë„ í™•ì¥ì„±ì„ í™•ë³´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\"\n",
        "]\n",
        "\n",
        "business_docs = [\n",
        "    \"ìŠ¤íƒ€íŠ¸ì—… ê¸°ì—…ë“¤ì´ í˜ì‹ ì ì¸ ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ë¡œ ì‹œì¥ì— ì§„ì…í•˜ì—¬ í° ì„±ê³¼ë¥¼ ê±°ë‘ê³  ìˆìŠµë‹ˆë‹¤\",\n",
        "    \"ë””ì§€í„¸ ë§ˆì¼€íŒ… ì „ëµì„ í†µí•´ ê¸°ì—…ë“¤ì€ ê³ ê°ê³¼ì˜ ì ‘ì ì„ ëŠ˜ë¦¬ê³  ë¸Œëœë“œ ì¸ì§€ë„ë¥¼ ë†’ì´ê³  ìˆìŠµë‹ˆë‹¤\",\n",
        "    \"ê¸€ë¡œë²Œ ê²½ì œ ë¶ˆí™•ì‹¤ì„± ì†ì—ì„œë„ ì¼ë¶€ ê¸°ì—…ë“¤ì€ ìƒˆë¡œìš´ ì‹œì¥ ê¸°íšŒë¥¼ ì°¾ì•„ ì„±ì¥í•˜ê³  ìˆìŠµë‹ˆë‹¤\",\n",
        "    \"íˆ¬ììë“¤ì€ ì§€ì†ê°€ëŠ¥í•œ ê²½ì˜ê³¼ ESG ìš”ì†Œë¥¼ ê³ ë ¤í•˜ì—¬ íˆ¬ì ê²°ì •ì„ ë‚´ë¦¬ëŠ” ì¶”ì„¸ì…ë‹ˆë‹¤\"\n",
        "]\n",
        "\n",
        "health_docs = [\n",
        "    \"ê°œì¸ ë§ì¶¤í˜• ì˜ë£Œ ì„œë¹„ìŠ¤ê°€ ë°œì „í•˜ë©´ì„œ ì§ˆë³‘ ì˜ˆë°©ê³¼ ì¹˜ë£Œ íš¨ê³¼ê°€ í¬ê²Œ í–¥ìƒë˜ê³  ìˆìŠµë‹ˆë‹¤\",\n",
        "    \"í—¬ìŠ¤ì¼€ì–´ ë¶„ì•¼ì— AI ê¸°ìˆ ì´ ë„ì…ë˜ì–´ ì§„ë‹¨ ì •í™•ë„ê°€ ë†’ì•„ì§€ê³  ìˆìŠµë‹ˆë‹¤\",\n",
        "    \"ì •ì‹ ê±´ê°•ì— ëŒ€í•œ ê´€ì‹¬ì´ ë†’ì•„ì§€ë©´ì„œ ë‹¤ì–‘í•œ ì¹˜ë£Œ ë°©ë²•ê³¼ ì˜ˆë°© í”„ë¡œê·¸ë¨ì´ ê°œë°œë˜ê³  ìˆìŠµë‹ˆë‹¤\",\n",
        "    \"ê³ ë ¹í™” ì‚¬íšŒì— ëŒ€ì‘í•˜ì—¬ ì‹¤ë²„ì¼€ì–´ ì‚°ì—…ì´ ìƒˆë¡œìš´ ì„±ì¥ ë™ë ¥ìœ¼ë¡œ ì£¼ëª©ë°›ê³  ìˆìŠµë‹ˆë‹¤\"\n",
        "]\n",
        "\n",
        "news_articles = [\n",
        "    {\n",
        "        'title': 'AI ê¸°ìˆ  ë°œì „ í˜„í™©',\n",
        "        'content': 'ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì´ ë¹ ë¥´ê²Œ ë°œì „í•˜ë©´ì„œ ë‹¤ì–‘í•œ ì‚°ì—… ë¶„ì•¼ì— í˜ì‹ ì„ ê°€ì ¸ì˜¤ê³  ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ ìì—°ì–¸ì–´ì²˜ë¦¬ì™€ ì»´í“¨í„° ë¹„ì „ ë¶„ì•¼ì—ì„œ ê´„ëª©í•  ë§Œí•œ ì„±ê³¼ë¥¼ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤.',\n",
        "        'category': 'ê¸°ìˆ '\n",
        "    },\n",
        "    {\n",
        "        'title': 'ìŠ¤íƒ€íŠ¸ì—… íˆ¬ì ë™í–¥',\n",
        "        'content': 'ì˜¬í•´ ìŠ¤íƒ€íŠ¸ì—…ì— ëŒ€í•œ íˆ¬ìê°€ ì „ë…„ ëŒ€ë¹„ í¬ê²Œ ì¦ê°€í–ˆìŠµë‹ˆë‹¤. íŠ¹íˆ í•€í…Œí¬ì™€ í—¬ìŠ¤ì¼€ì–´ ë¶„ì•¼ì— ëŒ€í•œ íˆ¬ììë“¤ì˜ ê´€ì‹¬ì´ ë†’ìŠµë‹ˆë‹¤.',\n",
        "        'category': 'ë¹„ì¦ˆë‹ˆìŠ¤'\n",
        "    },\n",
        "    {\n",
        "        'title': 'ê±´ê°•í•œ ì‹ë‹¨ ê´€ë¦¬ë²•',\n",
        "        'content': 'ê· í˜•ì¡íŒ ì˜ì–‘ì†Œ ì„­ì·¨ì™€ ê·œì¹™ì ì¸ ìš´ë™ì´ ê±´ê°• ê´€ë¦¬ì˜ í•µì‹¬ì…ë‹ˆë‹¤. íŠ¹íˆ í˜„ëŒ€ì¸ë“¤ì—ê²Œ ë¶€ì¡±í•œ ë¹„íƒ€ë¯¼ê³¼ ë¯¸ë„¤ë„ ì„­ì·¨ì— ì£¼ì˜í•´ì•¼ í•©ë‹ˆë‹¤.',\n",
        "        'category': 'ê±´ê°•'\n",
        "    },\n",
        "    {\n",
        "        'title': 'ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ ì†Œê°œ',\n",
        "        'content': 'ë¨¸ì‹ ëŸ¬ë‹ì€ ë°ì´í„°ë¡œë¶€í„° íŒ¨í„´ì„ í•™ìŠµí•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤. ì§€ë„í•™ìŠµ, ë¹„ì§€ë„í•™ìŠµ, ê°•í™”í•™ìŠµ ë“± ë‹¤ì–‘í•œ ë°©ë²•ë¡ ì´ ìˆìŠµë‹ˆë‹¤.',\n",
        "        'category': 'ê¸°ìˆ '\n",
        "    },\n",
        "    {\n",
        "        'title': 'ë””ì§€í„¸ ë§ˆì¼€íŒ… ì „ëµ',\n",
        "        'content': 'ì†Œì…œë¯¸ë””ì–´ì™€ ê²€ìƒ‰ì—”ì§„ ë§ˆì¼€íŒ…ì„ í†µí•´ ê³ ê°ë“¤ê³¼ì˜ ì ‘ì ì„ ëŠ˜ë¦¬ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ë°ì´í„° ë¶„ì„ì„ í†µí•œ íƒ€ê²Ÿ ë§ˆì¼€íŒ…ì´ íš¨ê³¼ì ì…ë‹ˆë‹¤.',\n",
        "        'category': 'ë¹„ì¦ˆë‹ˆìŠ¤'\n",
        "    }\n",
        "]\n",
        "\n",
        "all_docs_texts = (\n",
        "    documents_basic +\n",
        "    documents_compare +\n",
        "    tech_docs + business_docs + health_docs +\n",
        "    [a['content'] for a in news_articles]\n",
        ")\n"
      ],
      "metadata": {
        "id": "XnHtYRhJutHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2. ì „ì²˜ë¦¬ & í† í°í™” (í•œê¸€ ë³´ì¡´ + ê°„ë‹¨ ì •ê·œì‹)**"
      ],
      "metadata": {
        "id": "v1ROGQ3OuxCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------------\n",
        "# 2) ì „ì²˜ë¦¬ & í† í°í™” (í•œê¸€ ë³´ì¡´ + ê°„ë‹¨ ì •ê·œì‹)\n",
        "# ---------------------------------------------------------------------\n",
        "\n",
        "def tokenize_ko(text: str) -> List[str]:\n",
        "    # ì†Œë¬¸ì + í•œê¸€/ì˜ë¬¸/ìˆ«ì/ê³µë°±ë§Œ ë‚¨ê¹€\n",
        "    text = re.sub(r'[^\\w\\sê°€-í£]', ' ', text.lower())\n",
        "    tokens = [t for t in text.split() if t.strip()]\n",
        "    return tokens\n",
        "\n",
        "corpus: List[List[str]] = [tokenize_ko(t) for t in all_docs_texts]\n",
        "\n",
        "print(f\"ë¬¸ì„œ ìˆ˜: {len(corpus)}\")\n",
        "print(\"ìƒ˜í”Œ í† í°:\", corpus[0][:15])\n",
        "print()\n",
        "\n",
        "# corpus(ë¬¸ì„œ) ì¶œë ¥\n",
        "# for i, c in enumerate(corpus):\n",
        "#     print(f'{i:2<}, {c}')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "O2FyIRV8u25a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3. Word2Vec í•™ìŠµ** (CBOW vs Skip-gram)"
      ],
      "metadata": {
        "id": "Xa8sw19SvCZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------------\n",
        "# 3) Word2Vec í•™ìŠµ (CBOW vs Skip-gram)\n",
        "# ---------------------------------------------------------------------\n",
        "\n",
        "def train_w2v(corpus: List[List[str]], sg: int = 1, vector_size: int = 100, window: int = 5, min_count: int = 1, epochs: int = 100) -> Word2Vec:\n",
        "    model = Word2Vec(\n",
        "        sentences=corpus,\n",
        "        vector_size=vector_size,\n",
        "        window=window,\n",
        "        min_count=min_count,\n",
        "        workers=4,\n",
        "        sg=sg,              # 0: CBOW, 1: Skip-gram\n",
        "        negative=10,\n",
        "        seed=42\n",
        "    )\n",
        "    model.train(corpus, total_examples=len(corpus), epochs=epochs)\n",
        "    return model\n",
        "\n",
        "print(\"\\n[í•™ìŠµ] Skip-gram(sg=1) í•™ìŠµ ì¤‘...\")\n",
        "w2v_sg = train_w2v(corpus, sg=1, vector_size=100, window=5, min_count=1, epochs=200)\n",
        "\n",
        "print(\"[í•™ìŠµ] CBOW(sg=0) í•™ìŠµ ì¤‘...\")\n",
        "w2v_cbow = train_w2v(corpus, sg=0, vector_size=100, window=5, min_count=1, epochs=200)\n",
        "\n"
      ],
      "metadata": {
        "id": "pRQu4Wfcu76n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **4. ìœ ì‚¬ì–´/ìœ ì‚¬ë„/ìœ ì¶”(analogy)**"
      ],
      "metadata": {
        "id": "22mDxBILvHO4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------------\n",
        "# 4) ìœ ì‚¬ì–´/ìœ ì‚¬ë„/ìœ ì¶”(analogy) í…ŒìŠ¤íŠ¸\n",
        "# ---------------------------------------------------------------------\n",
        "\n",
        "def safe_most_similar(model: Word2Vec, positive: List[str], topn: int = 5):\n",
        "    try:\n",
        "        return model.wv.most_similar(positive=positive, topn=topn)\n",
        "    except KeyError as e:\n",
        "        return [(f\"âœ… [ì–´íœ˜ì—†ìŒ] {str(e)}\", 0.0)]\n",
        "\n",
        "def safe_similarity(model: Word2Vec, w1: str, w2: str) -> float:\n",
        "    try:\n",
        "        return float(model.wv.similarity(w1, w2))\n",
        "    except KeyError:\n",
        "        return float(\"nan\")\n",
        "\n",
        "def safe_analogy(model: Word2Vec, pos: Iterable[str], neg: Iterable[str], topn: int = 5):\n",
        "    try:\n",
        "        return model.wv.most_similar(positive=list(pos), negative=list(neg), topn=topn)\n",
        "    except KeyError as e:\n",
        "        return [(f\"[ì–´íœ˜ì—†ìŒ] {str(e)}\", 0.0)]\n",
        "\n",
        "\n",
        "seed_words = [\"ì¸ê³µì§€ëŠ¥\", \"ë¨¸ì‹ ëŸ¬ë‹\", \"ë”¥ëŸ¬ë‹\", \"íˆ¬ì\", \"ê±´ê°•\", \"ìì—°ì–¸ì–´ì²˜ë¦¬\", \"ë¹„ì „\"]\n",
        "\n",
        "print(\"\\nâœ… [ìœ ì‚¬ì–´ ì˜ˆì‹œ: Skip-gram]\")\n",
        "for w in seed_words:\n",
        "    if w in w2v_sg.wv:\n",
        "        sims = safe_most_similar(w2v_sg, [w], topn=5)\n",
        "        sorted_sims = sorted(sims, key=lambda x: x[1], reverse=True)\n",
        "        print(f\"  '{w}'ì™€(ê³¼) ìœ ì‚¬í•œ ë‹¨ì–´:\", sorted_sims)\n",
        "\n",
        "print(\"\\nâœ… [ë‹¨ì–´ ìœ ì‚¬ë„ ì˜ˆì‹œ: Skip-gram]\")\n",
        "pairs = [(\"ì¸ê³µì§€ëŠ¥\",\"ë¨¸ì‹ ëŸ¬ë‹\"), (\"ë¨¸ì‹ ëŸ¬ë‹\",\"ë”¥ëŸ¬ë‹\"), (\"íˆ¬ì\",\"ë¹„ì¦ˆë‹ˆìŠ¤\"), (\"ìì—°ì–¸ì–´ì²˜ë¦¬\",\"ë¹„ì „\")]\n",
        "for a,b in pairs:\n",
        "    print(f\"  sim({a}, {b}) = {safe_similarity(w2v_sg, a, b):.4f}\")\n",
        "\n",
        "print(\"\\nâœ… [ê°„ë‹¨ ìœ ì¶”(Analogy) ì˜ˆì‹œ: Skip-gram]\")\n",
        "# 'ë¨¸ì‹ ëŸ¬ë‹ - ì§€ë„í•™ìŠµ + ë¹„ì§€ë„í•™ìŠµ' ì€ ë§ë­‰ì¹˜ì— ë‹¨ì–´ê°€ ë¶€ì¡±í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì•ˆì „í•˜ê²Œ ì²´í¬\n",
        "analogy_tests = [\n",
        "    ([\"ì¸ê³µì§€ëŠ¥\",\"ë”¥ëŸ¬ë‹\"], [\"ë¨¸ì‹ ëŸ¬ë‹\"]),   # ì¸ê³µì§€ëŠ¥ + ë”¥ëŸ¬ë‹ - ë¨¸ì‹ ëŸ¬ë‹ â‰ˆ ?\n",
        "    ([\"íˆ¬ì\",\"ê¸°ì—…\"], [\"ê³ ê°\"]),           # íˆ¬ì + ê¸°ì—… - ê³ ê° â‰ˆ ?\n",
        "]\n",
        "for pos, neg in analogy_tests:\n",
        "    print(f\"  +{pos} -{neg} =>\", safe_analogy(w2v_sg, pos, neg, topn=5))\n"
      ],
      "metadata": {
        "id": "O4VX_SsLvQ2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **5. ë¬¸ì„œ ì„ë² ë”©(í‰ê· ) ìœ ì‚¬ë„ íˆíŠ¸ë§µ ì‹œê°í™”**"
      ],
      "metadata": {
        "id": "fzIva1XXvYxf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------------\n",
        "# 5) ë¬¸ì„œ ì„ë² ë”©(í‰ê· ) ìœ ì‚¬ë„ íˆíŠ¸ë§µ\n",
        "# ---------------------------------------------------------------------\n",
        "\n",
        "def doc_embedding(model: Word2Vec, tokens: List[str]) -> np.ndarray:\n",
        "    vecs = [model.wv[w] for w in tokens if w in model.wv]\n",
        "    if not vecs:\n",
        "        return np.zeros(model.vector_size, dtype=float)\n",
        "    return np.mean(vecs, axis=0)\n",
        "\n",
        "doc_titles = (\n",
        "    [f\"basic{i+1}\" for i in range(len(documents_basic))] +\n",
        "    [f\"cmp{i+1}\" for i in range(len(documents_compare))] +\n",
        "    [f\"tech{i+1}\" for i in range(len(tech_docs))] +\n",
        "    [f\"biz{i+1}\" for i in range(len(business_docs))] +\n",
        "    [f\"health{i+1}\" for i in range(len(health_docs))] +\n",
        "    [f\"news{i+1}\" for i in range(len(news_articles))]\n",
        ")\n",
        "\n",
        "doc_vecs = np.vstack([doc_embedding(w2v_sg, tokens) for tokens in corpus])\n",
        "sim_mat = cosine_similarity(doc_vecs)\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.imshow(sim_mat, aspect='auto')\n",
        "plt.title('Document Similarity (Avg Word2Vec, cosine)')\n",
        "plt.colorbar()\n",
        "plt.xticks(range(len(doc_titles)), doc_titles, rotation=90, fontsize=8)\n",
        "plt.yticks(range(len(doc_titles)), doc_titles, fontsize=8)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6Wx5m1xNvfOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "9FeIRlN_YHM9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ì˜ˆì œ : Word2Vec Quicktour**"
      ],
      "metadata": {
        "id": "Fn6kTUL0d8XY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Word2Vec ê°„ë‹¨ í…ŒìŠ¤íŠ¸\n",
        "import numpy as np\n",
        "from collections import Counter, defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def simple_word2vec_demo():\n",
        "    \"\"\"\n",
        "    Word2Vec ê¸°ë³¸ ê°œë…ì„ ë³´ì—¬ì£¼ëŠ” ê°„ë‹¨í•œ ì˜ˆì œ\n",
        "    \"\"\"\n",
        "    print(\"ğŸ§  Word2Vec ê¸°ë³¸ ê°œë… ì´í•´í•˜ê¸°\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # ì˜ˆì œ ë¬¸ì„œë“¤\n",
        "    sentences = [\n",
        "        \"ì™•ê³¼ ì—¬ì™•ì€ ì™•ê¶ì— ì‚½ë‹ˆë‹¤\",\n",
        "        \"ë‚¨ìì™€ ì—¬ìëŠ” ì‚¬ëŒì…ë‹ˆë‹¤\",\n",
        "        \"ì•„ë²„ì§€ì™€ ì–´ë¨¸ë‹ˆëŠ” ë¶€ëª¨ì…ë‹ˆë‹¤\",\n",
        "        \"ì•„ë“¤ê³¼ ë”¸ì€ ìë…€ì…ë‹ˆë‹¤\",\n",
        "        \"í˜•ê³¼ ëˆ„ë‚˜ëŠ” í˜•ì œìë§¤ì…ë‹ˆë‹¤\"\n",
        "    ]\n",
        "\n",
        "    print(\"ğŸ“„ ë¶„ì„í•  ë¬¸ì¥ë“¤:\")\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        print(f\"  ë¬¸ì¥{i+1}: {sentence}\")\n",
        "\n",
        "    #-------------------------------------\n",
        "    # 1ë‹¨ê³„: ì–´íœ˜ êµ¬ì¶•\n",
        "    #-------------------------------------\n",
        "    all_words = set()\n",
        "    processed_sentences = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        # ê°„ë‹¨í•œ ì „ì²˜ë¦¬\n",
        "        words = sentence.replace('ê³¼', '').replace('ëŠ”', '').replace('ì—', '').split()\n",
        "        processed_sentences.append(words)\n",
        "        all_words.update(words)\n",
        "\n",
        "    vocab = sorted(list(all_words))\n",
        "    word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "\n",
        "    print(f\"\\nğŸ“š êµ¬ì¶•ëœ ì–´íœ˜ ({len(vocab)}ê°œ):\")\n",
        "    print(f\"  {vocab}\")\n",
        "\n",
        "    #-------------------------------------\n",
        "    # 2ë‹¨ê³„: Skip-gram í•™ìŠµ ë°ì´í„° ìƒì„±\n",
        "    #-------------------------------------\n",
        "    print(f\"\\nğŸ¯ Skip-gram í•™ìŠµ ë°ì´í„° ìƒì„± (ìœˆë„ìš° í¬ê¸°: 2):\")\n",
        "\n",
        "    training_pairs = []\n",
        "    for sentence_words in processed_sentences:\n",
        "        for i, target_word in enumerate(sentence_words):\n",
        "            # ìœˆë„ìš° ë²”ìœ„ ì„¤ì •\n",
        "            start = max(0, i - 2)\n",
        "            end = min(len(sentence_words), i + 3)\n",
        "\n",
        "            for j in range(start, end):\n",
        "                if j != i:  # íƒ€ê²Ÿ ë‹¨ì–´ ì œì™¸\n",
        "                    context_word = sentence_words[j]\n",
        "                    training_pairs.append((target_word, context_word))\n",
        "\n",
        "    print(f\"  ì´ {len(training_pairs)}ê°œì˜ í•™ìŠµ ìŒ ìƒì„±\")\n",
        "    print(\"  ì˜ˆì‹œ í•™ìŠµ ìŒë“¤:\")\n",
        "    for i, (target, context) in enumerate(training_pairs[:8]):\n",
        "        print(f\"    {target} â†’ {context}\")\n",
        "    if len(training_pairs) > 8:\n",
        "        print(f\"    ... ì™¸ {len(training_pairs)-8}ê°œ\")\n",
        "\n",
        "    #-------------------------------------\n",
        "    # 3ë‹¨ê³„: ê°„ë‹¨í•œ ë²¡í„° ì´ˆê¸°í™” ë° í•™ìŠµ ì‹œë®¬ë ˆì´ì…˜\n",
        "    #-------------------------------------\n",
        "    print(f\"\\nâš¡ ë²¡í„° í•™ìŠµ ì‹œë®¬ë ˆì´ì…˜:\")\n",
        "\n",
        "    vector_size = 4  # ê°„ë‹¨í•œ ì‹œê°í™”ë¥¼ ìœ„í•´ 4ì°¨ì›\n",
        "    word_vectors = {}\n",
        "\n",
        "    # ëœë¤ ì´ˆê¸°í™”\n",
        "    np.random.seed(42)  # ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼\n",
        "    for word in vocab:\n",
        "        word_vectors[word] = np.random.normal(0, 0.1, vector_size)\n",
        "\n",
        "    print(\"  ì´ˆê¸° ë²¡í„° (ì¼ë¶€):\")\n",
        "    for word in vocab[:5]:\n",
        "        print(f\"    {word}: [{', '.join([f'{x:.3f}' for x in word_vectors[word]])}]\")\n",
        "\n",
        "    # ê°„ë‹¨í•œ í•™ìŠµ ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œ ì—­ì „íŒŒëŠ” ë³µì¡í•˜ë¯€ë¡œ ê°œë…ì  ì„¤ëª…)\n",
        "    print(f\"\\nğŸ“ˆ í•™ìŠµ ê³¼ì • ì‹œë®¬ë ˆì´ì…˜:\")\n",
        "    print(\"  (ì‹¤ì œë¡œëŠ” ê²½ì‚¬í•˜ê°•ë²•ê³¼ ì—­ì „íŒŒë¥¼ í†µí•´ ë²¡í„°ê°€ ì—…ë°ì´íŠ¸ë©ë‹ˆë‹¤)\")\n",
        "\n",
        "    # ìœ ì‚¬í•œ ë‹¨ì–´ë“¤ì„ ê°€ê¹Œì´ ë°°ì¹˜í•˜ëŠ” ì‹œë®¬ë ˆì´ì…˜\n",
        "    similar_pairs = [\n",
        "        (\"ì™•\", \"ì—¬ì™•\"), (\"ë‚¨ì\", \"ì—¬ì\"), (\"ì•„ë²„ì§€\", \"ì–´ë¨¸ë‹ˆ\"), (\"ì•„ë“¤\", \"ë”¸\"), (\"í˜•\", \"ëˆ„ë‚˜\")\n",
        "    ]\n",
        "\n",
        "    for word1, word2 in similar_pairs:\n",
        "        if word1 in word_vectors and word2 in word_vectors:\n",
        "            # ìœ ì‚¬í•œ ë‹¨ì–´ë“¤ì˜ ë²¡í„°ë¥¼ ì ì§„ì ìœ¼ë¡œ ê°€ê¹ê²Œ ë§Œë“¤ê¸°\n",
        "            avg_vector = (word_vectors[word1] + word_vectors[word2]) / 2\n",
        "            word_vectors[word1] = 0.7 * word_vectors[word1] + 0.3 * avg_vector\n",
        "            word_vectors[word2] = 0.7 * word_vectors[word2] + 0.3 * avg_vector\n",
        "\n",
        "    print(\"  í•™ìŠµ í›„ ë²¡í„° (ì¼ë¶€):\")\n",
        "    for word in vocab[:5]:\n",
        "        print(f\"    {word}: [{', '.join([f'{x:.3f}' for x in word_vectors[word]])}]\")\n",
        "\n",
        "    #-------------------------------------\n",
        "    # 4ë‹¨ê³„: ìœ ì‚¬ë„ ê³„ì‚°\n",
        "    #-------------------------------------\n",
        "    def cosine_similarity(vec1, vec2):\n",
        "        dot_product = np.dot(vec1, vec2)\n",
        "        norm1 = np.linalg.norm(vec1)\n",
        "        norm2 = np.linalg.norm(vec2)\n",
        "        return dot_product / (norm1 * norm2) if norm1 > 0 and norm2 > 0 else 0\n",
        "\n",
        "    print(f\"\\nğŸ”— ë‹¨ì–´ ê°„ ìœ ì‚¬ë„ ê³„ì‚°:\")\n",
        "\n",
        "    test_pairs = [\n",
        "        (\"ì™•\", \"ì—¬ì™•\"),\n",
        "        (\"ë‚¨ì\", \"ì—¬ì\"),\n",
        "        (\"ì•„ë²„ì§€\", \"ì–´ë¨¸ë‹ˆ\"),\n",
        "        (\"ì™•\", \"ë‚¨ì\"),\n",
        "        (\"ì™•\", \"ë”¸\")\n",
        "    ]\n",
        "\n",
        "    for word1, word2 in test_pairs:\n",
        "        if word1 in word_vectors and word2 in word_vectors:\n",
        "            similarity = cosine_similarity(word_vectors[word1], word_vectors[word2])\n",
        "            print(f\"  {word1} - {word2}: {similarity:.4f}\")\n",
        "\n",
        "    #-------------------------------------\n",
        "    # 5ë‹¨ê³„: ì‹œê°í™” (2ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œ)\n",
        "    #-------------------------------------\n",
        "    from sklearn.decomposition import PCA\n",
        "\n",
        "    # ë²¡í„°ë“¤ì„ ë°°ì—´ë¡œ ë³€í™˜\n",
        "    vectors = np.array([word_vectors[word] for word in vocab])\n",
        "\n",
        "    # PCAë¡œ 2ì°¨ì› ì¶•ì†Œ\n",
        "    pca = PCA(n_components=2)\n",
        "    vectors_2d = pca.fit_transform(vectors)\n",
        "\n",
        "    # ì‹œê°í™”\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1], s=100, alpha=0.7, c='skyblue')\n",
        "\n",
        "    # ë‹¨ì–´ ë¼ë²¨ ì¶”ê°€\n",
        "    for i, word in enumerate(vocab):\n",
        "        plt.annotate(word, (vectors_2d[i, 0], vectors_2d[i, 1]),\n",
        "                    xytext=(5, 5), textcoords='offset points', fontsize=12)\n",
        "\n",
        "    plt.title('Word2Vec ë²¡í„° ì‹œê°í™” (PCA 2ì°¨ì› ì¶•ì†Œ)', fontsize=16)\n",
        "    plt.xlabel('PC1')\n",
        "    plt.ylabel('PC2')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"\\nğŸ’¡ Word2Vec í•µì‹¬ ì›ë¦¬:\")\n",
        "    print(\"  âœ… ë¹„ìŠ·í•œ ë§¥ë½ì—ì„œ ë‚˜íƒ€ë‚˜ëŠ” ë‹¨ì–´ë“¤ì€ ë¹„ìŠ·í•œ ë²¡í„°ë¥¼ ê°€ì§‘ë‹ˆë‹¤\")\n",
        "    print(\"  âœ… ë²¡í„° ê°„ ê±°ë¦¬ê°€ ê°€ê¹Œìš°ë©´ ì˜ë¯¸ê°€ ìœ ì‚¬í•©ë‹ˆë‹¤\")\n",
        "    print(\"  âœ… ë²¡í„° ì—°ì‚°ìœ¼ë¡œ ë‹¨ì–´ ê´€ê³„ë¥¼ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\")\n",
        "\n",
        "    return word_vectors\n",
        "\n",
        "\n",
        "def gensim_word2vec_example():\n",
        "    \"\"\"\n",
        "    Gensimì„ ì‚¬ìš©í•œ ì‹¤ì œ Word2Vec ì˜ˆì œ\n",
        "    \"\"\"\n",
        "    print(f\"\\n\" + \"=\" * 50)\n",
        "    print(\"ğŸ”¬ Gensim Word2Vec ì‹¤ìŠµ\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    try:\n",
        "        from gensim.models import Word2Vec\n",
        "\n",
        "        # í•œêµ­ì–´ ë¬¸ì¥ë“¤ (í† í°í™”ëœ í˜•íƒœ)\n",
        "        sentences = [\n",
        "            ['ì‚¬ê³¼', 'ë°”ë‚˜ë‚˜', 'ê³¼ì¼', 'ë§›ìˆë‹¤'],\n",
        "            ['ë”¸ê¸°', 'í¬ë„', 'ê³¼ì¼', 'ë‹¬ë‹¤'],\n",
        "            ['ê³¼ì¼', 'ê±´ê°•', 'ì¢‹ë‹¤', 'ë¹„íƒ€ë¯¼'],\n",
        "            ['ê°œ', 'ê³ ì–‘ì´', 'ë™ë¬¼', 'ê·€ì—½ë‹¤'],\n",
        "            ['í† ë¼', 'í–„ìŠ¤í„°', 'ë™ë¬¼', 'ì‘ë‹¤'],\n",
        "            ['ë™ë¬¼', 'ìƒëª…', 'ì†Œì¤‘í•˜ë‹¤', 'ë³´í˜¸'],\n",
        "            ['ìë™ì°¨', 'ê¸°ì°¨', 'êµí†µìˆ˜ë‹¨', 'ë¹ ë¥´ë‹¤'],\n",
        "            ['ë²„ìŠ¤', 'ì§€í•˜ì² ', 'êµí†µìˆ˜ë‹¨', 'í¸ë¦¬í•˜ë‹¤'],\n",
        "            ['êµí†µìˆ˜ë‹¨', 'ì´ë™', 'í•„ìš”í•˜ë‹¤', 'ì¤‘ìš”'],\n",
        "            ['ì»´í“¨í„°', 'ìŠ¤ë§ˆíŠ¸í°', 'ì „ìê¸°ê¸°', 'ìœ ìš©í•˜ë‹¤'],\n",
        "            ['íƒœë¸”ë¦¿', 'ë…¸íŠ¸ë¶', 'ì „ìê¸°ê¸°', 'í¸ë¦¬í•˜ë‹¤'],\n",
        "            ['ì „ìê¸°ê¸°', 'ê¸°ìˆ ', 'ë°œì „', 'ë†€ëë‹¤']\n",
        "        ]\n",
        "\n",
        "        print(\"ğŸ“š í•™ìŠµ ë°ì´í„°:\")\n",
        "        for i, sentence in enumerate(sentences[:5]):\n",
        "            print(f\"  ë¬¸ì¥{i+1}: {' '.join(sentence)}\")\n",
        "        print(f\"  ... ì´ {len(sentences)}ê°œ ë¬¸ì¥\")\n",
        "\n",
        "        # Word2Vec ëª¨ë¸ í•™ìŠµ\n",
        "        model = Word2Vec(\n",
        "            sentences=sentences,\n",
        "            vector_size=50,      # ë²¡í„° ì°¨ì›\n",
        "            window=3,           # ìœˆë„ìš° í¬ê¸°\n",
        "            min_count=1,        # ìµœì†Œ ë¹ˆë„\n",
        "            workers=1,          # ë³‘ë ¬ ì²˜ë¦¬ ìˆ˜\n",
        "            sg=1,              # Skip-gram (0ì´ë©´ CBOW)\n",
        "            epochs=100\n",
        "        )\n",
        "\n",
        "        print(f\"\\nâœ… Word2Vec ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\")\n",
        "        print(f\"   ì–´íœ˜ í¬ê¸°: {len(model.wv.key_to_index)}\")\n",
        "        print(f\"   ë²¡í„° ì°¨ì›: {model.wv.vector_size}\")\n",
        "\n",
        "        # 1. ìœ ì‚¬í•œ ë‹¨ì–´ ì°¾ê¸°\n",
        "        print(f\"\\nğŸ” ìœ ì‚¬í•œ ë‹¨ì–´ ì°¾ê¸°:\")\n",
        "        test_words = ['ê³¼ì¼', 'ë™ë¬¼', 'êµí†µìˆ˜ë‹¨', 'ì „ìê¸°ê¸°']\n",
        "\n",
        "        for word in test_words:\n",
        "            if word in model.wv:\n",
        "                try:\n",
        "                    similar_words = model.wv.most_similar(word, topn=3)\n",
        "                    print(f\"  '{word}' ìœ ì‚¬ ë‹¨ì–´: {similar_words}\")\n",
        "                except:\n",
        "                    print(f\"  '{word}': ìœ ì‚¬ ë‹¨ì–´ ê³„ì‚° ì‹¤íŒ¨\")\n",
        "\n",
        "        # 2. ë‹¨ì–´ ê°„ ìœ ì‚¬ë„\n",
        "        print(f\"\\nğŸ¯ ë‹¨ì–´ ê°„ ìœ ì‚¬ë„:\")\n",
        "        word_pairs = [\n",
        "            ('ì‚¬ê³¼', 'ë°”ë‚˜ë‚˜'),      # ê°™ì€ ì¹´í…Œê³ ë¦¬\n",
        "            ('ê°œ', 'ê³ ì–‘ì´'),        # ê°™ì€ ì¹´í…Œê³ ë¦¬\n",
        "            ('ìë™ì°¨', 'ì»´í“¨í„°'),    # ë‹¤ë¥¸ ì¹´í…Œê³ ë¦¬\n",
        "            ('ê³¼ì¼', 'ë™ë¬¼')         # ë‹¤ë¥¸ ì¹´í…Œê³ ë¦¬\n",
        "        ]\n",
        "\n",
        "        for word1, word2 in word_pairs:\n",
        "            if word1 in model.wv and word2 in model.wv:\n",
        "                similarity = model.wv.similarity(word1, word2)\n",
        "                print(f\"  {word1} - {word2}: {similarity:.4f}\")\n",
        "\n",
        "        # 3. ë‹¨ì–´ ë²¡í„° ì‹œê°í™”\n",
        "        print(f\"\\nğŸ“Š ë²¡í„° ì‹œê°í™”:\")\n",
        "\n",
        "        # ì£¼ìš” ë‹¨ì–´ë“¤ì˜ ë²¡í„° ì¶”ì¶œ\n",
        "        words_to_plot = ['ê³¼ì¼', 'ì‚¬ê³¼', 'ë°”ë‚˜ë‚˜', 'ë™ë¬¼', 'ê°œ', 'ê³ ì–‘ì´',\n",
        "                        'êµí†µìˆ˜ë‹¨', 'ìë™ì°¨', 'ì „ìê¸°ê¸°', 'ì»´í“¨í„°']\n",
        "\n",
        "        vectors = []\n",
        "        labels = []\n",
        "\n",
        "        for word in words_to_plot:\n",
        "            if word in model.wv:\n",
        "                vectors.append(model.wv[word])\n",
        "                labels.append(word)\n",
        "\n",
        "        # PCAë¡œ 2ì°¨ì› ì¶•ì†Œ\n",
        "        from sklearn.decomposition import PCA\n",
        "\n",
        "        vectors = np.array(vectors)\n",
        "        pca = PCA(n_components=2)\n",
        "        vectors_2d = pca.fit_transform(vectors)\n",
        "\n",
        "        # ì¹´í…Œê³ ë¦¬ë³„ ìƒ‰ìƒ\n",
        "        categories = {\n",
        "            'ê³¼ì¼': ['ê³¼ì¼', 'ì‚¬ê³¼', 'ë°”ë‚˜ë‚˜'],\n",
        "            'ë™ë¬¼': ['ë™ë¬¼', 'ê°œ', 'ê³ ì–‘ì´'],\n",
        "            'êµí†µìˆ˜ë‹¨': ['êµí†µìˆ˜ë‹¨', 'ìë™ì°¨'],\n",
        "            'ì „ìê¸°ê¸°': ['ì „ìê¸°ê¸°', 'ì»´í“¨í„°']\n",
        "        }\n",
        "\n",
        "        colors = {'ê³¼ì¼': 'red', 'ë™ë¬¼': 'blue', 'êµí†µìˆ˜ë‹¨': 'green', 'ì „ìê¸°ê¸°': 'orange'}\n",
        "\n",
        "        plt.figure(figsize=(12, 8))\n",
        "\n",
        "        for category, words_in_cat in categories.items():\n",
        "            indices = [i for i, word in enumerate(labels) if word in words_in_cat]\n",
        "            if indices:\n",
        "                plt.scatter(vectors_2d[indices, 0], vectors_2d[indices, 1],\n",
        "                           c=colors[category], label=category, s=100, alpha=0.7)\n",
        "\n",
        "        # ë‹¨ì–´ ë¼ë²¨ ì¶”ê°€\n",
        "        for i, word in enumerate(labels):\n",
        "            plt.annotate(word, (vectors_2d[i, 0], vectors_2d[i, 1]),\n",
        "                        xytext=(5, 5), textcoords='offset points', fontsize=10)\n",
        "\n",
        "        plt.title('Gensim Word2Vec ë²¡í„° ì‹œê°í™”', fontsize=16)\n",
        "        plt.xlabel('PC1')\n",
        "        plt.ylabel('PC2')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.show()\n",
        "\n",
        "        print(\"âœ… ì‹œê°í™” ì™„ë£Œ! ê°™ì€ ì¹´í…Œê³ ë¦¬ì˜ ë‹¨ì–´ë“¤ì´ ê°€ê¹Œì´ ëª¨ì—¬ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\")\n",
        "\n",
        "        return model\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"âŒ Gensimì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
        "        print(\"ì„¤ì¹˜: pip install gensim\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def word2vec_applications_demo():\n",
        "    \"\"\"\n",
        "    Word2Vec ì‹¤ì „ ì‘ìš© ì˜ˆì œ\n",
        "    \"\"\"\n",
        "    print(f\"\\n\" + \"=\" * 50)\n",
        "    print(\"ğŸš€ Word2Vec ì‹¤ì „ ì‘ìš©\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    try:\n",
        "        from gensim.models import Word2Vec\n",
        "        from sklearn.cluster import KMeans\n",
        "\n",
        "        # ë” í° ë°ì´í„°ì…‹\n",
        "        extended_sentences = [\n",
        "            # ìŒì‹ ê´€ë ¨\n",
        "            ['í•œì‹', 'ê¹€ì¹˜', 'ëœì¥ì°Œê°œ', 'ë§›ìˆë‹¤', 'ì „í†µ', 'ìŒì‹'],\n",
        "            ['ì¤‘ì‹', 'ì§œì¥ë©´', 'íƒ•ìˆ˜ìœ¡', 'ë§›ìˆë‹¤', 'ì¤‘êµ­', 'ìŒì‹'],\n",
        "            ['ì¼ì‹', 'ì´ˆë°¥', 'ë¼ë©˜', 'ì‹ ì„ í•˜ë‹¤', 'ì¼ë³¸', 'ìŒì‹'],\n",
        "            ['ì–‘ì‹', 'íŒŒìŠ¤íƒ€', 'í”¼ì', 'ë§›ìˆë‹¤', 'ì„œì–‘', 'ìŒì‹'],\n",
        "            ['ìŒì‹', 'ìš”ë¦¬', 'ë§›', 'ê±´ê°•', 'ì˜ì–‘', 'ì¤‘ìš”'],\n",
        "\n",
        "            # ìŠ¤í¬ì¸  ê´€ë ¨\n",
        "            ['ì¶•êµ¬', 'ì›”ë“œì»µ', 'ì„ ìˆ˜', 'ê²½ê¸°', 'ê³¨', 'ìŠ¹ë¶€'],\n",
        "            ['ì•¼êµ¬', 'í™ˆëŸ°', 'íˆ¬ìˆ˜', 'íƒ€ì', 'ê²½ê¸°ì¥', 'ì‘ì›'],\n",
        "            ['ë†êµ¬', 'ë©í¬ìŠ›', 'ì„ ìˆ˜', 'ì½”íŠ¸', 'íŒ€ì›Œí¬', 'ì „ëµ'],\n",
        "            ['í…Œë‹ˆìŠ¤', 'ë¼ì¼“', 'ì„ ìˆ˜', 'ì½”íŠ¸', 'ì„œë¸Œ', 'ê²½ê¸°'],\n",
        "            ['ìŠ¤í¬ì¸ ', 'ìš´ë™', 'ê±´ê°•', 'ì²´ë ¥', 'íŒ€ì›Œí¬', 'ì¤‘ìš”'],\n",
        "\n",
        "            # ê¸°ìˆ  ê´€ë ¨\n",
        "            ['ì»´í“¨í„°', 'í”„ë¡œê·¸ë˜ë°', 'ì†Œí”„íŠ¸ì›¨ì–´', 'ê°œë°œ', 'ê¸°ìˆ ', 'í˜ì‹ '],\n",
        "            ['ìŠ¤ë§ˆíŠ¸í°', 'ì•±', 'ëª¨ë°”ì¼', 'í¸ë¦¬', 'í†µì‹ ', 'ê¸°ìˆ '],\n",
        "            ['ì¸í„°ë„·', 'ì›¹ì‚¬ì´íŠ¸', 'ì •ë³´', 'ê²€ìƒ‰', 'ì—°ê²°', 'ë„¤íŠ¸ì›Œí¬'],\n",
        "            ['ì¸ê³µì§€ëŠ¥', 'ë¨¸ì‹ ëŸ¬ë‹', 'ë°ì´í„°', 'í•™ìŠµ', 'ë¯¸ë˜', 'ê¸°ìˆ '],\n",
        "            ['ê¸°ìˆ ', 'ë°œì „', 'í˜ì‹ ', 'ë¯¸ë˜', 'ì‚¬íšŒ', 'ë³€í™”']\n",
        "        ]\n",
        "\n",
        "        print(f\"ğŸ“š í™•ì¥ ë°ì´í„°ì…‹ìœ¼ë¡œ í•™ìŠµ (ì´ {len(extended_sentences)}ê°œ ë¬¸ì¥)\")\n",
        "\n",
        "        # ëª¨ë¸ í•™ìŠµ\n",
        "        app_model = Word2Vec(\n",
        "            sentences=extended_sentences,\n",
        "            vector_size=100,\n",
        "            window=4,\n",
        "            min_count=1,\n",
        "            workers=1,\n",
        "            sg=1,\n",
        "            epochs=200\n",
        "        )\n",
        "\n",
        "        print(\"âœ… í™•ì¥ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\")\n",
        "\n",
        "        # 1. ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜\n",
        "        print(f\"\\nğŸ“‚ ìë™ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜:\")\n",
        "\n",
        "        # ì£¼ìš” ë‹¨ì–´ë“¤ì˜ ë²¡í„° ì¶”ì¶œ\n",
        "        food_words = ['í•œì‹', 'ì¤‘ì‹', 'ì¼ì‹', 'ì–‘ì‹', 'ê¹€ì¹˜', 'ì§œì¥ë©´', 'ì´ˆë°¥', 'íŒŒìŠ¤íƒ€']\n",
        "        sports_words = ['ì¶•êµ¬', 'ì•¼êµ¬', 'ë†êµ¬', 'í…Œë‹ˆìŠ¤', 'ì„ ìˆ˜', 'ê²½ê¸°', 'ê³¨', 'í™ˆëŸ°']\n",
        "        tech_words = ['ì»´í“¨í„°', 'ìŠ¤ë§ˆíŠ¸í°', 'ì¸í„°ë„·', 'ì¸ê³µì§€ëŠ¥', 'í”„ë¡œê·¸ë˜ë°', 'ì•±']\n",
        "\n",
        "        all_words = food_words + sports_words + tech_words\n",
        "        vectors = []\n",
        "        labels = []\n",
        "        categories = []\n",
        "\n",
        "        for word in all_words:\n",
        "            if word in app_model.wv:\n",
        "                vectors.append(app_model.wv[word])\n",
        "                labels.append(word)\n",
        "                if word in food_words:\n",
        "                    categories.append('ìŒì‹')\n",
        "                elif word in sports_words:\n",
        "                    categories.append('ìŠ¤í¬ì¸ ')\n",
        "                else:\n",
        "                    categories.append('ê¸°ìˆ ')\n",
        "\n",
        "        # K-means í´ëŸ¬ìŠ¤í„°ë§\n",
        "        vectors = np.array(vectors)\n",
        "        kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
        "        clusters = kmeans.fit_predict(vectors)\n",
        "\n",
        "        print(\"  í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼:\")\n",
        "        cluster_words = defaultdict(list)\n",
        "        for word, cluster, true_cat in zip(labels, clusters, categories):\n",
        "            cluster_words[cluster].append(f\"{word}({true_cat})\")\n",
        "\n",
        "        for cluster_id, words in cluster_words.items():\n",
        "            print(f\"    í´ëŸ¬ìŠ¤í„° {cluster_id}: {words}\")\n",
        "\n",
        "        # 2. ë¬¸ì„œ ìœ ì‚¬ë„ ê³„ì‚°\n",
        "        print(f\"\\nğŸ“„ ë¬¸ì„œ ìœ ì‚¬ë„ ê³„ì‚°:\")\n",
        "\n",
        "        test_documents = [\n",
        "            \"í•œì‹ ê¹€ì¹˜ ëœì¥ì°Œê°œ ë§›ìˆë‹¤\",\n",
        "            \"ì¶•êµ¬ ì„ ìˆ˜ ì›”ë“œì»µ ê²½ê¸°\",\n",
        "            \"ì»´í“¨í„° í”„ë¡œê·¸ë˜ë° ì†Œí”„íŠ¸ì›¨ì–´\",\n",
        "            \"ì¤‘ì‹ ì§œì¥ë©´ ë§›ìˆë‹¤\"\n",
        "        ]\n",
        "\n",
        "        def document_vector(doc, model):\n",
        "            \"\"\"ë¬¸ì„œë¥¼ ë²¡í„°ë¡œ ë³€í™˜ (ë‹¨ì–´ ë²¡í„°ë“¤ì˜ í‰ê· )\"\"\"\n",
        "            words = doc.split()\n",
        "            vectors = []\n",
        "            for word in words:\n",
        "                if word in model.wv:\n",
        "                    vectors.append(model.wv[word])\n",
        "\n",
        "            if vectors:\n",
        "                return np.mean(vectors, axis=0)\n",
        "            else:\n",
        "                return np.zeros(model.wv.vector_size)\n",
        "\n",
        "        doc_vectors = [document_vector(doc, app_model) for doc in test_documents]\n",
        "\n",
        "        # ë¬¸ì„œ ê°„ ìœ ì‚¬ë„ ê³„ì‚°\n",
        "        from sklearn.metrics.pairwise import cosine_similarity\n",
        "        doc_similarity = cosine_similarity(doc_vectors)\n",
        "\n",
        "        print(\"  ë¬¸ì„œ ê°„ ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤:\")\n",
        "        for i, doc in enumerate(test_documents):\n",
        "            print(f\"  ë¬¸ì„œ{i+1}: {doc}\")\n",
        "\n",
        "        print(f\"\\n  ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤:\")\n",
        "        for i in range(len(test_documents)):\n",
        "            for j in range(len(test_documents)):\n",
        "                print(f\"  {doc_similarity[i][j]:.3f}\", end=\"\")\n",
        "            print()\n",
        "\n",
        "        # 3. ì‹œê°í™”\n",
        "        plt.figure(figsize=(15, 5))\n",
        "\n",
        "        # í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ì‹œê°í™”\n",
        "        plt.subplot(1, 3, 1)\n",
        "\n",
        "        from sklearn.decomposition import PCA\n",
        "        pca = PCA(n_components=2)\n",
        "        vectors_2d = pca.fit_transform(vectors)\n",
        "\n",
        "        colors = ['red', 'blue', 'green']\n",
        "        for i, cluster in enumerate(set(clusters)):\n",
        "            indices = [j for j, c in enumerate(clusters) if c == cluster]\n",
        "            plt.scatter(vectors_2d[indices, 0], vectors_2d[indices, 1],\n",
        "                       c=colors[i], label=f'í´ëŸ¬ìŠ¤í„° {cluster}', s=100, alpha=0.7)\n",
        "\n",
        "        plt.title('Word2Vec í´ëŸ¬ìŠ¤í„°ë§', fontsize=12)\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # ì¹´í…Œê³ ë¦¬ë³„ ìƒ‰ìƒ\n",
        "        plt.subplot(1, 3, 2)\n",
        "\n",
        "        category_colors = {'ìŒì‹': 'red', 'ìŠ¤í¬ì¸ ': 'blue', 'ê¸°ìˆ ': 'green'}\n",
        "        for category in category_colors.keys():\n",
        "            indices = [i for i, cat in enumerate(categories) if cat == category]\n",
        "            if indices:\n",
        "                plt.scatter(vectors_2d[indices, 0], vectors_2d[indices, 1],\n",
        "                           c=category_colors[category], label=category, s=100, alpha=0.7)\n",
        "\n",
        "        plt.title('ì‹¤ì œ ì¹´í…Œê³ ë¦¬', fontsize=12)\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # ë¬¸ì„œ ìœ ì‚¬ë„ íˆíŠ¸ë§µ\n",
        "        plt.subplot(1, 3, 3)\n",
        "        import seaborn as sns\n",
        "\n",
        "        sns.heatmap(doc_similarity, annot=True, fmt='.2f', cmap='Blues',\n",
        "                    xticklabels=[f'ë¬¸ì„œ{i+1}' for i in range(len(test_documents))],\n",
        "                    yticklabels=[f'ë¬¸ì„œ{i+1}' for i in range(len(test_documents))])\n",
        "        plt.title('ë¬¸ì„œ ìœ ì‚¬ë„', fontsize=12)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        return app_model\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"âŒ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
        "        print(\"ì„¤ì¹˜: pip install gensim scikit-learn\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# ì „ì²´ ì‹¤í–‰ í•¨ìˆ˜\n",
        "def run_simple_word2vec_tests():\n",
        "    \"\"\"\n",
        "    ê°„ë‹¨í•œ Word2Vec í…ŒìŠ¤íŠ¸ë“¤ ì‹¤í–‰\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(\"ğŸ§  Word2Vec ë§ˆìŠ¤í„°í•˜ê¸°!\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # 1. ê¸°ë³¸ ê°œë… ì´í•´\n",
        "        print(\"1ï¸âƒ£ ê¸°ë³¸ ê°œë… ì‹œì—°...\")\n",
        "        word_vectors = simple_word2vec_demo()\n",
        "\n",
        "        # 2. Gensim ì‹¤ìŠµ\n",
        "        print(\"\\n2ï¸âƒ£ Gensim Word2Vec ì‹¤ìŠµ...\")\n",
        "        gensim_model = gensim_word2vec_example()\n",
        "\n",
        "        # 3. ì‹¤ì „ ì‘ìš©\n",
        "        print(\"\\n3ï¸âƒ£ ì‹¤ì „ ì‘ìš© ì˜ˆì œ...\")\n",
        "        app_model = word2vec_applications_demo()\n",
        "\n",
        "        print(f\"\\n\" + \"=\" * 60)\n",
        "        print(\"ğŸ‰ Word2Vec ê¸°ë³¸ê¸° í•™ìŠµ ì™„ë£Œ!\")\n",
        "        print(\"ğŸ’¡ í•µì‹¬ í¬ì¸íŠ¸:\")\n",
        "        print(\"   âœ… Skip-gram: ì¤‘ì‹¬ë‹¨ì–´ â†’ ì£¼ë³€ë‹¨ì–´ ì˜ˆì¸¡\")\n",
        "        print(\"   âœ… CBOW: ì£¼ë³€ë‹¨ì–´ â†’ ì¤‘ì‹¬ë‹¨ì–´ ì˜ˆì¸¡\")\n",
        "        print(\"   âœ… ë²¡í„° ê³µê°„: ìœ ì‚¬í•œ ë‹¨ì–´ëŠ” ê°€ê¹Œì´ ìœ„ì¹˜\")\n",
        "        print(\"   âœ… ì‘ìš©: ë¶„ë¥˜, í´ëŸ¬ìŠ¤í„°ë§, ì¶”ì²œ ì‹œìŠ¤í…œ\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
        "        print(\"âš ï¸ í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ í•„ìš”:\")\n",
        "        print(\"   pip install gensim scikit-learn matplotlib seaborn\")\n",
        "\n",
        "\n",
        "# ì‹¤í–‰\n",
        "if __name__ == \"__main__\":\n",
        "    run_simple_word2vec_tests()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "H6rx2baoHlXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- t-SNE (t-distributed Stochastic Neighbor Embedding)\n",
        "    - ê³ ì°¨ì› ë°ì´í„°ì˜ ìœ ì‚¬ì„±ì„ ì €ì°¨ì›(2D/3D) ê³µê°„ì— ë³´ì¡´í•˜ì—¬ ì‹œê°í™”í•˜ëŠ” ë¹„ì„ í˜• ì°¨ì› ì¶•ì†Œ ê¸°ë²•"
      ],
      "metadata": {
        "id": "ERxN4c3Poeb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------------\n",
        "# 6) t-SNEë¡œ ë‹¨ì–´ ë²¡í„° 2D ì‹œê°í™”\n",
        "# ---------------------------------------------------------------------\n",
        "\n",
        "# ë¹ˆë„ê°€ 2íšŒ ì´ìƒì¸ ë‹¨ì–´ë§Œ ì‹œê°í™” (ì¡ìŒ ì œê±°)\n",
        "freq = Counter([w for sent in corpus for w in sent])\n",
        "vocab_tsne = [w for w,c in freq.items() if c >= 2 and w in w2v_sg.wv]\n",
        "\n",
        "if len(vocab_tsne) >= 10:\n",
        "    X = np.vstack([w2v_sg.wv[w] for w in vocab_tsne])\n",
        "    tsne = TSNE(n_components=2, perplexity=min(30, max(5, len(vocab_tsne)//3)), random_state=42, n_iter=1000)\n",
        "    X2 = tsne.fit_transform(X)\n",
        "\n",
        "    plt.figure(figsize=(9,7))\n",
        "    # plt.rcParams[\"font.family\"] = 'NanumBarunGothic'\n",
        "    plt.scatter(X2[:,0], X2[:,1])\n",
        "    for i, w in enumerate(vocab_tsne):\n",
        "        plt.annotate(w, (X2[i,0], X2[i,1]), fontsize=9, alpha=0.9)\n",
        "    plt.title('t-SNE visualization of Word2Vec (Skip-gram)')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"ë‹¨ì–´ ìˆ˜ê°€ ì ì–´ t-SNE ì‹œê°í™”ë¥¼ ìƒëµí•©ë‹ˆë‹¤. (ë¹ˆë„â‰¥2 ë‹¨ì–´ê°€ 10ê°œ ë¯¸ë§Œ)\")\n",
        "\n",
        "# ë‹¤ìŒì„ ì‹œë„í•´ë³´ì„¸ìš”\n",
        "# - sg=0(CBOW) ëª¨ë¸ë¡œ ë™ì¼í•œ ë¶„ì„ ë¹„êµ\n",
        "# - window, vector_size, epochs ë³€ê²½í•´ ì„±ëŠ¥ ë¹„êµ\n",
        "# - ë„ë©”ì¸ë³„(ê¸°ìˆ /ë¹„ì¦ˆë‹ˆìŠ¤/ê±´ê°•) ë§ë­‰ì¹˜ë¡œ ë”°ë¡œ í•™ìŠµí•´ êµ°ì§‘ ì°¨ì´ ê´€ì°°"
      ],
      "metadata": {
        "id": "iqn71bBeXOzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter, defaultdict\n",
        "import re\n",
        "import math\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cluster import KMeans\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# í•œê¸€ í°íŠ¸ ì„¤ì • ê°•í™”\n",
        "import matplotlib.font_manager as fm\n",
        "\n",
        "def ensure_korean_font():\n",
        "    \"\"\"í•œê¸€ í°íŠ¸ ì„¤ì • í™•ì¸ ë° ì¬ì„¤ì •\"\"\"\n",
        "    try:\n",
        "        plt.rcParams['font.family'] = 'NanumBarunGothic'\n",
        "    except:\n",
        "        try:\n",
        "            plt.rcParams['font.family'] = 'NanumGothic'\n",
        "        except:\n",
        "            plt.rcParams['font.family'] = 'DejaVu Sans'\n",
        "\n",
        "    plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "    # ì‚¬ìš© ê°€ëŠ¥í•œ í•œê¸€ í°íŠ¸ ì°¾ê¸°\n",
        "    available_fonts = [f.name for f in fm.fontManager.ttflist]\n",
        "    korean_fonts = [f for f in available_fonts if any(keyword in f for keyword in\n",
        "                   ['Nanum', 'Malgun', 'Dotum', 'Gulim', 'Batang', 'Gungsuh'])]\n",
        "\n",
        "    if korean_fonts:\n",
        "        plt.rcParams['font.family'] = korean_fonts[0]\n",
        "\n",
        "# ì´ˆê¸° í°íŠ¸ ì„¤ì •\n",
        "ensure_korean_font()\n",
        "\n",
        "class Word2VecImplementation:\n",
        "    def __init__(self, vector_size=100, window_size=3, min_count=1,\n",
        "                 epochs=100, learning_rate=0.025, model_type='skipgram'):\n",
        "        \"\"\"\n",
        "        Word2Vec êµ¬í˜„ í´ë˜ìŠ¤\n",
        "\n",
        "        Args:\n",
        "            vector_size: ë²¡í„° ì°¨ì› ìˆ˜\n",
        "            window_size: ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° í¬ê¸°\n",
        "            min_count: ìµœì†Œ ë‹¨ì–´ ë¹ˆë„\n",
        "            epochs: í•™ìŠµ ì—í¬í¬ ìˆ˜\n",
        "            learning_rate: í•™ìŠµë¥ \n",
        "            model_type: 'skipgram' ë˜ëŠ” 'cbow'\n",
        "        \"\"\"\n",
        "        self.vector_size = vector_size\n",
        "        self.window_size = window_size\n",
        "        self.min_count = min_count\n",
        "        self.epochs = epochs\n",
        "        self.learning_rate = learning_rate\n",
        "        self.model_type = model_type\n",
        "\n",
        "        self.vocab = {}\n",
        "        self.word_to_idx = {}\n",
        "        self.idx_to_word = {}\n",
        "        self.word_vectors = None\n",
        "        self.context_vectors = None\n",
        "        self.vocab_size = 0\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬\"\"\"\n",
        "        # í•œê¸€ê³¼ ì˜ë¬¸, ìˆ«ìë§Œ ìœ ì§€\n",
        "        text = re.sub(r'[^\\w\\sê°€-í£]', '', text.lower())\n",
        "        return [word for word in text.split() if word.strip()]\n",
        "\n",
        "    def build_vocabulary(self, sentences):\n",
        "        \"\"\"ì–´íœ˜ êµ¬ì¶•\"\"\"\n",
        "        word_counts = Counter()\n",
        "\n",
        "        # ëª¨ë“  ë¬¸ì¥ì—ì„œ ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°\n",
        "        for sentence in sentences:\n",
        "            words = self.preprocess_text(sentence)\n",
        "            word_counts.update(words)\n",
        "\n",
        "        # ìµœì†Œ ë¹ˆë„ í•„í„°ë§\n",
        "        filtered_words = {word: count for word, count in word_counts.items()\n",
        "                         if count >= self.min_count}\n",
        "\n",
        "        # ì–´íœ˜ ë§¤í•‘ ìƒì„±\n",
        "        self.vocab = filtered_words\n",
        "        self.word_to_idx = {word: idx for idx, word in enumerate(filtered_words.keys())}\n",
        "        self.idx_to_word = {idx: word for word, idx in self.word_to_idx.items()}\n",
        "        self.vocab_size = len(self.vocab)\n",
        "\n",
        "        print(f\"êµ¬ì¶•ëœ ì–´íœ˜ í¬ê¸°: {self.vocab_size}\")\n",
        "        return filtered_words\n",
        "\n",
        "    def generate_training_data(self, sentences):\n",
        "        \"\"\"í•™ìŠµ ë°ì´í„° ìƒì„±\"\"\"\n",
        "        training_data = []\n",
        "\n",
        "        for sentence in sentences:\n",
        "            words = self.preprocess_text(sentence)\n",
        "            words = [w for w in words if w in self.word_to_idx]\n",
        "\n",
        "            for i, target_word in enumerate(words):\n",
        "                # ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ë‚´ì˜ ë‹¨ì–´ë“¤\n",
        "                start = max(0, i - self.window_size)\n",
        "                end = min(len(words), i + self.window_size + 1)\n",
        "\n",
        "                context_words = []\n",
        "                for j in range(start, end):\n",
        "                    if j != i:  # íƒ€ê²Ÿ ë‹¨ì–´ ì œì™¸\n",
        "                        context_words.append(words[j])\n",
        "\n",
        "                if len(context_words) >= 1:\n",
        "                    if self.model_type == 'skipgram':\n",
        "                        # Skip-gram: ì¤‘ì‹¬ ë‹¨ì–´ â†’ ì£¼ë³€ ë‹¨ì–´ë“¤\n",
        "                        for context_word in context_words:\n",
        "                            training_data.append((target_word, context_word))\n",
        "                    else:  # CBOW\n",
        "                        # CBOW: ì£¼ë³€ ë‹¨ì–´ë“¤ â†’ ì¤‘ì‹¬ ë‹¨ì–´\n",
        "                        training_data.append((context_words, target_word))\n",
        "\n",
        "        return training_data\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        \"\"\"ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜\"\"\"\n",
        "        x = np.clip(x, -500, 500)  # ì˜¤ë²„í”Œë¡œìš° ë°©ì§€\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def train(self, sentences):\n",
        "        \"\"\"ëª¨ë¸ í•™ìŠµ\"\"\"\n",
        "        print(f\"{self.model_type.upper()} ëª¨ë¸ í•™ìŠµ ì‹œì‘...\")\n",
        "\n",
        "        # ì–´íœ˜ êµ¬ì¶•\n",
        "        self.build_vocabulary(sentences)\n",
        "\n",
        "        if self.vocab_size < 2:\n",
        "            print(\"ì–´íœ˜ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤.\")\n",
        "            return\n",
        "\n",
        "        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”\n",
        "        self.word_vectors = np.random.uniform(-0.5, 0.5,\n",
        "                                            (self.vocab_size, self.vector_size))\n",
        "        self.context_vectors = np.random.uniform(-0.5, 0.5,\n",
        "                                               (self.vocab_size, self.vector_size))\n",
        "\n",
        "        # í•™ìŠµ ë°ì´í„° ìƒì„±\n",
        "        training_data = self.generate_training_data(sentences)\n",
        "        print(f\"í•™ìŠµ ìƒ˜í”Œ ìˆ˜: {len(training_data)}\")\n",
        "\n",
        "        if not training_data:\n",
        "            print(\"í•™ìŠµ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "            return\n",
        "\n",
        "        # í•™ìŠµ ë£¨í”„\n",
        "        for epoch in range(self.epochs):\n",
        "            total_loss = 0\n",
        "\n",
        "            for sample in training_data:\n",
        "                if self.model_type == 'skipgram':\n",
        "                    loss = self._train_skipgram(sample)\n",
        "                else:\n",
        "                    loss = self._train_cbow(sample)\n",
        "                total_loss += loss\n",
        "\n",
        "            if epoch % 20 == 0:\n",
        "                avg_loss = total_loss / len(training_data)\n",
        "                print(f\"Epoch {epoch}, Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        print(\"í•™ìŠµ ì™„ë£Œ!\")\n",
        "\n",
        "    def _train_skipgram(self, sample):\n",
        "        \"\"\"Skip-gram í•™ìŠµ\"\"\"\n",
        "        target_word, context_word = sample\n",
        "\n",
        "        if target_word not in self.word_to_idx or context_word not in self.word_to_idx:\n",
        "            return 0\n",
        "\n",
        "        target_idx = self.word_to_idx[target_word]\n",
        "        context_idx = self.word_to_idx[context_word]\n",
        "\n",
        "        # Forward pass\n",
        "        target_vector = self.word_vectors[target_idx]\n",
        "        context_vector = self.context_vectors[context_idx]\n",
        "\n",
        "        # ë‚´ì  ê³„ì‚°\n",
        "        score = np.dot(target_vector, context_vector)\n",
        "        pred = self.sigmoid(score)\n",
        "\n",
        "        # Loss ê³„ì‚° (ì´ì§„ ë¶„ë¥˜)\n",
        "        loss = -np.log(pred + 1e-10)\n",
        "\n",
        "        # Backward pass\n",
        "        error = pred - 1  # ì •ë‹µì´ 1ì´ë¯€ë¡œ\n",
        "\n",
        "        # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë° ì—…ë°ì´íŠ¸\n",
        "        word_grad = error * context_vector\n",
        "        context_grad = error * target_vector\n",
        "\n",
        "        self.word_vectors[target_idx] -= self.learning_rate * word_grad\n",
        "        self.context_vectors[context_idx] -= self.learning_rate * context_grad\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def _train_cbow(self, sample):\n",
        "        \"\"\"CBOW í•™ìŠµ\"\"\"\n",
        "        context_words, target_word = sample\n",
        "\n",
        "        if target_word not in self.word_to_idx:\n",
        "            return 0\n",
        "\n",
        "        # ì»¨í…ìŠ¤íŠ¸ ë²¡í„°ë“¤ì˜ í‰ê· \n",
        "        context_indices = [self.word_to_idx[w] for w in context_words\n",
        "                          if w in self.word_to_idx]\n",
        "\n",
        "        if not context_indices:\n",
        "            return 0\n",
        "\n",
        "        context_vector = np.mean([self.context_vectors[idx] for idx in context_indices], axis=0)\n",
        "        target_idx = self.word_to_idx[target_word]\n",
        "\n",
        "        # Forward pass\n",
        "        scores = np.dot(self.word_vectors, context_vector)\n",
        "        scores = scores - np.max(scores)  # ìˆ˜ì¹˜ ì•ˆì •ì„±\n",
        "\n",
        "        # Softmax\n",
        "        exp_scores = np.exp(scores)\n",
        "        probs = exp_scores / np.sum(exp_scores)\n",
        "\n",
        "        # Loss ê³„ì‚°\n",
        "        loss = -np.log(probs[target_idx] + 1e-10)\n",
        "\n",
        "        # Backward pass\n",
        "        grad_output = probs.copy()\n",
        "        grad_output[target_idx] -= 1.0\n",
        "\n",
        "        # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸\n",
        "        word_grad = np.outer(grad_output, context_vector)\n",
        "        self.word_vectors -= self.learning_rate * word_grad\n",
        "\n",
        "        context_grad = np.dot(grad_output, self.word_vectors)\n",
        "        for idx in context_indices:\n",
        "            self.context_vectors[idx] -= self.learning_rate * context_grad / len(context_indices)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def get_vector(self, word):\n",
        "        \"\"\"ë‹¨ì–´ ë²¡í„° ë°˜í™˜\"\"\"\n",
        "        if word in self.word_to_idx:\n",
        "            idx = self.word_to_idx[word]\n",
        "            return self.word_vectors[idx]\n",
        "        return None\n",
        "\n",
        "    def similarity(self, word1, word2):\n",
        "        \"\"\"ë‘ ë‹¨ì–´ ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„\"\"\"\n",
        "        vec1 = self.get_vector(word1)\n",
        "        vec2 = self.get_vector(word2)\n",
        "\n",
        "        if vec1 is None or vec2 is None:\n",
        "            return 0.0\n",
        "\n",
        "        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„\n",
        "        norm1 = np.linalg.norm(vec1)\n",
        "        norm2 = np.linalg.norm(vec2)\n",
        "\n",
        "        if norm1 == 0 or norm2 == 0:\n",
        "            return 0.0\n",
        "\n",
        "        return np.dot(vec1, vec2) / (norm1 * norm2)\n",
        "\n",
        "    def most_similar(self, word, top_k=5):\n",
        "        \"\"\"ê°€ì¥ ìœ ì‚¬í•œ ë‹¨ì–´ë“¤ ì°¾ê¸°\"\"\"\n",
        "        if word not in self.word_to_idx:\n",
        "            return []\n",
        "\n",
        "        target_vector = self.get_vector(word)\n",
        "        similarities = []\n",
        "\n",
        "        for other_word in self.word_to_idx.keys():\n",
        "            if other_word != word:\n",
        "                sim = self.similarity(word, other_word)\n",
        "                similarities.append((other_word, sim))\n",
        "\n",
        "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "        return similarities[:top_k]\n",
        "\n",
        "    def analogy(self, word_a, word_b, word_c, top_k=1):\n",
        "        \"\"\"ë‹¨ì–´ ìœ ì¶”: A is to B as C is to ?\"\"\"\n",
        "        try:\n",
        "            vec_a = self.get_vector(word_a)\n",
        "            vec_b = self.get_vector(word_b)\n",
        "            vec_c = self.get_vector(word_c)\n",
        "\n",
        "            if any(v is None for v in [vec_a, vec_b, vec_c]):\n",
        "                return []\n",
        "\n",
        "            # ë²¡í„° ì—°ì‚°: B - A + C\n",
        "            result_vector = vec_b - vec_a + vec_c\n",
        "\n",
        "            # ê°€ì¥ ìœ ì‚¬í•œ ë‹¨ì–´ ì°¾ê¸°\n",
        "            similarities = []\n",
        "            for word in self.word_to_idx.keys():\n",
        "                if word not in [word_a, word_b, word_c]:\n",
        "                    word_vec = self.get_vector(word)\n",
        "                    if word_vec is not None:\n",
        "                        sim = cosine_similarity([result_vector], [word_vec])[0][0]\n",
        "                        similarities.append((word, sim))\n",
        "\n",
        "            similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "            return similarities[:top_k]\n",
        "\n",
        "        except:\n",
        "            return []\n",
        "\n",
        "\n",
        "def demonstrate_basic_word2vec():\n",
        "    \"\"\"ê¸°ë³¸ Word2Vec êµ¬í˜„ ì‹œì—°\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"1. ê¸°ë³¸ Word2Vec êµ¬í˜„ ì‹œì—°\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # ì˜ˆì œ ë¬¸ì„œë“¤ (ë” ë§ì€ ë°ì´í„°ë¡œ í™•ì¥)\n",
        "    sentences = [\n",
        "        \"ì™•ì€ ì™•ê¶ì—ì„œ ìƒí™œí•©ë‹ˆë‹¤\",\n",
        "        \"ì—¬ì™•ì€ ì™•ê¶ì—ì„œ ìƒí™œí•©ë‹ˆë‹¤\",\n",
        "        \"ë‚¨ìëŠ” ë‚¨ì„±ì´ë¼ê³ ë„ ë¶€ë¦…ë‹ˆë‹¤\",\n",
        "        \"ì—¬ìëŠ” ì—¬ì„±ì´ë¼ê³ ë„ ë¶€ë¦…ë‹ˆë‹¤\",\n",
        "        \"ì™•ê³¼ ì—¬ì™•ì€ ì™•ì‹¤ ê°€ì¡±ì…ë‹ˆë‹¤\",\n",
        "        \"ë‚¨ìì™€ ì—¬ìëŠ” ì‚¬ëŒì…ë‹ˆë‹¤\",\n",
        "        \"ì•„ë²„ì§€ëŠ” ê°€ì¡±ì˜ ê°€ì¥ì…ë‹ˆë‹¤\",\n",
        "        \"ì–´ë¨¸ë‹ˆëŠ” ê°€ì¡±ì„ ëŒë´…ë‹ˆë‹¤\",\n",
        "        \"ì•„ë“¤ì€ ì•„ë²„ì§€ì˜ ìë…€ì…ë‹ˆë‹¤\",\n",
        "        \"ë”¸ì€ ì–´ë¨¸ë‹ˆì˜ ìë…€ì…ë‹ˆë‹¤\",\n",
        "        \"ì»´í“¨í„°ëŠ” ì „ì ê¸°ê¸°ì…ë‹ˆë‹¤\",\n",
        "        \"ë…¸íŠ¸ë¶ì€ íœ´ëŒ€ìš© ì»´í“¨í„°ì…ë‹ˆë‹¤\",\n",
        "        \"ìŠ¤ë§ˆíŠ¸í°ì€ íœ´ëŒ€ìš© ì „í™”ê¸°ì…ë‹ˆë‹¤\",\n",
        "        \"íƒœë¸”ë¦¿ì€ íœ´ëŒ€ìš© ê¸°ê¸°ì…ë‹ˆë‹¤\",\n",
        "        \"ì¸ê³µì§€ëŠ¥ì€ ì»´í“¨í„° ê¸°ìˆ ì…ë‹ˆë‹¤\",\n",
        "        \"ë¨¸ì‹ ëŸ¬ë‹ì€ ì¸ê³µì§€ëŠ¥ ë¶„ì•¼ì…ë‹ˆë‹¤\",\n",
        "        \"ë”¥ëŸ¬ë‹ì€ ë¨¸ì‹ ëŸ¬ë‹ ë°©ë²•ì…ë‹ˆë‹¤\",\n",
        "        \"ë°ì´í„°ëŠ” ì •ë³´ì˜ ì§‘í•©ì…ë‹ˆë‹¤\"\n",
        "    ]\n",
        "\n",
        "    # Skip-gram ëª¨ë¸ í•™ìŠµ\n",
        "    print(\"\\nğŸ§  Skip-gram ëª¨ë¸ í•™ìŠµ:\")\n",
        "    skipgram_model = Word2VecImplementation(\n",
        "        vector_size=50,\n",
        "        window_size=2,\n",
        "        epochs=200,\n",
        "        model_type='skipgram',\n",
        "        learning_rate=0.1\n",
        "    )\n",
        "    skipgram_model.train(sentences)\n",
        "\n",
        "    # CBOW ëª¨ë¸ í•™ìŠµ\n",
        "    print(f\"\\nğŸ§  CBOW ëª¨ë¸ í•™ìŠµ:\")\n",
        "    cbow_model = Word2VecImplementation(\n",
        "        vector_size=50,\n",
        "        window_size=2,\n",
        "        epochs=200,\n",
        "        model_type='cbow',\n",
        "        learning_rate=0.1\n",
        "    )\n",
        "    cbow_model.train(sentences)\n",
        "\n",
        "    # ê²°ê³¼ ë¹„êµ\n",
        "    test_words = [\"ì™•\", \"ì—¬ì™•\", \"ë‚¨ì\", \"ì—¬ì\", \"ì»´í“¨í„°\"]\n",
        "\n",
        "    print(f\"\\nğŸ“Š ëª¨ë¸ ê²°ê³¼ ë¹„êµ:\")\n",
        "    for word in test_words:\n",
        "        if word in skipgram_model.word_to_idx:\n",
        "            print(f\"\\nğŸ” '{word}'ì™€ ìœ ì‚¬í•œ ë‹¨ì–´ë“¤:\")\n",
        "\n",
        "            # Skip-gram ê²°ê³¼\n",
        "            skipgram_similar = skipgram_model.most_similar(word, top_k=3)\n",
        "            print(f\"  Skip-gram: {skipgram_similar}\")\n",
        "\n",
        "            # CBOW ê²°ê³¼\n",
        "            cbow_similar = cbow_model.most_similar(word, top_k=3)\n",
        "            print(f\"  CBOW: {cbow_similar}\")\n",
        "\n",
        "    return skipgram_model, cbow_model\n",
        "\n",
        "\n",
        "def demonstrate_gensim_word2vec():\n",
        "    \"\"\"Gensim Word2Vecê³¼ ë¹„êµ\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"2. Gensim Word2Vecê³¼ ì„±ëŠ¥ ë¹„êµ\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    try:\n",
        "        from gensim.models import Word2Vec\n",
        "\n",
        "        # í•œêµ­ì–´ ë¬¸ì¥ë“¤\n",
        "        korean_sentences = [\n",
        "            ['ì¸ê³µì§€ëŠ¥', 'ê¸°ìˆ ', 'ë°œì „', 'ë¹ ë¥´ë‹¤'],\n",
        "            ['ë¨¸ì‹ ëŸ¬ë‹', 'ì•Œê³ ë¦¬ì¦˜', 'í•™ìŠµ', 'ë°ì´í„°'],\n",
        "            ['ë”¥ëŸ¬ë‹', 'ì‹ ê²½ë§', 'ë³µì¡í•œ', 'ë¬¸ì œ', 'í•´ê²°'],\n",
        "            ['ìì—°ì–¸ì–´', 'ì²˜ë¦¬', 'NLP', 'ì¤‘ìš”í•˜ë‹¤'],\n",
        "            ['ì»´í“¨í„°', 'ë¹„ì „', 'ì´ë¯¸ì§€', 'ì¸ì‹'],\n",
        "            ['ë¹…ë°ì´í„°', 'ë¶„ì„', 'íŒ¨í„´', 'ë°œê²¬'],\n",
        "            ['í´ë¼ìš°ë“œ', 'ì»´í“¨íŒ…', 'ì„œë¹„ìŠ¤', 'ì œê³µ'],\n",
        "            ['ë¡œë´‡', 'ìë™í™”', 'ë¯¸ë˜', 'ì‚°ì—…'],\n",
        "            ['ë¸”ë¡ì²´ì¸', 'ì•”í˜¸í™”', 'ë³´ì•ˆ', 'ê¸°ìˆ '],\n",
        "            ['ì‚¬ë¬¼ì¸í„°ë„·', 'IoT', 'ì—°ê²°', 'ë„¤íŠ¸ì›Œí¬']\n",
        "        ]\n",
        "\n",
        "        # Gensim Word2Vec ëª¨ë¸\n",
        "        gensim_model = Word2Vec(\n",
        "            sentences=korean_sentences,\n",
        "            vector_size=100,\n",
        "            window=3,\n",
        "            min_count=1,\n",
        "            workers=1,\n",
        "            sg=1,  # Skip-gram\n",
        "            epochs=100\n",
        "        )\n",
        "\n",
        "        print(\"âœ… Gensim Word2Vec ëª¨ë¸ í•™ìŠµ ì™„ë£Œ\")\n",
        "        print(f\"ì–´íœ˜ í¬ê¸°: {len(gensim_model.wv.key_to_index)}\")\n",
        "\n",
        "        # ë‹¨ì–´ ìœ ì‚¬ë„ í…ŒìŠ¤íŠ¸\n",
        "        test_words = ['ì¸ê³µì§€ëŠ¥', 'ë¨¸ì‹ ëŸ¬ë‹', 'ì»´í“¨í„°', 'ë°ì´í„°']\n",
        "\n",
        "        print(f\"\\nğŸ“Š Gensim ëª¨ë¸ ê²°ê³¼:\")\n",
        "        for word in test_words:\n",
        "            if word in gensim_model.wv:\n",
        "                try:\n",
        "                    similar_words = gensim_model.wv.most_similar(word, topn=3)\n",
        "                    print(f\"  '{word}' ìœ ì‚¬ ë‹¨ì–´: {similar_words}\")\n",
        "                except:\n",
        "                    print(f\"  '{word}': ìœ ì‚¬ ë‹¨ì–´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "        # ë‹¨ì–´ ê°„ ìœ ì‚¬ë„\n",
        "        print(f\"\\nğŸ”— ë‹¨ì–´ ê°„ ìœ ì‚¬ë„:\")\n",
        "        word_pairs = [\n",
        "            ('ì¸ê³µì§€ëŠ¥', 'ë¨¸ì‹ ëŸ¬ë‹'),\n",
        "            ('ì»´í“¨í„°', 'ë°ì´í„°'),\n",
        "            ('ì‹ ê²½ë§', 'ë”¥ëŸ¬ë‹')\n",
        "        ]\n",
        "\n",
        "        for word1, word2 in word_pairs:\n",
        "            if word1 in gensim_model.wv and word2 in gensim_model.wv:\n",
        "                similarity = gensim_model.wv.similarity(word1, word2)\n",
        "                print(f\"  {word1} - {word2}: {similarity:.4f}\")\n",
        "\n",
        "        return gensim_model\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"âŒ Gensimì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
        "        print(\"ì„¤ì¹˜: pip install gensim\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def demonstrate_word2vec_visualization():\n",
        "    \"\"\"Word2Vec ë²¡í„° ì‹œê°í™”\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"3. Word2Vec ë²¡í„° ì‹œê°í™”\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # ë” ë§ì€ ë°ì´í„°ë¡œ ëª¨ë¸ í•™ìŠµ\n",
        "    extended_sentences = [\n",
        "        \"ì‚¬ê³¼ëŠ” ë¹¨ê°„ ê³¼ì¼ì…ë‹ˆë‹¤\", \"ë°”ë‚˜ë‚˜ëŠ” ë…¸ë€ ê³¼ì¼ì…ë‹ˆë‹¤\", \"ì˜¤ë Œì§€ëŠ” ì˜¤ë Œì§€ìƒ‰ ê³¼ì¼ì…ë‹ˆë‹¤\",\n",
        "        \"í¬ë„ëŠ” ë³´ë¼ìƒ‰ ê³¼ì¼ì…ë‹ˆë‹¤\", \"ê³¼ì¼ì€ ë‹¬ê³  ë§›ìˆìŠµë‹ˆë‹¤\", \"ê³¼ì¼ì€ ê±´ê°•ì— ì¢‹ìŠµë‹ˆë‹¤\",\n",
        "        \"ê°œëŠ” ì¶©ì‹¤í•œ ë™ë¬¼ì…ë‹ˆë‹¤\", \"ê³ ì–‘ì´ëŠ” ê·€ì—¬ìš´ ë™ë¬¼ì…ë‹ˆë‹¤\", \"í† ë¼ëŠ” ë¹ ë¥¸ ë™ë¬¼ì…ë‹ˆë‹¤\",\n",
        "        \"ì‚¬ìëŠ” ê°•í•œ ë™ë¬¼ì…ë‹ˆë‹¤\", \"ë™ë¬¼ì€ ìƒëª…ì²´ì…ë‹ˆë‹¤\", \"ë™ë¬¼ì€ ìì—°ì— ì‚½ë‹ˆë‹¤\",\n",
        "        \"ìë™ì°¨ëŠ” ë¹ ë¥¸ êµí†µìˆ˜ë‹¨ì…ë‹ˆë‹¤\", \"ê¸°ì°¨ëŠ” ê¸´ êµí†µìˆ˜ë‹¨ì…ë‹ˆë‹¤\", \"ë¹„í–‰ê¸°ëŠ” ë†’ì€ êµí†µìˆ˜ë‹¨ì…ë‹ˆë‹¤\",\n",
        "        \"ë°°ëŠ” ë¬¼ìœ„ì˜ êµí†µìˆ˜ë‹¨ì…ë‹ˆë‹¤\", \"êµí†µìˆ˜ë‹¨ì€ ì´ë™ì— ì‚¬ìš©ë©ë‹ˆë‹¤\", \"êµí†µìˆ˜ë‹¨ì€ í¸ë¦¬í•©ë‹ˆë‹¤\",\n",
        "        \"ì±…ì€ ì§€ì‹ì˜ ë³´ê³ ì…ë‹ˆë‹¤\", \"ì‹ ë¬¸ì€ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤\", \"ì¡ì§€ëŠ” ì¬ë¯¸ìˆëŠ” ì½ì„ê±°ë¦¬ì…ë‹ˆë‹¤\",\n",
        "        \"ì»´í“¨í„°ëŠ” ìœ ìš©í•œ ë„êµ¬ì…ë‹ˆë‹¤\", \"ìŠ¤ë§ˆíŠ¸í°ì€ í¸ë¦¬í•œ ë„êµ¬ì…ë‹ˆë‹¤\", \"ë„êµ¬ëŠ” ì¼ì„ ë•ìŠµë‹ˆë‹¤\"\n",
        "    ]\n",
        "\n",
        "    # ëª¨ë¸ í•™ìŠµ\n",
        "    viz_model = Word2VecImplementation(\n",
        "        vector_size=50,\n",
        "        window_size=3,\n",
        "        epochs=300,\n",
        "        model_type='skipgram',\n",
        "        learning_rate=0.1\n",
        "    )\n",
        "    viz_model.train(extended_sentences)\n",
        "\n",
        "    # ë²¡í„° ì¶”ì¶œ\n",
        "    words = list(viz_model.word_to_idx.keys())\n",
        "    vectors = []\n",
        "    word_labels = []\n",
        "\n",
        "    for word in words:\n",
        "        vector = viz_model.get_vector(word)\n",
        "        if vector is not None:\n",
        "            vectors.append(vector)\n",
        "            word_labels.append(word)\n",
        "\n",
        "    if len(vectors) < 2:\n",
        "        print(\"âŒ ì‹œê°í™”í•  ë²¡í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
        "        return\n",
        "\n",
        "    vectors = np.array(vectors)\n",
        "\n",
        "    # PCAë¡œ 2ì°¨ì› ì¶•ì†Œ\n",
        "    pca = PCA(n_components=2)\n",
        "    vectors_2d = pca.fit_transform(vectors)\n",
        "\n",
        "    # ì‹œê°í™”\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # í•œê¸€ í°íŠ¸ ì¬ì„¤ì •\n",
        "    ensure_korean_font()\n",
        "\n",
        "    # 1. PCA ì‹œê°í™”\n",
        "    plt.subplot(2, 2, 1)\n",
        "    scatter = plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1],\n",
        "                         c=np.arange(len(word_labels)),\n",
        "                         cmap='tab20', s=100, alpha=0.7)\n",
        "\n",
        "    # ë‹¨ì–´ ë¼ë²¨ ì¶”ê°€\n",
        "    for i, word in enumerate(word_labels):\n",
        "        plt.annotate(word, (vectors_2d[i, 0], vectors_2d[i, 1]),\n",
        "                    xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
        "\n",
        "    plt.title('Word2Vec ë²¡í„° ì‹œê°í™” (PCA)', fontsize=14)\n",
        "    plt.xlabel('PC1')\n",
        "    plt.ylabel('PC2')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # 2. ì¹´í…Œê³ ë¦¬ë³„ ìƒ‰ìƒ êµ¬ë¶„\n",
        "    plt.subplot(2, 2, 2)\n",
        "\n",
        "    # ë‹¨ì–´ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ (ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜)\n",
        "    categories = {\n",
        "        'ê³¼ì¼': ['ì‚¬ê³¼', 'ë°”ë‚˜ë‚˜', 'ì˜¤ë Œì§€', 'í¬ë„', 'ê³¼ì¼'],\n",
        "        'ë™ë¬¼': ['ê°œ', 'ê³ ì–‘ì´', 'í† ë¼', 'ì‚¬ì', 'ë™ë¬¼'],\n",
        "        'êµí†µ': ['ìë™ì°¨', 'ê¸°ì°¨', 'ë¹„í–‰ê¸°', 'ë°°', 'êµí†µìˆ˜ë‹¨'],\n",
        "        'ë„êµ¬': ['ì»´í“¨í„°', 'ìŠ¤ë§ˆíŠ¸í°', 'ë„êµ¬', 'ì±…', 'ì‹ ë¬¸', 'ì¡ì§€']\n",
        "    }\n",
        "\n",
        "    colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
        "    category_colors = {}\n",
        "\n",
        "    for i, (cat, words_in_cat) in enumerate(categories.items()):\n",
        "        for word in words_in_cat:\n",
        "            if word in word_labels:\n",
        "                category_colors[word] = colors[i % len(colors)]\n",
        "\n",
        "    # ì¹´í…Œê³ ë¦¬ë³„ ìƒ‰ìƒìœ¼ë¡œ í”Œë¡¯\n",
        "    for i, word in enumerate(word_labels):\n",
        "        color = category_colors.get(word, 'gray')\n",
        "        plt.scatter(vectors_2d[i, 0], vectors_2d[i, 1],\n",
        "                   c=color, s=100, alpha=0.7)\n",
        "        plt.annotate(word, (vectors_2d[i, 0], vectors_2d[i, 1]),\n",
        "                    xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
        "\n",
        "    plt.title('ì¹´í…Œê³ ë¦¬ë³„ ë‹¨ì–´ ë¶„í¬', fontsize=14)\n",
        "    plt.xlabel('PC1')\n",
        "    plt.ylabel('PC2')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # ë²”ë¡€ ì¶”ê°€\n",
        "    for cat, color in zip(categories.keys(), colors):\n",
        "        plt.scatter([], [], c=color, s=100, label=cat, alpha=0.7)\n",
        "    plt.legend()\n",
        "\n",
        "    # 3. ìœ ì‚¬ë„ íˆíŠ¸ë§µ\n",
        "    plt.subplot(2, 2, 3)\n",
        "\n",
        "    # ì£¼ìš” ë‹¨ì–´ë“¤ë§Œ ì„ íƒ\n",
        "    main_words = [w for w in word_labels if len(w) > 1][:10]\n",
        "    similarity_matrix = np.zeros((len(main_words), len(main_words)))\n",
        "\n",
        "    for i, word1 in enumerate(main_words):\n",
        "        for j, word2 in enumerate(main_words):\n",
        "            similarity_matrix[i, j] = viz_model.similarity(word1, word2)\n",
        "\n",
        "    sns.heatmap(similarity_matrix,\n",
        "                xticklabels=main_words,\n",
        "                yticklabels=main_words,\n",
        "                annot=True, fmt='.2f', cmap='Blues')\n",
        "    plt.title('ë‹¨ì–´ ê°„ ìœ ì‚¬ë„ íˆíŠ¸ë§µ', fontsize=14)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.yticks(rotation=0)\n",
        "\n",
        "    # 4. ë²¡í„° í¬ê¸° ë¶„í¬\n",
        "    plt.subplot(2, 2, 4)\n",
        "    vector_norms = [np.linalg.norm(viz_model.get_vector(word)) for word in word_labels]\n",
        "\n",
        "    plt.hist(vector_norms, bins=15, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "    plt.title('ë²¡í„° í¬ê¸° ë¶„í¬', fontsize=14)\n",
        "    plt.xlabel('ë²¡í„° í¬ê¸°')\n",
        "    plt.ylabel('ë¹ˆë„')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„\n",
        "    kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
        "    clusters = kmeans.fit_predict(vectors)\n",
        "\n",
        "    print(f\"\\nğŸ¯ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼:\")\n",
        "    cluster_words = defaultdict(list)\n",
        "    for word, cluster in zip(word_labels, clusters):\n",
        "        cluster_words[cluster].append(word)\n",
        "\n",
        "    for cluster_id, words in cluster_words.items():\n",
        "        print(f\"  í´ëŸ¬ìŠ¤í„° {cluster_id}: {words}\")\n",
        "\n",
        "    return viz_model\n",
        "\n",
        "\n",
        "def demonstrate_word_analogies():\n",
        "    \"\"\"ë‹¨ì–´ ìœ ì¶” ì˜ˆì œ\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"4. Word2Vec ë‹¨ì–´ ìœ ì¶” (Word Analogies)\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # ê´€ê³„ê°€ ëª…í™•í•œ ë¬¸ì¥ë“¤ë¡œ í•™ìŠµ\n",
        "    analogy_sentences = [\n",
        "        \"ë‚¨ìëŠ” ì•„ë²„ì§€ê°€ ë©ë‹ˆë‹¤\", \"ì—¬ìëŠ” ì–´ë¨¸ë‹ˆê°€ ë©ë‹ˆë‹¤\",\n",
        "        \"ë‚¨ìëŠ” ì™•ì´ ë©ë‹ˆë‹¤\", \"ì—¬ìëŠ” ì—¬ì™•ì´ ë©ë‹ˆë‹¤\",\n",
        "        \"ë‚¨ìëŠ” ì•„ë“¤ì…ë‹ˆë‹¤\", \"ì—¬ìëŠ” ë”¸ì…ë‹ˆë‹¤\",\n",
        "        \"ë‚¨ìëŠ” í˜•ì´ ë©ë‹ˆë‹¤\", \"ì—¬ìëŠ” ëˆ„ë‚˜ê°€ ë©ë‹ˆë‹¤\",\n",
        "        \"ë‚¨ìëŠ” ë‚¨í¸ì…ë‹ˆë‹¤\", \"ì—¬ìëŠ” ì•„ë‚´ì…ë‹ˆë‹¤\",\n",
        "        \"ì–´ë¦° ë‚¨ìëŠ” ì†Œë…„ì…ë‹ˆë‹¤\", \"ì–´ë¦° ì—¬ìëŠ” ì†Œë…€ì…ë‹ˆë‹¤\",\n",
        "        \"ì„œìš¸ì€ í•œêµ­ì˜ ìˆ˜ë„ì…ë‹ˆë‹¤\", \"ë„ì¿„ëŠ” ì¼ë³¸ì˜ ìˆ˜ë„ì…ë‹ˆë‹¤\",\n",
        "        \"ì›Œì‹±í„´ì€ ë¯¸êµ­ì˜ ìˆ˜ë„ì…ë‹ˆë‹¤\", \"ëŸ°ë˜ì€ ì˜êµ­ì˜ ìˆ˜ë„ì…ë‹ˆë‹¤\",\n",
        "        \"í•œêµ­ ì‚¬ëŒì€ í•œêµ­ì–´ë¥¼ ì”ë‹ˆë‹¤\", \"ì¼ë³¸ ì‚¬ëŒì€ ì¼ë³¸ì–´ë¥¼ ì”ë‹ˆë‹¤\",\n",
        "        \"ë¯¸êµ­ ì‚¬ëŒì€ ì˜ì–´ë¥¼ ì”ë‹ˆë‹¤\", \"ì¤‘êµ­ ì‚¬ëŒì€ ì¤‘êµ­ì–´ë¥¼ ì”ë‹ˆë‹¤\",\n",
        "        \"ê°œëŠ” ë©ë©í•˜ê³  ì›ë‹ˆë‹¤\", \"ê³ ì–‘ì´ëŠ” ì•¼ì˜¹í•˜ê³  ì›ë‹ˆë‹¤\",\n",
        "        \"ì†ŒëŠ” ìŒë©”í•˜ê³  ì›ë‹ˆë‹¤\", \"ë¼ì§€ëŠ” ê¿€ê¿€í•˜ê³  ì›ë‹ˆë‹¤\",\n",
        "        \"í¬ë‹¤ì˜ ë°˜ëŒ€ëŠ” ì‘ë‹¤ì…ë‹ˆë‹¤\", \"ë†’ë‹¤ì˜ ë°˜ëŒ€ëŠ” ë‚®ë‹¤ì…ë‹ˆë‹¤\",\n",
        "        \"ëœ¨ê²ë‹¤ì˜ ë°˜ëŒ€ëŠ” ì°¨ê°‘ë‹¤ì…ë‹ˆë‹¤\", \"ë°ë‹¤ì˜ ë°˜ëŒ€ëŠ” ì–´ë‘¡ë‹¤ì…ë‹ˆë‹¤\"\n",
        "    ]\n",
        "\n",
        "    # ëª¨ë¸ í•™ìŠµ\n",
        "    analogy_model = Word2VecImplementation(\n",
        "        vector_size=100,\n",
        "        window_size=3,\n",
        "        epochs=500,\n",
        "        model_type='skipgram',\n",
        "        learning_rate=0.1\n",
        "    )\n",
        "    analogy_model.train(analogy_sentences)\n",
        "\n",
        "    # ë‹¨ì–´ ìœ ì¶” í…ŒìŠ¤íŠ¸\n",
        "    print(f\"\\nğŸ§© ë‹¨ì–´ ìœ ì¶” í…ŒìŠ¤íŠ¸:\")\n",
        "\n",
        "    analogy_tests = [\n",
        "        (\"ë‚¨ì\", \"ì•„ë²„ì§€\", \"ì—¬ì\"),  # ë‚¨ì:ì•„ë²„ì§€ = ì—¬ì:?\n",
        "        (\"ë‚¨ì\", \"ì™•\", \"ì—¬ì\"),      # ë‚¨ì:ì™• = ì—¬ì:?\n",
        "        (\"ì„œìš¸\", \"í•œêµ­\", \"ë„ì¿„\"),    # ì„œìš¸:í•œêµ­ = ë„ì¿„:?\n",
        "        (\"í¬ë‹¤\", \"ì‘ë‹¤\", \"ë†’ë‹¤\"),    # í¬ë‹¤:ì‘ë‹¤ = ë†’ë‹¤:?\n",
        "        (\"ê°œ\", \"ë©ë©\", \"ê³ ì–‘ì´\")     # ê°œ:ë©ë© = ê³ ì–‘ì´:?\n",
        "    ]\n",
        "\n",
        "    for word_a, word_b, word_c in analogy_tests:\n",
        "        print(f\"\\n  {word_a} : {word_b} = {word_c} : ?\")\n",
        "\n",
        "        results = analogy_model.analogy(word_a, word_b, word_c, top_k=3)\n",
        "        if results:\n",
        "            print(f\"    ì˜ˆì¸¡ ê²°ê³¼:\")\n",
        "            for word, score in results:\n",
        "                print(f\"      {word} (ìœ ì‚¬ë„: {score:.4f})\")\n",
        "        else:\n",
        "            print(f\"    ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "\n",
        "def demonstrate_practical_applications():\n",
        "    \"\"\"Word2Vec ì‹¤ì „ ì‘ìš©\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"5. Word2Vec ì‹¤ì „ ì‘ìš© - ë¬¸ì„œ ë¶„ë¥˜ ë° ì¶”ì²œ\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ë³„ ë¬¸ì„œë“¤\n",
        "    news_data = {\n",
        "        'ê¸°ìˆ ': [\n",
        "            \"ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì´ ë¹ ë¥´ê²Œ ë°œì „í•˜ë©´ì„œ ë‹¤ì–‘í•œ ì‚°ì—… ë¶„ì•¼ì— í˜ì‹ ì„ ê°€ì ¸ì˜¤ê³  ìˆìŠµë‹ˆë‹¤\",\n",
        "            \"ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì„ í™œìš©í•œ ìë™í™” ì‹œìŠ¤í…œì´ ë„ì…ë˜ê³  ìˆìŠµë‹ˆë‹¤\",\n",
        "            \"ë¹…ë°ì´í„° ë¶„ì„ ê¸°ìˆ ë¡œ ê³ ê° í–‰ë™ íŒ¨í„´ì„ ì˜ˆì¸¡í•˜ëŠ” ì„œë¹„ìŠ¤ê°€ ë“±ì¥í–ˆìŠµë‹ˆë‹¤\",\n",
        "            \"í´ë¼ìš°ë“œ ì»´í“¨íŒ… í”Œë«í¼ì´ ê¸°ì—…ì˜ ë””ì§€í„¸ ì „í™˜ì„ ê°€ì†í™”í•˜ê³  ìˆìŠµë‹ˆë‹¤\",\n",
        "            \"ì‚¬ë¬¼ì¸í„°ë„·ê³¼ ìŠ¤ë§ˆíŠ¸ì‹œí‹° ê¸°ìˆ ì´ ë„ì‹œ ì¸í”„ë¼ë¥¼ í˜ì‹ í•˜ê³  ìˆìŠµë‹ˆë‹¤\"\n",
        "        ],\n",
        "        'ìŠ¤í¬ì¸ ': [\n",
        "            \"ì›”ë“œì»µ ì¶•êµ¬ ëŒ€íšŒì—ì„œ í•œêµ­ íŒ€ì´ ë›°ì–´ë‚œ ê²½ê¸°ë ¥ì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤\",\n",
        "            \"ì˜¬ë¦¼í”½ ìˆ˜ì˜ ì„ ìˆ˜ê°€ ìƒˆë¡œìš´ ì„¸ê³„ ê¸°ë¡ì„ ê²½ì‹ í–ˆìŠµë‹ˆë‹¤\",\n",
        "            \"í”„ë¡œì•¼êµ¬ ì‹œì¦Œì´ ì‹œì‘ë˜ë©´ì„œ íŒ¬ë“¤ì˜ ê´€ì‹¬ì´ ë†’ì•„ì§€ê³  ìˆìŠµë‹ˆë‹¤\",\n",
        "            \"í…Œë‹ˆìŠ¤ ê·¸ëœë“œìŠ¬ë¨ ëŒ€íšŒì—ì„œ í•œêµ­ ì„ ìˆ˜ê°€ ìš°ìŠ¹ì„ ì°¨ì§€í–ˆìŠµë‹ˆë‹¤\",\n",
        "            \"ë§ˆë¼í†¤ ëŒ€íšŒì— ìˆ˜ë§ì€ ì‹œë¯¼ë“¤ì´ ì°¸ì—¬í•˜ì—¬ ê±´ê°•í•œ ì¶•ì œê°€ ë˜ì—ˆìŠµë‹ˆë‹¤\"\n",
        "        ],\n",
        "        'ê²½ì œ': [\n",
        "            \"êµ­ë‚´ ê²½ì œ ì„±ì¥ë¥ ì´ ì „ë…„ ëŒ€ë¹„ ìƒìŠ¹ì„¸ë¥¼ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤\",\n",
        "            \"ì£¼ì‹ì‹œì¥ì—ì„œ ê¸°ìˆ ì£¼ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ìƒìŠ¹ ë ë¦¬ê°€ ì´ì–´ì§€ê³  ìˆìŠµë‹ˆë‹¤\",\n",
        "            \"ë¶€ë™ì‚° ì‹œì¥ì˜ ì•ˆì •í™” ì •ì±…ì´ ì‹œí–‰ë˜ë©´ì„œ ê±°ë˜ëŸ‰ì´ ì¡°ì •ë˜ê³  ìˆìŠµë‹ˆë‹¤\",\n",
        "            \"ì¤‘ì†Œê¸°ì—… ì§€ì› ì •ì±…ìœ¼ë¡œ ì°½ì—… ìƒíƒœê³„ê°€ í™œì„±í™”ë˜ê³  ìˆìŠµë‹ˆë‹¤\",\n",
        "            \"ê¸€ë¡œë²Œ ê³µê¸‰ë§ ì´ìŠˆê°€ êµ­ë‚´ ì œì¡°ì—…ì— ì˜í–¥ì„ ë¯¸ì¹˜ê³  ìˆìŠµë‹ˆë‹¤\"\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # ëª¨ë“  ë¬¸ì„œì™€ ë¼ë²¨ ìƒì„±\n",
        "    all_documents = []\n",
        "    all_labels = []\n",
        "\n",
        "    for category, documents in news_data.items():\n",
        "        all_documents.extend(documents)\n",
        "        all_labels.extend([category] * len(documents))\n",
        "\n",
        "    # Word2Vec ëª¨ë¸ í•™ìŠµ\n",
        "    app_model = Word2VecImplementation(\n",
        "        vector_size=100,\n",
        "        window_size=4,\n",
        "        epochs=300,\n",
        "        model_type='skipgram',\n",
        "        learning_rate=0.1\n",
        "    )\n",
        "    app_model.train(all_documents)\n",
        "\n",
        "    print(f\"âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ (ì–´íœ˜ í¬ê¸°: {app_model.vocab_size})\")\n",
        "\n",
        "    # ë¬¸ì„œ ë²¡í„°í™” (ë‹¨ì–´ ë²¡í„°ë“¤ì˜ í‰ê· )\n",
        "    def document_to_vector(doc, model):\n",
        "        words = model.preprocess_text(doc)\n",
        "        vectors = []\n",
        "        for word in words:\n",
        "            vec = model.get_vector(word)\n",
        "            if vec is not None:\n",
        "                vectors.append(vec)\n",
        "\n",
        "        if vectors:\n",
        "            return np.mean(vectors, axis=0)\n",
        "        else:\n",
        "            return np.zeros(model.vector_size)\n",
        "\n",
        "    # ë¬¸ì„œ ë²¡í„°ë“¤ ìƒì„±\n",
        "    doc_vectors = []\n",
        "    for doc in all_documents:\n",
        "        doc_vec = document_to_vector(doc, app_model)\n",
        "        doc_vectors.append(doc_vec)\n",
        "\n",
        "    doc_vectors = np.array(doc_vectors)\n",
        "\n",
        "    # ë¬¸ì„œ ìœ ì‚¬ë„ ë¶„ì„\n",
        "    print(f\"\\nğŸ“Š ë¬¸ì„œ ìœ ì‚¬ë„ ë¶„ì„:\")\n",
        "\n",
        "    # ì¹´í…Œê³ ë¦¬ ë‚´/ê°„ í‰ê·  ìœ ì‚¬ë„ ê³„ì‚°\n",
        "    category_similarities = defaultdict(list)\n",
        "\n",
        "    for i in range(len(all_documents)):\n",
        "        for j in range(i+1, len(all_documents)):\n",
        "            sim = cosine_similarity([doc_vectors[i]], [doc_vectors[j]])[0][0]\n",
        "\n",
        "            if all_labels[i] == all_labels[j]:\n",
        "                category_similarities['ê°™ì€_ì¹´í…Œê³ ë¦¬'].append(sim)\n",
        "            else:\n",
        "                category_similarities['ë‹¤ë¥¸_ì¹´í…Œê³ ë¦¬'].append(sim)\n",
        "\n",
        "    same_cat_avg = np.mean(category_similarities['ê°™ì€_ì¹´í…Œê³ ë¦¬'])\n",
        "    diff_cat_avg = np.mean(category_similarities['ë‹¤ë¥¸_ì¹´í…Œê³ ë¦¬'])\n",
        "\n",
        "    print(f\"  ê°™ì€ ì¹´í…Œê³ ë¦¬ ë¬¸ì„œ ê°„ í‰ê·  ìœ ì‚¬ë„: {same_cat_avg:.4f}\")\n",
        "    print(f\"  ë‹¤ë¥¸ ì¹´í…Œê³ ë¦¬ ë¬¸ì„œ ê°„ í‰ê·  ìœ ì‚¬ë„: {diff_cat_avg:.4f}\")\n",
        "    print(f\"  êµ¬ë³„ ì„±ëŠ¥: {same_cat_avg - diff_cat_avg:.4f}\")\n",
        "\n",
        "    # ë¬¸ì„œ ì¶”ì²œ ì‹œìŠ¤í…œ\n",
        "    print(f\"\\nğŸ¯ ë¬¸ì„œ ì¶”ì²œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸:\")\n",
        "\n",
        "    test_queries = [\n",
        "        \"ì¸ê³µì§€ëŠ¥ê³¼ ë¨¸ì‹ ëŸ¬ë‹ ê¸°ìˆ  ë™í–¥\",\n",
        "        \"ì¶•êµ¬ ê²½ê¸° ê²°ê³¼ì™€ ì„ ìˆ˜ ë¶„ì„\",\n",
        "        \"ê²½ì œ ì„±ì¥ê³¼ ì£¼ì‹ì‹œì¥ ì „ë§\"\n",
        "    ]\n",
        "\n",
        "    for query in test_queries:\n",
        "        print(f\"\\n  ì¿¼ë¦¬: '{query}'\")\n",
        "        query_vector = document_to_vector(query, app_model)\n",
        "\n",
        "        # ëª¨ë“  ë¬¸ì„œì™€ì˜ ìœ ì‚¬ë„ ê³„ì‚°\n",
        "        similarities = []\n",
        "        for i, doc_vec in enumerate(doc_vectors):\n",
        "            sim = cosine_similarity([query_vector], [doc_vec])[0][0]\n",
        "            similarities.append((i, sim, all_labels[i]))\n",
        "\n",
        "        # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬\n",
        "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        print(f\"    ì¶”ì²œ ë¬¸ì„œ (ìƒìœ„ 3ê°œ):\")\n",
        "        for i, (doc_idx, sim, category) in enumerate(similarities[:3]):\n",
        "            print(f\"      {i+1}. [{category}] ìœ ì‚¬ë„: {sim:.4f}\")\n",
        "            print(f\"         {all_documents[doc_idx][:50]}...\")\n",
        "\n",
        "    # ì‹œê°í™”\n",
        "    ensure_korean_font()\n",
        "\n",
        "    # PCAë¡œ ë¬¸ì„œ ë²¡í„° ì‹œê°í™”\n",
        "    pca = PCA(n_components=2)\n",
        "    doc_vectors_2d = pca.fit_transform(doc_vectors)\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    colors = {'ê¸°ìˆ ': 'red', 'ìŠ¤í¬ì¸ ': 'blue', 'ê²½ì œ': 'green'}\n",
        "\n",
        "    for category in colors.keys():\n",
        "        indices = [i for i, label in enumerate(all_labels) if label == category]\n",
        "        plt.scatter(doc_vectors_2d[indices, 0], doc_vectors_2d[indices, 1],\n",
        "                   c=colors[category], label=category, s=100, alpha=0.7)\n",
        "\n",
        "    plt.title('Word2Vec ê¸°ë°˜ ë¬¸ì„œ ë²¡í„° ì‹œê°í™”', fontsize=16, pad=20)\n",
        "    plt.xlabel('PC1')\n",
        "    plt.ylabel('PC2')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return app_model, doc_vectors\n",
        "\n",
        "\n",
        "def analyze_word2vec_properties():\n",
        "    \"\"\"Word2Vec ì†ì„± ë° íŠ¹ì„± ë¶„ì„\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"6. Word2Vec ì†ì„± ë° íŠ¹ì„± ë¶„ì„\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # ë¶„ì„ìš© ë¬¸ì„œ - ëª…í™•í•œ ê´€ê³„ê°€ ìˆëŠ” ë‹¨ì–´ë“¤\n",
        "    analysis_sentences = [\n",
        "        \"ì‚¬ê³¼ ë°”ë‚˜ë‚˜ ì˜¤ë Œì§€ëŠ” ë‹¬ì½¤í•œ ê³¼ì¼ì…ë‹ˆë‹¤\",\n",
        "        \"í¬ë„ ë”¸ê¸° ë³µìˆ­ì•„ë„ ë§›ìˆëŠ” ê³¼ì¼ì…ë‹ˆë‹¤\",\n",
        "        \"ê°œ ê³ ì–‘ì´ í† ë¼ëŠ” ê·€ì—¬ìš´ ë™ë¬¼ì…ë‹ˆë‹¤\",\n",
        "        \"ì‚¬ì í˜¸ë‘ì´ ì½”ë¼ë¦¬ëŠ” í° ë™ë¬¼ì…ë‹ˆë‹¤\",\n",
        "        \"ë¹¨ê°„ìƒ‰ íŒŒë€ìƒ‰ ë…¸ë€ìƒ‰ì€ ì•„ë¦„ë‹¤ìš´ ìƒ‰ê¹”ì…ë‹ˆë‹¤\",\n",
        "        \"ê²€ì€ìƒ‰ í•˜ì–€ìƒ‰ íšŒìƒ‰ë„ ì¤‘ìš”í•œ ìƒ‰ê¹”ì…ë‹ˆë‹¤\",\n",
        "        \"í° ì‘ì€ ë†’ì€ ë‚®ì€ì€ í¬ê¸°ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤\",\n",
        "        \"ë¹ ë¥¸ ëŠë¦° ê°•í•œ ì•½í•œì€ ì •ë„ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤\"\n",
        "    ]\n",
        "\n",
        "    # ëª¨ë¸ í•™ìŠµ\n",
        "    analysis_model = Word2VecImplementation(\n",
        "        vector_size=50,\n",
        "        window_size=3,\n",
        "        epochs=400,\n",
        "        model_type='skipgram',\n",
        "        learning_rate=0.1\n",
        "    )\n",
        "    analysis_model.train(analysis_sentences)\n",
        "\n",
        "    # 1. ë²¡í„° ê³µê°„ì—ì„œì˜ ê±°ë¦¬ì™€ ìœ ì‚¬ë„ ê´€ê³„\n",
        "    print(f\"\\nğŸ“ ë²¡í„° ê±°ë¦¬ì™€ ìœ ì‚¬ë„ ê´€ê³„:\")\n",
        "\n",
        "    test_pairs = [\n",
        "        (\"ì‚¬ê³¼\", \"ë°”ë‚˜ë‚˜\"),    # ê°™ì€ ì¹´í…Œê³ ë¦¬\n",
        "        (\"ì‚¬ê³¼\", \"ê°œ\"),       # ë‹¤ë¥¸ ì¹´í…Œê³ ë¦¬\n",
        "        (\"í°\", \"ì‘ì€\"),       # ë°˜ëŒ€ ê´€ê³„\n",
        "        (\"ë¹¨ê°„ìƒ‰\", \"íŒŒë€ìƒ‰\")  # ê°™ì€ ì¹´í…Œê³ ë¦¬, ë‹¤ë¥¸ íŠ¹ì„±\n",
        "    ]\n",
        "\n",
        "    for word1, word2 in test_pairs:\n",
        "        if word1 in analysis_model.word_to_idx and word2 in analysis_model.word_to_idx:\n",
        "            vec1 = analysis_model.get_vector(word1)\n",
        "            vec2 = analysis_model.get_vector(word2)\n",
        "\n",
        "            # ìœ í´ë¦¬ë“œ ê±°ë¦¬\n",
        "            euclidean_dist = np.linalg.norm(vec1 - vec2)\n",
        "\n",
        "            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„\n",
        "            cosine_sim = analysis_model.similarity(word1, word2)\n",
        "\n",
        "            print(f\"  {word1} - {word2}:\")\n",
        "            print(f\"    ìœ í´ë¦¬ë“œ ê±°ë¦¬: {euclidean_dist:.4f}\")\n",
        "            print(f\"    ì½”ì‚¬ì¸ ìœ ì‚¬ë„: {cosine_sim:.4f}\")\n",
        "\n",
        "    # 2. ì°¨ì› ìˆ˜ì— ë”°ë¥¸ ì„±ëŠ¥ ë¹„êµ\n",
        "    print(f\"\\nğŸ“Š ì°¨ì› ìˆ˜ì— ë”°ë¥¸ ì„±ëŠ¥ ë¹„êµ:\")\n",
        "\n",
        "    dimensions = [10, 30, 50, 100]\n",
        "    results = []\n",
        "\n",
        "    for dim in dimensions:\n",
        "        model = Word2VecImplementation(\n",
        "            vector_size=dim,\n",
        "            window_size=3,\n",
        "            epochs=200,\n",
        "            model_type='skipgram',\n",
        "            learning_rate=0.1\n",
        "        )\n",
        "        model.train(analysis_sentences)\n",
        "\n",
        "        # ê°™ì€ ì¹´í…Œê³ ë¦¬ ë‹¨ì–´ë“¤ì˜ í‰ê·  ìœ ì‚¬ë„\n",
        "        fruit_words = [\"ì‚¬ê³¼\", \"ë°”ë‚˜ë‚˜\", \"í¬ë„\"]\n",
        "        similarities = []\n",
        "\n",
        "        for i in range(len(fruit_words)):\n",
        "            for j in range(i+1, len(fruit_words)):\n",
        "                if fruit_words[i] in model.word_to_idx and fruit_words[j] in model.word_to_idx:\n",
        "                    sim = model.similarity(fruit_words[i], fruit_words[j])\n",
        "                    similarities.append(sim)\n",
        "\n",
        "        avg_sim = np.mean(similarities) if similarities else 0\n",
        "        results.append(avg_sim)\n",
        "        print(f\"  {dim}ì°¨ì›: ê³¼ì¼ ë‹¨ì–´ í‰ê·  ìœ ì‚¬ë„ = {avg_sim:.4f}\")\n",
        "\n",
        "    # 3. Skip-gram vs CBOW ì„±ëŠ¥ ë¹„êµ\n",
        "    print(f\"\\nâš”ï¸ Skip-gram vs CBOW ì„±ëŠ¥ ë¹„êµ:\")\n",
        "\n",
        "    # Skip-gram ëª¨ë¸\n",
        "    sg_model = Word2VecImplementation(\n",
        "        vector_size=50, window_size=3, epochs=300,\n",
        "        model_type='skipgram', learning_rate=0.1\n",
        "    )\n",
        "    sg_model.train(analysis_sentences)\n",
        "\n",
        "    # CBOW ëª¨ë¸\n",
        "    cbow_model = Word2VecImplementation(\n",
        "        vector_size=50, window_size=3, epochs=300,\n",
        "        model_type='cbow', learning_rate=0.1\n",
        "    )\n",
        "    cbow_model.train(analysis_sentences)\n",
        "\n",
        "    # ì„±ëŠ¥ ë¹„êµ: ìœ ì‚¬ ë‹¨ì–´ ì°¾ê¸°\n",
        "    test_word = \"ì‚¬ê³¼\"\n",
        "    if test_word in sg_model.word_to_idx and test_word in cbow_model.word_to_idx:\n",
        "        sg_similar = sg_model.most_similar(test_word, top_k=3)\n",
        "        cbow_similar = cbow_model.most_similar(test_word, top_k=3)\n",
        "\n",
        "        print(f\"  '{test_word}'ì˜ ìœ ì‚¬ ë‹¨ì–´:\")\n",
        "        print(f\"    Skip-gram: {sg_similar}\")\n",
        "        print(f\"    CBOW: {cbow_similar}\")\n",
        "\n",
        "    # 4. ì‹œê°í™” - í•™ìŠµ ê³¼ì • ì‹œë®¬ë ˆì´ì…˜\n",
        "    ensure_korean_font()\n",
        "\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # ì°¨ì›ë³„ ì„±ëŠ¥ ê·¸ë˜í”„\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(dimensions, results, 'bo-', linewidth=2, markersize=8)\n",
        "    plt.title('ë²¡í„° ì°¨ì› ìˆ˜ì— ë”°ë¥¸ ì„±ëŠ¥', fontsize=14)\n",
        "    plt.xlabel('ì°¨ì› ìˆ˜')\n",
        "    plt.ylabel('í‰ê·  ìœ ì‚¬ë„')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # ë‹¨ì–´ ë²¡í„° ì‹œê°í™” (PCA)\n",
        "    plt.subplot(2, 2, 2)\n",
        "\n",
        "    # ì£¼ìš” ë‹¨ì–´ë“¤ì˜ ë²¡í„°\n",
        "    target_words = [\"ì‚¬ê³¼\", \"ë°”ë‚˜ë‚˜\", \"ê°œ\", \"ê³ ì–‘ì´\", \"ë¹¨ê°„ìƒ‰\", \"íŒŒë€ìƒ‰\"]\n",
        "    vectors = []\n",
        "    labels = []\n",
        "\n",
        "    for word in target_words:\n",
        "        if word in analysis_model.word_to_idx:\n",
        "            vec = analysis_model.get_vector(word)\n",
        "            if vec is not None:\n",
        "                vectors.append(vec)\n",
        "                labels.append(word)\n",
        "\n",
        "    if len(vectors) >= 2:\n",
        "        pca = PCA(n_components=2)\n",
        "        vectors_2d = pca.fit_transform(vectors)\n",
        "\n",
        "        colors = ['red', 'red', 'blue', 'blue', 'green', 'green']\n",
        "\n",
        "        for i, (word, color) in enumerate(zip(labels, colors)):\n",
        "            plt.scatter(vectors_2d[i, 0], vectors_2d[i, 1],\n",
        "                       c=color, s=150, alpha=0.7)\n",
        "            plt.annotate(word, (vectors_2d[i, 0], vectors_2d[i, 1]),\n",
        "                        xytext=(5, 5), textcoords='offset points', fontsize=10)\n",
        "\n",
        "    plt.title('ë‹¨ì–´ ë²¡í„° ê³µê°„ (PCA)', fontsize=14)\n",
        "    plt.xlabel('PC1')\n",
        "    plt.ylabel('PC2')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # ìœ ì‚¬ë„ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨\n",
        "    plt.subplot(2, 2, 3)\n",
        "\n",
        "    all_similarities = []\n",
        "    words = list(analysis_model.word_to_idx.keys())\n",
        "\n",
        "    for i in range(len(words)):\n",
        "        for j in range(i+1, min(len(words), i+10)):  # ê³„ì‚°ëŸ‰ ì œí•œ\n",
        "            sim = analysis_model.similarity(words[i], words[j])\n",
        "            all_similarities.append(sim)\n",
        "\n",
        "    plt.hist(all_similarities, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "    plt.title('ë‹¨ì–´ ê°„ ìœ ì‚¬ë„ ë¶„í¬', fontsize=14)\n",
        "    plt.xlabel('ìœ ì‚¬ë„')\n",
        "    plt.ylabel('ë¹ˆë„')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # ë²¡í„° í¬ê¸° ë¶„í¬\n",
        "    plt.subplot(2, 2, 4)\n",
        "\n",
        "    vector_norms = []\n",
        "    for word in words:\n",
        "        vec = analysis_model.get_vector(word)\n",
        "        if vec is not None:\n",
        "            norm = np.linalg.norm(vec)\n",
        "            vector_norms.append(norm)\n",
        "\n",
        "    plt.hist(vector_norms, bins=15, alpha=0.7, color='lightcoral', edgecolor='black')\n",
        "    plt.title('ë²¡í„° í¬ê¸° ë¶„í¬', fontsize=14)\n",
        "    plt.xlabel('ë²¡í„° í¬ê¸°')\n",
        "    plt.ylabel('ë¹ˆë„')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return analysis_model\n",
        "\n",
        "\n",
        "def run_complete_word2vec_demo():\n",
        "    \"\"\"ì „ì²´ Word2Vec ë°ëª¨ ì‹¤í–‰\"\"\"\n",
        "    print(\"ğŸš€ Word2Vec ì™„ì „ ê°€ì´ë“œ - ì‹¤ìŠµ ì‹œì‘!\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    try:\n",
        "        # 1. ê¸°ë³¸ êµ¬í˜„\n",
        "        print(\"1ï¸âƒ£ ê¸°ë³¸ Word2Vec êµ¬í˜„...\")\n",
        "        skipgram_model, cbow_model = demonstrate_basic_word2vec()\n",
        "\n",
        "        # 2. Gensim ë¹„êµ\n",
        "        print(\"\\n2ï¸âƒ£ Gensim Word2Vec ë¹„êµ...\")\n",
        "        gensim_model = demonstrate_gensim_word2vec()\n",
        "\n",
        "        # 3. ì‹œê°í™”\n",
        "        print(\"\\n3ï¸âƒ£ Word2Vec ë²¡í„° ì‹œê°í™”...\")\n",
        "        viz_model = demonstrate_word2vec_visualization()\n",
        "\n",
        "        # 4. ë‹¨ì–´ ìœ ì¶”\n",
        "        print(\"\\n4ï¸âƒ£ ë‹¨ì–´ ìœ ì¶” í…ŒìŠ¤íŠ¸...\")\n",
        "        demonstrate_word_analogies()\n",
        "\n",
        "        # 5. ì‹¤ì „ ì‘ìš©\n",
        "        print(\"\\n5ï¸âƒ£ ì‹¤ì „ ì‘ìš©...\")\n",
        "        app_model, doc_vectors = demonstrate_practical_applications()\n",
        "\n",
        "        # 6. ì†ì„± ë¶„ì„\n",
        "        print(\"\\n6ï¸âƒ£ Word2Vec ì†ì„± ë¶„ì„...\")\n",
        "        analysis_model = analyze_word2vec_properties()\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"ğŸ‰ Word2Vec ì™„ì „ ê°€ì´ë“œ ì‹¤ìŠµ ì™„ë£Œ!\")\n",
        "        print(\"ğŸ“š í•™ìŠµí•œ ë‚´ìš©:\")\n",
        "        print(\"  âœ… Word2Vec ê¸°ë³¸ ì›ë¦¬ ë° êµ¬í˜„ (Skip-gram & CBOW)\")\n",
        "        print(\"  âœ… Gensimê³¼ì˜ ì„±ëŠ¥ ë¹„êµ\")\n",
        "        print(\"  âœ… ë²¡í„° ì‹œê°í™” ë° í´ëŸ¬ìŠ¤í„°ë§\")\n",
        "        print(\"  âœ… ë‹¨ì–´ ìœ ì¶” (Word Analogies)\")\n",
        "        print(\"  âœ… ë¬¸ì„œ ë¶„ë¥˜ ë° ì¶”ì²œ ì‹œìŠ¤í…œ\")\n",
        "        print(\"  âœ… Word2Vec íŠ¹ì„± ë¶„ì„\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\")\n",
        "        print(\"ê¸°ë³¸ ì˜ˆì œë§Œ ì‹¤í–‰í•©ë‹ˆë‹¤.\")\n",
        "\n",
        "        # ê¸°ë³¸ ì˜ˆì œë§Œ ì‹¤í–‰\n",
        "        sentences = [\n",
        "            \"ì¸ê³µì§€ëŠ¥ì€ ë¯¸ë˜ ê¸°ìˆ ì…ë‹ˆë‹¤\",\n",
        "            \"ë¨¸ì‹ ëŸ¬ë‹ìœ¼ë¡œ ë°ì´í„°ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤\",\n",
        "            \"ë”¥ëŸ¬ë‹ì€ ë³µì¡í•œ íŒ¨í„´ì„ í•™ìŠµí•©ë‹ˆë‹¤\"\n",
        "        ]\n",
        "\n",
        "        basic_model = Word2VecImplementation(vector_size=50, epochs=100)\n",
        "        basic_model.train(sentences)\n",
        "\n",
        "        print(\"âœ… ê¸°ë³¸ Word2Vec ëª¨ë¸ ìƒì„± ì™„ë£Œ\")\n",
        "        print(f\"ì–´íœ˜ í¬ê¸°: {basic_model.vocab_size}\")\n",
        "\n",
        "\n",
        "# ì‹¤í–‰ë¶€\n",
        "if __name__ == \"__main__\":\n",
        "    # ì „ì²´ ë°ëª¨ ì‹¤í–‰\n",
        "    run_complete_word2vec_demo()"
      ],
      "metadata": {
        "id": "EtuRoYnMTgeH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "-IvIUrIXB3hH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. ë‹¤ì–‘í•œ ì„ë² ë”© ì¢…ë¥˜**\n"
      ],
      "metadata": {
        "id": "C5oQMyRtB4SG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ì˜ˆì œ : ì´ë¯¸ì§€ ì„ë² ë”©**"
      ],
      "metadata": {
        "id": "7XqRLmBLF3ZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python scikit-learn matplotlib seaborn numpy pandas"
      ],
      "metadata": {
        "id": "6wDeF4fIZ9Z6",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ì´ë¯¸ì§€ íŒŒì¼ ì¤€ë¹„(dog.jpg)"
      ],
      "metadata": {
        "id": "bPsdvqoYAOQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "\n",
        "# 1. ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° (ImageNet ì‚¬ì „í•™ìŠµ)\n",
        "model = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
        "\n",
        "# 2. ì´ë¯¸ì§€ ë¡œë“œ & ì „ì²˜ë¦¬ (ì´ë¯¸ì§€ íŒŒì¼ ì¤€ë¹„)\n",
        "img_path = \"dog.jpg\"\n",
        "img = image.load_img(img_path, target_size=(224, 224))\n",
        "x = image.img_to_array(img)\n",
        "x = np.expand_dims(x, axis=0)\n",
        "x = preprocess_input(x)\n",
        "\n",
        "# 3. ì„ë² ë”© ì¶”ì¶œ\n",
        "embedding = model.predict(x)\n",
        "print(\"ì„ë² ë”© ë²¡í„° shape:\", embedding.shape)  # (1, 2048)\n"
      ],
      "metadata": {
        "id": "W47wSwFy7Sv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================\n",
        "# URL ì—†ì´ ë™ì‘: CIFAR-10ì—ì„œ dog/cat/car ì¶”ì¶œ â†’ ResNet50 ì„ë² ë”© â†’ ì½”ì‚¬ì¸ ìœ ì‚¬ë„\n",
        "# ===============================================\n",
        "import os, pathlib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "\n",
        "print(\"TensorFlow:\", tf.__version__)\n",
        "\n",
        "\n",
        "# 0) ì €ì¥ í´ë”\n",
        "IMG_DIR = \"sample_images\"\n",
        "os.makedirs(IMG_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "# 1) CIFAR-10 ë¡œë“œ (ìë™ ë‹¤ìš´ë¡œë“œ) â€” í´ë˜ìŠ¤ ë¼ë²¨ ë§¤í•‘\n",
        "# CIFAR-10 label names: 0 airplane, 1 automobile, 2 bird, 3 cat, 4 deer, 5 dog, 6 frog, 7 horse, 8 ship, 9 truck\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "y_train = y_train.flatten()\n",
        "\n",
        "label_to_name = {\n",
        "    1: \"car\",   # automobile\n",
        "    3: \"cat\",\n",
        "    5: \"dog\"\n",
        "}\n",
        "\n",
        "picked_paths = {}\n",
        "for label, name in label_to_name.items():\n",
        "    # í•´ë‹¹ í´ë˜ìŠ¤ë¥¼ ê°€ì§„ ì²« ë²ˆì§¸ ì´ë¯¸ì§€ë¥¼ ì„ íƒ\n",
        "    idx = np.where(y_train == label)[0][0]\n",
        "    img_arr = x_train[idx]  # (32,32,3) uint8\n",
        "    # ì €ì¥ íŒŒì¼ëª…\n",
        "    fname = f\"{name}.jpg\"\n",
        "    fpath = os.path.join(IMG_DIR, fname)\n",
        "    # 224x224ë¡œ ì—…ìŠ¤ì¼€ì¼ ì €ì¥ (ResNet ì…ë ¥ í¬ê¸°)\n",
        "    Image.fromarray(img_arr).resize((224, 224), Image.BICUBIC).save(fpath, format=\"JPEG\", quality=95)\n",
        "    picked_paths[name] = fpath\n",
        "\n",
        "print(\"ì €ì¥ëœ íŒŒì¼:\", picked_paths)\n",
        "\n",
        "\n",
        "# 2) ResNet50 (ImageNet ì‚¬ì „í•™ìŠµ, ë¶„ë¥˜í—¤ë“œ ì œê±°)\n",
        "model = ResNet50(weights='imagenet', include_top=False, pooling='avg')  # (1, 2048) ì„ë² ë”©\n",
        "\n",
        "def get_embedding(img_path):\n",
        "    img = image.load_img(img_path, target_size=(224, 224))   # í˜¹ì‹œ ëª¨ë¥¼ í¬ê¸° ë³´ì •\n",
        "    x = image.img_to_array(img)[None, ...]                   # (1,224,224,3)\n",
        "    x = preprocess_input(x)\n",
        "    emb = model.predict(x, verbose=0)                        # (1,2048)\n",
        "    return emb\n",
        "\n",
        "\n",
        "# 3) ì„ë² ë”© ì¶”ì¶œ\n",
        "image_files = [picked_paths[\"dog\"], picked_paths[\"cat\"], picked_paths[\"car\"]]\n",
        "embeddings = {p: get_embedding(p) for p in image_files}\n",
        "\n",
        "\n",
        "# 4) ì½”ì‚¬ì¸ ìœ ì‚¬ë„ í–‰ë ¬\n",
        "paths = list(embeddings.keys())\n",
        "mat = np.vstack([embeddings[p] for p in paths])              # (N,2048)\n",
        "sim = cosine_similarity(mat, mat)\n",
        "\n",
        "df_sim = pd.DataFrame(sim,\n",
        "                      index=[pathlib.Path(p).name for p in paths],\n",
        "                      columns=[pathlib.Path(p).name for p in paths])\n",
        "print(\"\\nğŸ“Š ì „ì²´ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ í–‰ë ¬\")\n",
        "try:\n",
        "    display(df_sim.style.format(\"{:.4f}\"))\n",
        "except NameError:\n",
        "    print(df_sim.round(4))\n",
        "\n",
        "\n",
        "\n",
        "# 5) dog.jpg ê¸°ì¤€ ìœ ì‚¬ë„ ì •ë ¬\n",
        "target_name = \"dog.jpg\"\n",
        "target_path = os.path.join(IMG_DIR, target_name)\n",
        "target_idx = paths.index(target_path)\n",
        "scores = sim[target_idx]\n",
        "\n",
        "ranked = sorted(\n",
        "    [(pathlib.Path(p).name, float(scores[i])) for i, p in enumerate(paths) if i != target_idx],\n",
        "    key=lambda x: x[1], reverse=True\n",
        ")\n",
        "\n",
        "print(f\"\\nâœ… '{target_name}'ì™€ ìœ ì‚¬ë„ê°€ ë†’ì€ ìˆœ\")\n",
        "for name, s in ranked:\n",
        "    print(f\"  {name:10s} : {s:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "# 6) ë¯¸ë‹ˆ ê°¤ëŸ¬ë¦¬ ì‹œê°í™”\n",
        "fig, axes = plt.subplots(1, 3, figsize=(10, 4))\n",
        "for ax, p in zip(axes, image_files):\n",
        "    ax.imshow(Image.open(p))\n",
        "    ax.set_title(pathlib.Path(p).name)\n",
        "    ax.axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# 7) ë°” ì°¨íŠ¸\n",
        "plt.figure(figsize=(5,3))\n",
        "names = [n for n, _ in ranked]\n",
        "vals  = [s for _, s in ranked]\n",
        "plt.bar(names, vals)\n",
        "plt.ylim(0, 1.0)\n",
        "for i, v in enumerate(vals):\n",
        "    plt.text(i, v+0.02, f\"{v:.3f}\", ha=\"center\")\n",
        "plt.title(f\"Cosine similarity to {target_name}\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "NuioOLed_PH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "_hGl48O7Hqxr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ì˜ˆì œ : í—ˆê¹…í˜ì´ìŠ¤ í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ í™œìš©**"
      ],
      "metadata": {
        "id": "EsGjt-zjHrGh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **ê¸°ë³¸ ì˜ˆì œ**"
      ],
      "metadata": {
        "id": "fe-am4AfIoMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "from typing import List, Dict\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class KoreanEmbeddingSystem:\n",
        "    def __init__(self, model_name='jhgan/ko-sroberta-multitask'):\n",
        "        \"\"\"\n",
        "        í•œêµ­ì–´ ì„ë² ë”© ì‹œìŠ¤í…œ ì´ˆê¸°í™”\n",
        "\n",
        "        ì¶”ì²œ ëª¨ë¸ë“¤:\n",
        "        - 'jhgan/ko-sroberta-multitask': í•œêµ­ì–´ Sentence-RoBERTa (ë¬¸ì¥ ì„ë² ë”© íŠ¹í™”)\n",
        "        - 'BM-K/KoSimCSE-roberta-multitask': KoSimCSE (ì˜ë¯¸ ìœ ì‚¬ë„ íŠ¹í™”)\n",
        "        - 'klue/roberta-base': KLUE RoBERTa (ë²”ìš©)\n",
        "        - 'monologg/kobert': KoBERT (SKT)\n",
        "        \"\"\"\n",
        "        print(f\"âœ… ëª¨ë¸ ë¡œë”© ì¤‘: {model_name}\")\n",
        "\n",
        "        self.model_name = model_name\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModel.from_pretrained(model_name)\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        print(f\"\\nâœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ! ë””ë°”ì´ìŠ¤: {self.device}\")\n",
        "\n",
        "    def encode_sentences(self, sentences: List[str], max_length: int = 512) -> np.ndarray:\n",
        "        \"\"\"ë¬¸ì¥ë“¤ì„ ë²¡í„°ë¡œ ì¸ì½”ë”©\"\"\"\n",
        "        embeddings = []\n",
        "\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for sentence in sentences:\n",
        "                # í† í°í™”\n",
        "                inputs = self.tokenizer(\n",
        "                    sentence,\n",
        "                    padding=True,\n",
        "                    truncation=True,\n",
        "                    max_length=max_length,\n",
        "                    return_tensors='pt'\n",
        "                ).to(self.device)\n",
        "\n",
        "                # ì„ë² ë”© ìƒì„±\n",
        "                outputs = self.model(**inputs)\n",
        "\n",
        "                # [CLS] í† í°ì˜ ì„ë² ë”©ì„ ë¬¸ì¥ ì„ë² ë”©ìœ¼ë¡œ ì‚¬ìš©\n",
        "                # ë˜ëŠ” í‰ê·  í’€ë§ ì‚¬ìš©\n",
        "                if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:\n",
        "                    embedding = outputs.pooler_output\n",
        "                else:\n",
        "                    # í‰ê·  í’€ë§\n",
        "                    embedding = outputs.last_hidden_state.mean(dim=1)\n",
        "\n",
        "                embeddings.append(embedding.cpu().numpy())\n",
        "\n",
        "        return np.vstack(embeddings)\n",
        "\n",
        "    def calculate_similarity(self, sentences1: List[str], sentences2: List[str] = None) -> np.ndarray:\n",
        "        \"\"\"ë¬¸ì¥ë“¤ ê°„ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°\"\"\"\n",
        "        embeddings1 = self.encode_sentences(sentences1)\n",
        "\n",
        "        if sentences2 is None:\n",
        "            # ìê¸° ìì‹ ê³¼ì˜ ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤\n",
        "            similarity_matrix = cosine_similarity(embeddings1)\n",
        "        else:\n",
        "            # ë‘ ê·¸ë£¹ ê°„ì˜ ìœ ì‚¬ë„\n",
        "            embeddings2 = self.encode_sentences(sentences2)\n",
        "            similarity_matrix = cosine_similarity(embeddings1, embeddings2)\n",
        "\n",
        "        return similarity_matrix\n",
        "\n",
        "    def find_most_similar(self, query: str, candidates: List[str], top_k: int = 5) -> List[Dict]:\n",
        "        \"\"\"ì§ˆì˜ë¬¸ê³¼ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì¥ë“¤ ì°¾ê¸°\"\"\"\n",
        "        all_sentences = [query] + candidates\n",
        "        embeddings = self.encode_sentences(all_sentences)\n",
        "\n",
        "        query_embedding = embeddings[0:1]\n",
        "        candidate_embeddings = embeddings[1:]\n",
        "\n",
        "        similarities = cosine_similarity(query_embedding, candidate_embeddings)[0]\n",
        "\n",
        "        # ìƒìœ„ kê°œ ê²°ê³¼\n",
        "        top_indices = similarities.argsort()[-top_k:][::-1]\n",
        "\n",
        "        results = []\n",
        "        for i, idx in enumerate(top_indices):\n",
        "            results.append({\n",
        "                'rank': i + 1,\n",
        "                'sentence': candidates[idx],\n",
        "                'similarity': similarities[idx]\n",
        "            })\n",
        "\n",
        "        return results\n",
        "\n",
        "# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜\n",
        "def main():\n",
        "    print(\"\\n=== í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ í™œìš© ì‹œìŠ¤í…œ ===\\n\")\n",
        "\n",
        "    # ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ì—¬ëŸ¬ ëª¨ë¸ ì¤‘ ì„ íƒ ê°€ëŠ¥)\n",
        "    embedding_system = KoreanEmbeddingSystem('jhgan/ko-sroberta-multitask')\n",
        "\n",
        "    # í…ŒìŠ¤íŠ¸ ë¬¸ì¥ë“¤\n",
        "    korean_sentences = [\n",
        "        \"ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì •ë§ ì¢‹ìŠµë‹ˆë‹¤.\",\n",
        "        \"ë‚ ì”¨ê°€ ë§¤ìš° í™”ì°½í•˜ë„¤ìš”.\",\n",
        "        \"ë¹„ê°€ ë§ì´ ë‚´ë¦¬ê³  ìˆì–´ìš”.\",\n",
        "        \"ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì´ ë¹ ë¥´ê²Œ ë°œì „í•˜ê³  ìˆë‹¤.\",\n",
        "        \"ë¨¸ì‹ ëŸ¬ë‹ì€ ì»´í“¨í„°ê°€ ë°ì´í„°ë¡œë¶€í„° í•™ìŠµí•˜ëŠ” ê¸°ìˆ ì´ë‹¤.\",\n",
        "        \"ì ì‹¬ì— ë­˜ ë¨¹ì„ì§€ ê³ ë¯¼ì´ì—ìš”.\",\n",
        "        \"ì˜¤ëŠ˜ ì €ë… ë©”ë‰´ë¥¼ ì •í•´ì•¼ê² ì–´ìš”.\",\n",
        "        \"ì£¼ì‹ ì‹œì¥ì´ ë¶ˆì•ˆì •í•©ë‹ˆë‹¤.\",\n",
        "        \"ê²½ì œ ìƒí™©ì´ ì¢‹ì§€ ì•Šì•„ ë³´ì…ë‹ˆë‹¤.\"\n",
        "    ]\n",
        "\n",
        "    print(\"\\nâœ… í…ŒìŠ¤íŠ¸ ë¬¸ì¥ë“¤:\")\n",
        "    for i, sentence in enumerate(korean_sentences, 1):\n",
        "        print(f\"{i}. {sentence}\")\n",
        "\n",
        "    print(f\"\\nâœ… ì´ {len(korean_sentences)}ê°œ ë¬¸ì¥ì˜ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤...\")\n",
        "\n",
        "    # 1. ë¬¸ì¥ ì„ë² ë”© ìƒì„±\n",
        "    embeddings = embedding_system.encode_sentences(korean_sentences)\n",
        "    print(f\"ì„ë² ë”© shape: {embeddings.shape}\")\n",
        "    print(f\"ê° ë¬¸ì¥ì€ {embeddings.shape[1]}ì°¨ì› ë²¡í„°ë¡œ í‘œí˜„ë©ë‹ˆë‹¤.\")\n",
        "\n",
        "    # 2. ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°\n",
        "    print(\"\\nâœ… === ë¬¸ì¥ ê°„ ìœ ì‚¬ë„ ë¶„ì„ ===\")\n",
        "    similarity_matrix = embedding_system.calculate_similarity(korean_sentences)\n",
        "\n",
        "    # ìœ ì‚¬ë„ê°€ ë†’ì€ ë¬¸ì¥ ìŒ ì°¾ê¸°\n",
        "    print(\"\\nê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì¥ ìŒë“¤ (Top 5):\")\n",
        "    similar_pairs = []\n",
        "\n",
        "    for i in range(len(korean_sentences)):\n",
        "        for j in range(i+1, len(korean_sentences)):\n",
        "            similarity = similarity_matrix[i][j]\n",
        "            similar_pairs.append({\n",
        "                'sentence1': korean_sentences[i],\n",
        "                'sentence2': korean_sentences[j],\n",
        "                'similarity': similarity\n",
        "            })\n",
        "\n",
        "    # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬\n",
        "    similar_pairs.sort(key=lambda x: x['similarity'], reverse=True)\n",
        "\n",
        "    for i, pair in enumerate(similar_pairs[:5], 1):\n",
        "        print(f\"{i}. ìœ ì‚¬ë„: {pair['similarity']:.4f}\")\n",
        "        print(f\"   ë¬¸ì¥1: {pair['sentence1']}\")\n",
        "        print(f\"   ë¬¸ì¥2: {pair['sentence2']}\")\n",
        "        print()\n",
        "\n",
        "    # 3. ì§ˆì˜ ê¸°ë°˜ ìœ ì‚¬ ë¬¸ì¥ ê²€ìƒ‰\n",
        "    print(\"\\nâœ… === ì§ˆì˜ ê¸°ë°˜ ë¬¸ì¥ ê²€ìƒ‰ ===\")\n",
        "    query = \"ë‚ ì”¨ì— ê´€í•œ ì´ì•¼ê¸°\"\n",
        "    print(f\"ì§ˆì˜: '{query}'\")\n",
        "\n",
        "    results = embedding_system.find_most_similar(query, korean_sentences, top_k=3)\n",
        "\n",
        "    print(f\"\\n'{query}'ì™€ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì¥ë“¤:\")\n",
        "    for result in results:\n",
        "        print(f\"{result['rank']}ìœ„. {result['sentence']} (ìœ ì‚¬ë„: {result['similarity']:.4f})\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "GzUSU8FhH0pK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ**"
      ],
      "metadata": {
        "id": "7-iK2Ht7IgZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_korean_models():\n",
        "    \"\"\"ì—¬ëŸ¬ í•œêµ­ì–´ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ\"\"\"\n",
        "    print(\"=== í•œêµ­ì–´ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ===\")\n",
        "\n",
        "    models = [\n",
        "        'jhgan/ko-sroberta-multitask',\n",
        "        'BM-K/KoSimCSE-roberta-multitask',\n",
        "        'klue/roberta-base'\n",
        "    ]\n",
        "\n",
        "    test_sentences = [\n",
        "        \"í•œêµ­ì–´ ìì—°ì–´ì²˜ë¦¬ëŠ” ì–´ë ¤ìš´ ë¶„ì•¼ì…ë‹ˆë‹¤.\",\n",
        "        \"ìì—°ì–´ì²˜ë¦¬ ê¸°ìˆ ì€ ë³µì¡í•œ ì˜ì—­ì´ì—ìš”.\",\n",
        "        \"ì˜¤ëŠ˜ì€ ë¹„ê°€ ì™€ì„œ ìš°ìš¸í•´ìš”.\",\n",
        "        \"ë‚ ì”¨ê°€ íë ¤ì„œ ê¸°ë¶„ì´ ì•ˆ ì¢‹ë„¤ìš”.\"\n",
        "    ]\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for model_name in models:\n",
        "        try:\n",
        "            print(f\"\\n{model_name} í…ŒìŠ¤íŠ¸ ì¤‘...\")\n",
        "            system = KoreanEmbeddingSystem(model_name)\n",
        "\n",
        "            # ìœ ì‚¬ë„ ê³„ì‚°\n",
        "            similarity_matrix = system.calculate_similarity(test_sentences)\n",
        "\n",
        "            # ê´€ë ¨ ë¬¸ì¥ ìŒì˜ ìœ ì‚¬ë„ ì¸¡ì •\n",
        "            related_similarity = (similarity_matrix[0][1] + similarity_matrix[2][3]) / 2\n",
        "            unrelated_similarity = (similarity_matrix[0][2] + similarity_matrix[1][3]) / 2\n",
        "\n",
        "            results[model_name] = {\n",
        "                'related_similarity': related_similarity,\n",
        "                'unrelated_similarity': unrelated_similarity,\n",
        "                'discrimination': related_similarity - unrelated_similarity\n",
        "            }\n",
        "\n",
        "            print(f\"ê´€ë ¨ ë¬¸ì¥ ìœ ì‚¬ë„: {related_similarity:.4f}\")\n",
        "            print(f\"ë¬´ê´€í•œ ë¬¸ì¥ ìœ ì‚¬ë„: {unrelated_similarity:.4f}\")\n",
        "            print(f\"êµ¬ë¶„ë ¥: {results[model_name]['discrimination']:.4f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
        "\n",
        "    # ê²°ê³¼ ì •ë¦¬\n",
        "    print(\"\\n=== ëª¨ë¸ ì„±ëŠ¥ ìš”ì•½ ===\")\n",
        "    for model, metrics in results.items():\n",
        "        print(f\"{model.split('/')[-1]}: êµ¬ë¶„ë ¥ {metrics['discrimination']:.4f}\")\n",
        "\n",
        "# ì˜ˆì œ ì‹¤í–‰\n",
        "compare_korean_models()"
      ],
      "metadata": {
        "id": "dW03Vu4JIc6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **í•œêµ­ì–´ ê°ì • ë¶„ì„ ì„ë² ë”©**"
      ],
      "metadata": {
        "id": "Xk0oWjGgIvyX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_korean_emotions():\n",
        "    \"\"\"í•œêµ­ì–´ ê°ì • í‘œí˜„ ë¶„ì„\"\"\"\n",
        "    print(\"=== í•œêµ­ì–´ ê°ì • í‘œí˜„ ë¶„ì„ ===\")\n",
        "\n",
        "    system = KoreanEmbeddingSystem('jhgan/ko-sroberta-multitask')\n",
        "\n",
        "    emotion_sentences = {\n",
        "        'ê¸°ì¨': [\n",
        "            \"ì˜¤ëŠ˜ ì •ë§ í–‰ë³µí•œ í•˜ë£¨ì˜€ì–´ìš”!\",\n",
        "            \"ì¢‹ì€ ì†Œì‹ì„ ë“¤ì–´ì„œ ê¸°ë¶„ì´ ë‚ ì•„ê°ˆ ê²ƒ ê°™ì•„ìš”.\",\n",
        "            \"ë“œë””ì–´ ê¿ˆê¾¸ë˜ ì¼ì´ ì´ë£¨ì–´ì¡ŒìŠµë‹ˆë‹¤!\"\n",
        "        ],\n",
        "        'ìŠ¬í””': [\n",
        "            \"ì˜¤ëŠ˜ ë„ˆë¬´ ìš°ìš¸í•˜ê³  ìŠ¬íì–´ìš”.\",\n",
        "            \"í˜ë“  ì¼ì´ ìˆì–´ì„œ ë§ˆìŒì´ ì•„í”•ë‹ˆë‹¤.\",\n",
        "            \"ì´ë³„ì˜ ì•„í””ì´ ë„ˆë¬´ ì»¤ìš”.\"\n",
        "        ],\n",
        "        'ë¶„ë…¸': [\n",
        "            \"ì •ë§ í™”ê°€ ë‚˜ì„œ ì°¸ì„ ìˆ˜ ì—†ì–´ìš”!\",\n",
        "            \"ì´ëŸ° ì¼ì´ ìˆì„ ìˆ˜ ìˆë‚˜ìš”? ë„ˆë¬´ ì–µìš¸í•´ìš”.\",\n",
        "            \"ë¶„í†µì´ í„°ì§ˆ ê²ƒ ê°™ìŠµë‹ˆë‹¤.\"\n",
        "        ],\n",
        "        'ë†€ë¼ì›€': [\n",
        "            \"ì´ëŸ° ì¼ì´! ì •ë§ ë†€ëë„¤ìš”.\",\n",
        "            \"ê¹œì§ ë†€ëì–´ìš”. ì˜ˆìƒì¹˜ ëª»í•œ ì¼ì´ì—ìš”.\",\n",
        "            \"ë¯¿ì„ ìˆ˜ ì—†ì–´ìš”. ë„ˆë¬´ ì‹ ê¸°í•©ë‹ˆë‹¤.\"\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # ê° ê°ì •ë³„ ëŒ€í‘œ ì„ë² ë”© ê³„ì‚°\n",
        "    emotion_embeddings = {}\n",
        "    all_sentences = []\n",
        "    emotion_labels = []\n",
        "\n",
        "    for emotion, sentences in emotion_sentences.items():\n",
        "        embeddings = system.encode_sentences(sentences)\n",
        "        emotion_embeddings[emotion] = np.mean(embeddings, axis=0)\n",
        "        all_sentences.extend(sentences)\n",
        "        emotion_labels.extend([emotion] * len(sentences))\n",
        "\n",
        "    # ìƒˆë¡œìš´ ë¬¸ì¥ì˜ ê°ì • ì˜ˆì¸¡\n",
        "    test_sentences = [\n",
        "        \"ì‹œí—˜ì— í•©ê²©í•´ì„œ ë„ˆë¬´ ê¸°ë»ìš”!\",\n",
        "        \"ì¹œêµ¬ì™€ ì‹¸ì›Œì„œ ë§ˆìŒì´ ì•„íŒŒìš”.\",\n",
        "        \"ê°‘ìê¸° ë¹„ê°€ ì™€ì„œ ë‹¹í™©ìŠ¤ëŸ¬ì›Œìš”.\"\n",
        "    ]\n",
        "\n",
        "    print(\"ê°ì • ì˜ˆì¸¡ ê²°ê³¼:\")\n",
        "    for test_sentence in test_sentences:\n",
        "        test_embedding = system.encode_sentences([test_sentence])[0]\n",
        "\n",
        "        similarities = {}\n",
        "        for emotion, emotion_emb in emotion_embeddings.items():\n",
        "            similarity = cosine_similarity([test_embedding], [emotion_emb])[0][0]\n",
        "            similarities[emotion] = similarity\n",
        "\n",
        "        predicted_emotion = max(similarities, key=similarities.get)\n",
        "        confidence = similarities[predicted_emotion]\n",
        "\n",
        "        print(f\"\\në¬¸ì¥: '{test_sentence}'\")\n",
        "        print(f\"ì˜ˆì¸¡ ê°ì •: {predicted_emotion} (ì‹ ë¢°ë„: {confidence:.4f})\")\n",
        "\n",
        "        # ëª¨ë“  ê°ì •ë³„ ì ìˆ˜\n",
        "        print(\"ê°ì •ë³„ ìœ ì‚¬ë„:\")\n",
        "        for emotion, score in sorted(similarities.items(), key=lambda x: x[1], reverse=True):\n",
        "            print(f\"  {emotion}: {score:.4f}\")\n",
        "\n",
        "# ì˜ˆì œ ì‹¤í–‰\n",
        "analyze_korean_emotions()"
      ],
      "metadata": {
        "id": "E0PLA4jyI0G0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **í•œêµ­ì–´ ë¬¸ì„œ í´ëŸ¬ìŠ¤í„°ë§**"
      ],
      "metadata": {
        "id": "UmQFdr_hI3_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cluster_korean_documents():\n",
        "    \"\"\"í•œêµ­ì–´ ë¬¸ì„œ í´ëŸ¬ìŠ¤í„°ë§\"\"\"\n",
        "    print(\"=== í•œêµ­ì–´ ë¬¸ì„œ í´ëŸ¬ìŠ¤í„°ë§ ===\")\n",
        "\n",
        "    from sklearn.cluster import KMeans\n",
        "    import matplotlib.pyplot as plt\n",
        "    from sklearn.decomposition import PCA\n",
        "\n",
        "    system = KoreanEmbeddingSystem('jhgan/ko-sroberta-multitask')\n",
        "\n",
        "    # ë‹¤ì–‘í•œ ì£¼ì œì˜ í•œêµ­ì–´ ë¬¸ì„œ\n",
        "    documents = [\n",
        "        # ê¸°ìˆ  ê´€ë ¨\n",
        "        \"ì¸ê³µì§€ëŠ¥ê³¼ ë¨¸ì‹ ëŸ¬ë‹ ê¸°ìˆ ì´ ë¹ ë¥´ê²Œ ë°œì „í•˜ê³  ìˆìŠµë‹ˆë‹¤.\",\n",
        "        \"ë”¥ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì€ ë³µì¡í•œ íŒ¨í„´ì„ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\",\n",
        "        \"ìì—°ì–´ì²˜ë¦¬ ê¸°ìˆ ë¡œ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\",\n",
        "\n",
        "        # ìš”ë¦¬ ê´€ë ¨\n",
        "        \"ê¹€ì¹˜ì°Œê°œëŠ” í•œêµ­ì˜ ëŒ€í‘œì ì¸ ìŒì‹ì…ë‹ˆë‹¤.\",\n",
        "        \"ë¶ˆê³ ê¸°ë¥¼ ë§Œë“¤ ë•ŒëŠ” ì–‘ë…ì´ ì¤‘ìš”í•´ìš”.\",\n",
        "        \"ë¹„ë¹”ë°¥ì—ëŠ” ë‹¤ì–‘í•œ ë‚˜ë¬¼ì´ ë“¤ì–´ê°‘ë‹ˆë‹¤.\",\n",
        "\n",
        "        # ì—¬í–‰ ê´€ë ¨\n",
        "        \"ì œì£¼ë„ëŠ” ì•„ë¦„ë‹¤ìš´ ìì—°ê²½ê´€ì„ ìë‘í•©ë‹ˆë‹¤.\",\n",
        "        \"ë¶€ì‚°ì˜ í•´ìš´ëŒ€ ë°”ë‹¤ëŠ” ì •ë§ ë©‹ì ¸ìš”.\",\n",
        "        \"ê²½ì£¼ì—ëŠ” ë§ì€ ì—­ì‚¬ì  ìœ ì ì´ ìˆìŠµë‹ˆë‹¤.\",\n",
        "\n",
        "        # ë‚ ì”¨ ê´€ë ¨\n",
        "        \"ì˜¤ëŠ˜ì€ ë§‘ê³  í™”ì°½í•œ ë‚ ì”¨ì…ë‹ˆë‹¤.\",\n",
        "        \"ë¹„ê°€ ì™€ì„œ ìŠµë„ê°€ ë†’ì•„ì¡Œì–´ìš”.\",\n",
        "        \"ê²¨ìš¸ì´ë¼ ë‚ ì”¨ê°€ ë§¤ìš° ì¶¥ìŠµë‹ˆë‹¤.\"\n",
        "    ]\n",
        "\n",
        "    # ë¬¸ì„œ ì„ë² ë”© ìƒì„±\n",
        "    embeddings = system.encode_sentences(documents)\n",
        "\n",
        "    # K-means í´ëŸ¬ìŠ¤í„°ë§\n",
        "    n_clusters = 4\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "    cluster_labels = kmeans.fit_predict(embeddings)\n",
        "\n",
        "    # ê²°ê³¼ ì¶œë ¥\n",
        "    print(f\"{n_clusters}ê°œ í´ëŸ¬ìŠ¤í„°ë¡œ ë¶„ë¥˜ ê²°ê³¼:\")\n",
        "    for i in range(n_clusters):\n",
        "        print(f\"\\ní´ëŸ¬ìŠ¤í„° {i+1}:\")\n",
        "        cluster_docs = [documents[j] for j, label in enumerate(cluster_labels) if label == i]\n",
        "        for doc in cluster_docs:\n",
        "            print(f\"  - {doc}\")\n",
        "\n",
        "    # ì‹œê°í™” (PCAë¡œ 2D ì¶•ì†Œ)\n",
        "    try:\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        plt.rcParams['font.family'] = 'NanumBarunGothic'  # í•œê¸€ í°íŠ¸\n",
        "\n",
        "        pca = PCA(n_components=2)\n",
        "        embeddings_2d = pca.fit_transform(embeddings)\n",
        "\n",
        "        colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
        "        for i in range(n_clusters):\n",
        "            mask = cluster_labels == i\n",
        "            plt.scatter(embeddings_2d[mask, 0], embeddings_2d[mask, 1],\n",
        "                       c=colors[i], label=f'í´ëŸ¬ìŠ¤í„° {i+1}', alpha=0.7)\n",
        "\n",
        "        plt.title('í•œêµ­ì–´ ë¬¸ì„œ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼')\n",
        "        plt.xlabel(f'PC1 (ì„¤ëª…ë³€ëŸ‰: {pca.explained_variance_ratio_[0]:.2%})')\n",
        "        plt.ylabel(f'PC2 (ì„¤ëª…ë³€ëŸ‰: {pca.explained_variance_ratio_[1]:.2%})')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.show()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ì‹œê°í™” ì˜¤ë¥˜: {e}\")\n",
        "        print(\"matplotlib í•œê¸€ í°íŠ¸ ì„¤ì •ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "# ì˜ˆì œ ì‹¤í–‰\n",
        "cluster_korean_documents()"
      ],
      "metadata": {
        "id": "l5PMlTCoI6B0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ì˜ˆì œ: ë©€í‹°ëª¨ë‹¬ ì„ë² ë”©**"
      ],
      "metadata": {
        "id": "t8w-MnTSJ8h2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import (\n",
        "    CLIPProcessor, CLIPModel,\n",
        "    AutoProcessor, AutoModel,\n",
        "    BlipProcessor, BlipModel,\n",
        "    AutoTokenizer\n",
        ")\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class MultimodalEmbeddingSystem:\n",
        "    def __init__(self, model_type='clip'):\n",
        "        \"\"\"\n",
        "        ë©€í‹°ëª¨ë‹¬ ì„ë² ë”© ì‹œìŠ¤í…œ ì´ˆê¸°í™”\n",
        "\n",
        "        ì§€ì› ëª¨ë¸ë“¤:\n",
        "        - 'clip': OpenAI CLIP (ê¸°ë³¸)\n",
        "        - 'korean-clip': í•œêµ­ì–´ CLIP\n",
        "        - 'siglip': Google SigLIP\n",
        "        - 'blip': BLIP (Salesforce)\n",
        "        \"\"\"\n",
        "        self.model_type = model_type\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        print(f\"ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ ë¡œë”© ì¤‘: {model_type}\")\n",
        "        self._load_model()\n",
        "        print(f\"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ! ë””ë°”ì´ìŠ¤: {self.device}\")\n",
        "\n",
        "    def _load_model(self):\n",
        "        \"\"\"ëª¨ë¸ë³„ ë¡œë”©\"\"\"\n",
        "        if self.model_type == 'clip':\n",
        "            model_name = \"openai/clip-vit-base-patch32\"\n",
        "            self.processor = CLIPProcessor.from_pretrained(model_name)\n",
        "            self.model = CLIPModel.from_pretrained(model_name)\n",
        "\n",
        "        elif self.model_type == 'korean-clip':\n",
        "            # í•œêµ­ì–´ CLIP ëª¨ë¸ (ì˜ˆì‹œ)\n",
        "            model_name = \"Bingsu/clip-vit-base-patch32-ko\"\n",
        "            self.processor = CLIPProcessor.from_pretrained(model_name)\n",
        "            self.model = CLIPModel.from_pretrained(model_name)\n",
        "\n",
        "        elif self.model_type == 'siglip':\n",
        "            model_name = \"google/siglip-base-patch16-224\"\n",
        "            self.processor = AutoProcessor.from_pretrained(model_name)\n",
        "            self.model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "        elif self.model_type == 'blip':\n",
        "            model_name = \"Salesforce/blip-image-captioning-base\"\n",
        "            self.processor = BlipProcessor.from_pretrained(model_name)\n",
        "            self.model = BlipModel.from_pretrained(model_name)\n",
        "\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "\n",
        "    def encode_text(self, texts):\n",
        "        \"\"\"í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ì¸ì½”ë”©\"\"\"\n",
        "        if isinstance(texts, str):\n",
        "            texts = [texts]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if self.model_type in ['clip', 'korean-clip']:\n",
        "                inputs = self.processor(text=texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "                text_features = self.model.get_text_features(**inputs)\n",
        "                text_features = F.normalize(text_features, p=2, dim=1)\n",
        "\n",
        "            elif self.model_type == 'siglip':\n",
        "                inputs = self.processor(text=texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "                outputs = self.model.get_text_features(**inputs)\n",
        "                text_features = F.normalize(outputs, p=2, dim=1)\n",
        "\n",
        "            elif self.model_type == 'blip':\n",
        "                inputs = self.processor(text=texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "                text_features = self.model.get_text_features(**inputs)\n",
        "                text_features = F.normalize(text_features, p=2, dim=1)\n",
        "\n",
        "        return text_features.cpu().numpy()\n",
        "\n",
        "    def encode_image(self, images):\n",
        "        \"\"\"ì´ë¯¸ì§€ë¥¼ ë²¡í„°ë¡œ ì¸ì½”ë”©\"\"\"\n",
        "        if not isinstance(images, list):\n",
        "            images = [images]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if self.model_type in ['clip', 'korean-clip']:\n",
        "                inputs = self.processor(images=images, return_tensors=\"pt\", padding=True)\n",
        "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "                image_features = self.model.get_image_features(**inputs)\n",
        "                image_features = F.normalize(image_features, p=2, dim=1)\n",
        "\n",
        "            elif self.model_type == 'siglip':\n",
        "                inputs = self.processor(images=images, return_tensors=\"pt\", padding=True)\n",
        "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "                outputs = self.model.get_image_features(**inputs)\n",
        "                image_features = F.normalize(outputs, p=2, dim=1)\n",
        "\n",
        "            elif self.model_type == 'blip':\n",
        "                inputs = self.processor(images=images, return_tensors=\"pt\", padding=True)\n",
        "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "                image_features = self.model.get_image_features(**inputs)\n",
        "                image_features = F.normalize(image_features, p=2, dim=1)\n",
        "\n",
        "        return image_features.cpu().numpy()\n",
        "\n",
        "    def calculate_similarity(self, embeddings1, embeddings2=None):\n",
        "        \"\"\"ì„ë² ë”© ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°\"\"\"\n",
        "        if embeddings2 is None:\n",
        "            return cosine_similarity(embeddings1)\n",
        "        else:\n",
        "            return cosine_similarity(embeddings1, embeddings2)\n",
        "\n",
        "    def text_image_similarity(self, texts, images):\n",
        "        \"\"\"í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ê°„ ìœ ì‚¬ë„ ê³„ì‚°\"\"\"\n",
        "        text_embeddings = self.encode_text(texts)\n",
        "        image_embeddings = self.encode_image(images)\n",
        "\n",
        "        similarity_matrix = self.calculate_similarity(text_embeddings, image_embeddings)\n",
        "        return similarity_matrix\n",
        "\n",
        "    def cross_modal_search(self, query, candidates, query_type='text', candidate_type='image', top_k=5):\n",
        "        \"\"\"êµì°¨ ëª¨ë‹¬ ê²€ìƒ‰\"\"\"\n",
        "        if query_type == 'text':\n",
        "            query_embedding = self.encode_text([query])\n",
        "        else:\n",
        "            query_embedding = self.encode_image([query])\n",
        "\n",
        "        if candidate_type == 'text':\n",
        "            candidate_embeddings = self.encode_text(candidates)\n",
        "        else:\n",
        "            candidate_embeddings = self.encode_image(candidates)\n",
        "\n",
        "        similarities = cosine_similarity(query_embedding, candidate_embeddings)[0]\n",
        "        top_indices = similarities.argsort()[-top_k:][::-1]\n",
        "\n",
        "        results = []\n",
        "        for i, idx in enumerate(top_indices):\n",
        "            results.append({\n",
        "                'rank': i + 1,\n",
        "                'candidate': candidates[idx] if candidate_type == 'text' else f\"Image_{idx}\",\n",
        "                'similarity': similarities[idx]\n",
        "            })\n",
        "\n",
        "        return results\n",
        "\n",
        "# ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ìœ í‹¸ë¦¬í‹°\n",
        "def download_image(url):\n",
        "    \"\"\"URLì—ì„œ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        image = Image.open(BytesIO(response.content)).convert('RGB')\n",
        "        return image\n",
        "    except Exception as e:\n",
        "        print(f\"ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
        "        return None\n",
        "\n",
        "def create_sample_images():\n",
        "    \"\"\"ìƒ˜í”Œ ì´ë¯¸ì§€ URL ëª©ë¡\"\"\"\n",
        "    return {\n",
        "        'cat': 'https://images.unsplash.com/photo-1514888286974-6c03e2ca1dba?w=400',\n",
        "        'dog': 'https://images.unsplash.com/photo-1552053831-71594a27632d?w=400',\n",
        "        'car': 'https://images.unsplash.com/photo-1549924231-f129b911e442?w=400',\n",
        "        'flower': 'https://images.unsplash.com/photo-1441974231531-c6227db76b6e?w=400',\n",
        "        'mountain': 'https://images.unsplash.com/photo-1506905925346-21bda4d32df4?w=400'\n",
        "    }\n",
        "\n",
        "# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜\n",
        "def main():\n",
        "    print(\"=== ë©€í‹°ëª¨ë‹¬ í†µí•© ì„ë² ë”© ì‹œìŠ¤í…œ ===\\n\")\n",
        "\n",
        "    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”\n",
        "    embedding_system = MultimodalEmbeddingSystem('clip')\n",
        "\n",
        "    # ìƒ˜í”Œ í…ìŠ¤íŠ¸\n",
        "    sample_texts = [\n",
        "        \"a cute cat sitting on a chair\",\n",
        "        \"a happy dog running in the park\",\n",
        "        \"a red sports car on the road\",\n",
        "        \"beautiful flowers in the garden\",\n",
        "        \"snow-covered mountain peaks\"\n",
        "    ]\n",
        "\n",
        "    print(\"ìƒ˜í”Œ í…ìŠ¤íŠ¸:\")\n",
        "    for i, text in enumerate(sample_texts, 1):\n",
        "        print(f\"{i}. {text}\")\n",
        "\n",
        "    # ìƒ˜í”Œ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ\n",
        "    print(\"\\nìƒ˜í”Œ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì¤‘...\")\n",
        "    image_urls = create_sample_images()\n",
        "    sample_images = []\n",
        "    image_names = []\n",
        "\n",
        "    for name, url in image_urls.items():\n",
        "        image = download_image(url)\n",
        "        if image:\n",
        "            sample_images.append(image)\n",
        "            image_names.append(name)\n",
        "            print(f\"âœ“ {name} ì´ë¯¸ì§€ ë¡œë“œ ì™„ë£Œ\")\n",
        "\n",
        "            plt.imshow(image)\n",
        "            plt.axis('off')\n",
        "            plt.show()\n",
        "\n",
        "    if not sample_images:\n",
        "        print(\"ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨. ë¡œì»¬ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ URLì„ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n",
        "        return\n",
        "\n",
        "    # 1. í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±\n",
        "    print(f\"\\n=== í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± ===\")\n",
        "    text_embeddings = embedding_system.encode_text(sample_texts)\n",
        "    print(f\"í…ìŠ¤íŠ¸ ì„ë² ë”© shape: {text_embeddings.shape}\")\n",
        "\n",
        "    # 2. ì´ë¯¸ì§€ ì„ë² ë”© ìƒì„±\n",
        "    print(f\"\\n=== ì´ë¯¸ì§€ ì„ë² ë”© ìƒì„± ===\")\n",
        "    image_embeddings = embedding_system.encode_image(sample_images)\n",
        "    print(f\"ì´ë¯¸ì§€ ì„ë² ë”© shape: {image_embeddings.shape}\")\n",
        "\n",
        "    # 3. êµì°¨ ëª¨ë‹¬ ìœ ì‚¬ë„ ê³„ì‚°\n",
        "    print(f\"\\n=== í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ êµì°¨ ëª¨ë‹¬ ìœ ì‚¬ë„ ===\")\n",
        "    cross_modal_similarity = embedding_system.text_image_similarity(sample_texts, sample_images)\n",
        "\n",
        "    print(\"í…ìŠ¤íŠ¸ â†’ ì´ë¯¸ì§€ ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤:\")\n",
        "    print(\"í…ìŠ¤íŠ¸ \\\\ ì´ë¯¸ì§€\", end=\"\")\n",
        "    for name in image_names:\n",
        "        print(f\"\\t{name[:8]}\", end=\"\")\n",
        "    print()\n",
        "\n",
        "    for i, text in enumerate(sample_texts):\n",
        "        print(f\"{text[:20]}...\", end=\"\")\n",
        "        for j in range(len(sample_images)):\n",
        "            print(f\"\\t{cross_modal_similarity[i][j]:.3f}\", end=\"\")\n",
        "        print()\n",
        "\n",
        "    # 4. í…ìŠ¤íŠ¸ë¡œ ì´ë¯¸ì§€ ê²€ìƒ‰\n",
        "    print(f\"\\n=== í…ìŠ¤íŠ¸ ì¿¼ë¦¬ë¡œ ì´ë¯¸ì§€ ê²€ìƒ‰ ===\")\n",
        "    text_query = \"cute animal pet\"\n",
        "\n",
        "    results = embedding_system.cross_modal_search(\n",
        "        query=text_query,\n",
        "        candidates=image_names,  # ì‹¤ì œë¡œëŠ” ì´ë¯¸ì§€ ê°ì²´ë¥¼ ì‚¬ìš©\n",
        "        query_type='text',\n",
        "        candidate_type='text',  # ê²°ê³¼ í‘œì‹œìš©ìœ¼ë¡œ ì´ë¦„ ì‚¬ìš©\n",
        "        top_k=3\n",
        "    )\n",
        "\n",
        "    print(f\"ê²€ìƒ‰ì–´: '{text_query}'\")\n",
        "    for result in results:\n",
        "        print(f\"{result['rank']}. {result['candidate']} (ìœ ì‚¬ë„: {result['similarity']:.4f})\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "lMJblWX3J_2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **ì‚¬ì§„ì— ë§ëŠ” í…ìŠ¤íŠ¸ ì„ íƒí•˜ê¸°**\n",
        "\n",
        "- image : ê°•ì•„ì§€ ì‚¬ì§„\n",
        "- texts = [\"a photo of a cat\", \"a photo of a dog\", \"a photo of a car\"]"
      ],
      "metadata": {
        "id": "-x7CBJU3MS-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from PIL import Image\n",
        "import requests\n",
        "import torch\n",
        "\n",
        "# 1. ëª¨ë¸ ë¡œë“œ\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "# 2. ìƒ˜í”Œ ë°ì´í„°\n",
        "url = \"https://images.unsplash.com/photo-1552053831-71594a27632d?w=400\"  # ê°•ì•„ì§€ ì‚¬ì§„\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "texts = [\"a photo of a cat\", \"a photo of a dog\", \"a photo of a car\"]\n",
        "\n",
        "# 3. ì „ì²˜ë¦¬\n",
        "inputs = processor(text=texts, images=image, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "# 4. ì˜ˆì¸¡\n",
        "outputs = model(**inputs)\n",
        "logits_per_image = outputs.logits_per_image   # ì´ë¯¸ì§€ vs í…ìŠ¤íŠ¸ ìœ ì‚¬ë„\n",
        "probs = logits_per_image.softmax(dim=1)\n",
        "\n",
        "print(\"í…ìŠ¤íŠ¸ í›„ë³´:\", texts)\n",
        "print(\"ì˜ˆì¸¡ í™•ë¥ :\", probs.detach().numpy())\n"
      ],
      "metadata": {
        "id": "KnDr3HjMMTT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from PIL import Image\n",
        "import requests\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class CLIPImageTextMatcher:\n",
        "    def __init__(self):\n",
        "        \"\"\"CLIP ëª¨ë¸ ì´ˆê¸°í™”\"\"\"\n",
        "        print(\"CLIP ëª¨ë¸ ë¡œë”© ì¤‘...\")\n",
        "        self.model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.model.to(self.device)\n",
        "        print(f\"ëª¨ë¸ ë¡œë”© ì™„ë£Œ! ë””ë°”ì´ìŠ¤: {self.device}\")\n",
        "\n",
        "    def predict_image_text_match(self, image, texts, show_all_probs=True):\n",
        "        \"\"\"ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ ê°„ ë§¤ì¹­ ì˜ˆì¸¡\"\"\"\n",
        "        # ì „ì²˜ë¦¬\n",
        "        inputs = self.processor(text=texts, images=image, return_tensors=\"pt\", padding=True)\n",
        "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "\n",
        "        # ì˜ˆì¸¡ ìˆ˜í–‰\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "            logits_per_image = outputs.logits_per_image\n",
        "            probs = logits_per_image.softmax(dim=1)\n",
        "\n",
        "        # CPUë¡œ ì´ë™ í›„ numpy ë³€í™˜\n",
        "        probs_np = probs.cpu().numpy()[0]\n",
        "\n",
        "        # ê²°ê³¼ ë¶„ì„\n",
        "        max_prob_idx = np.argmax(probs_np)\n",
        "        max_prob_text = texts[max_prob_idx]\n",
        "        max_prob_value = probs_np[max_prob_idx]\n",
        "\n",
        "        print(\"=\" * 60)\n",
        "        print(\"ğŸ¯ CLIP ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ë§¤ì¹­ ê²°ê³¼\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        if show_all_probs:\n",
        "            print(\"\\nğŸ“Š ëª¨ë“  í…ìŠ¤íŠ¸ í›„ë³´ë³„ ì˜ˆì¸¡ í™•ë¥ :\")\n",
        "            for i, (text, prob) in enumerate(zip(texts, probs_np)):\n",
        "                status = \"âœ… ìµœê³  í™•ë¥ \" if i == max_prob_idx else \"  \"\n",
        "                print(f\"{status} {i+1}. '{text}': {prob:.4f} ({prob*100:.2f}%)\")\n",
        "\n",
        "        print(f\"\\nğŸ† ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ ë§¤ì¹­:\")\n",
        "        print(f\"   í…ìŠ¤íŠ¸: '{max_prob_text}'\")\n",
        "        print(f\"   í™•ë¥ : {max_prob_value:.4f} ({max_prob_value*100:.2f}%)\")\n",
        "        print(f\"   ì‹ ë¢°ë„: {'ë†’ìŒ' if max_prob_value > 0.7 else 'ë³´í†µ' if max_prob_value > 0.4 else 'ë‚®ìŒ'}\")\n",
        "\n",
        "        return {\n",
        "            'best_match': max_prob_text,\n",
        "            'best_probability': max_prob_value,\n",
        "            'all_probabilities': list(zip(texts, probs_np)),\n",
        "            'confidence_level': 'high' if max_prob_value > 0.7 else 'medium' if max_prob_value > 0.4 else 'low'\n",
        "        }\n",
        "\n",
        "def download_and_display_image(url):\n",
        "    \"\"\"ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë° ì •ë³´ í‘œì‹œ\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, stream=True)\n",
        "        image = Image.open(response.raw).convert('RGB')\n",
        "        print(f\"âœ… ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì„±ê³µ\")\n",
        "        print(f\"   URL: {url}\")\n",
        "        print(f\"   í¬ê¸°: {image.size}\")\n",
        "        print(f\"   ëª¨ë“œ: {image.mode}\")\n",
        "        return image\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
        "        return None\n",
        "\n",
        "def main():\n",
        "    \"\"\"ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜\"\"\"\n",
        "    print(\"ğŸš€ CLIP ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ë§¤ì¹­ ì‹œìŠ¤í…œ ì‹œì‘\")\n",
        "\n",
        "    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”\n",
        "    matcher = CLIPImageTextMatcher()\n",
        "\n",
        "    # ìƒ˜í”Œ ì´ë¯¸ì§€ URLë“¤\n",
        "    sample_images = {\n",
        "        'dog': \"https://images.unsplash.com/photo-1552053831-71594a27632d?w=400\",\n",
        "        'cat': \"https://images.unsplash.com/photo-1514888286974-6c03e2ca1dba?w=400\",\n",
        "        'car': \"https://images.unsplash.com/photo-1549924231-f129b911e442?w=400\",\n",
        "        'flower': \"https://images.unsplash.com/photo-1441974231531-c6227db76b6e?w=400\"\n",
        "    }\n",
        "\n",
        "    # í…ìŠ¤íŠ¸ í›„ë³´ë“¤\n",
        "    text_candidates = [\"a photo of a cat\", \"a photo of a dog\", \"a photo of a car\", \"a photo of a flower\"]\n",
        "\n",
        "    print(f\"\\nğŸ“ í…ìŠ¤íŠ¸ í›„ë³´ë“¤:\")\n",
        "    for i, text in enumerate(text_candidates, 1):\n",
        "        print(f\"   {i}. {text}\")\n",
        "\n",
        "    # ê° ì´ë¯¸ì§€ì— ëŒ€í•´ ì˜ˆì¸¡ ìˆ˜í–‰\n",
        "    results_summary = []\n",
        "\n",
        "    for img_name, img_url in sample_images.items():\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"ğŸ–¼ï¸  {img_name.upper()} ì´ë¯¸ì§€ ë¶„ì„\")\n",
        "        print(f\"{'='*80}\")\n",
        "\n",
        "        # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ\n",
        "        image = download_and_display_image(img_url)\n",
        "\n",
        "        if image is None:\n",
        "            continue\n",
        "\n",
        "        # ì˜ˆì¸¡ ìˆ˜í–‰\n",
        "        result = matcher.predict_image_text_match(image, text_candidates)\n",
        "\n",
        "        # ê²°ê³¼ ì €ì¥\n",
        "        results_summary.append({\n",
        "            'image_name': img_name,\n",
        "            'predicted_text': result['best_match'],\n",
        "            'probability': result['best_probability'],\n",
        "            'confidence': result['confidence_level']\n",
        "        })\n",
        "\n",
        "    # ì „ì²´ ê²°ê³¼ ìš”ì•½\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"ğŸ“‹ ì „ì²´ ê²°ê³¼ ìš”ì•½\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    for result in results_summary:\n",
        "        confidence_emoji = \"ğŸ”¥\" if result['confidence'] == 'high' else \"ğŸ‘\" if result['confidence'] == 'medium' else \"ğŸ¤”\"\n",
        "        print(f\"{confidence_emoji} {result['image_name'].upper():8} â†’ '{result['predicted_text']}' \"\n",
        "              f\"({result['probability']:.4f}, {result['confidence']})\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "mNngrcMaP-qn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **ë‹¤ì–‘í•œ ì–¸ì–´ë¡œ í…ŒìŠ¤íŠ¸**"
      ],
      "metadata": {
        "id": "WYwNMN30RL4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def multilingual_test():\n",
        "    \"\"\"ë‹¤êµ­ì–´ í…ìŠ¤íŠ¸ë¡œ í…ŒìŠ¤íŠ¸\"\"\"\n",
        "    print(\"\\nğŸŒ ë‹¤êµ­ì–´ í…ìŠ¤íŠ¸ ë§¤ì¹­ í…ŒìŠ¤íŠ¸\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    matcher = CLIPImageTextMatcher()\n",
        "\n",
        "    # ê°•ì•„ì§€ ì´ë¯¸ì§€\n",
        "    dog_url = \"https://images.unsplash.com/photo-1552053831-71594a27632d?w=400\"\n",
        "    image = download_and_display_image(dog_url)\n",
        "\n",
        "    if image:\n",
        "        # ë‹¤êµ­ì–´ í…ìŠ¤íŠ¸ í›„ë³´\n",
        "        multilingual_texts = [\n",
        "            \"a photo of a dog\",      # ì˜ì–´\n",
        "            \"un chien\",              # í”„ë‘ìŠ¤ì–´\n",
        "            \"ein Hund\",              # ë…ì¼ì–´\n",
        "            \"un perro\",              # ìŠ¤í˜ì¸ì–´\n",
        "            \"ê°œ ì‚¬ì§„\",                # í•œêµ­ì–´ (ì œí•œì  ì§€ì›)\n",
        "            \"a cat sleeping\",         # ë‹¤ë¥¸ ë™ë¬¼\n",
        "            \"a beautiful landscape\"   # ì™„ì „íˆ ë‹¤ë¥¸ ê°œë…\n",
        "        ]\n",
        "\n",
        "        result = matcher.predict_image_text_match(image, multilingual_texts)\n",
        "\n",
        "        print(f\"\\nğŸ’¡ ë¶„ì„: CLIPì€ ì£¼ë¡œ ì˜ì–´ ë°ì´í„°ë¡œ í•™ìŠµë˜ì–´ ì˜ì–´ í…ìŠ¤íŠ¸ì—ì„œ ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.\")\n",
        "\n",
        "multilingual_test()"
      ],
      "metadata": {
        "id": "Q7XJ3-A0RPvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "7turu6ayn1B5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. í…ìŠ¤íŠ¸ ì¸ì½”ë”©ê³¼ ë””ì½”ë”©ì„ ì´ìš©í•œ ê¸°ê³„ë²ˆì—­**"
      ],
      "metadata": {
        "id": "tk0AETJAnjPu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ì˜ˆì œ : í…ìŠ¤íŠ¸ ì¸ì½”ë”©ê³¼ ë””ì½”ë”©ì„ ì´ìš©í•œ ê¸°ê³„ë²ˆì—­**\n",
        "- RNN / Seq2Seq / Attention / Transformer"
      ],
      "metadata": {
        "id": "xG2OZPWun9-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers sentencepiece"
      ],
      "metadata": {
        "id": "MBD3Dqvpn8di"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import time\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "# ------------------------------\n",
        "# Utilities\n",
        "# ------------------------------\n",
        "SRC_SENT = \"ë‚˜ëŠ” ì ì‹¬ ì‹ì‚¬ë¡œ íŒŒìŠ¤íƒ€ë¥¼ ë¨¹ì„ ì˜ˆì •ì…ë‹ˆë‹¤.\"\n",
        "EXPECTED = \"I am going to eat pasta for lunch.\"\n",
        "\n",
        "\n",
        "def pretty(s: str) -> str:\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    s = s[0:1].upper() + s[1:] if s else s\n",
        "    if not s.endswith(\".\"):\n",
        "        s += \".\"\n",
        "    return s\n",
        "\n",
        "\n",
        "def timer(func, *args, **kwargs):\n",
        "    t0 = time.perf_counter()\n",
        "    out = func(*args, **kwargs)\n",
        "    t1 = time.perf_counter()\n",
        "    return out, (t1 - t0)\n",
        "\n",
        "# ------------------------------\n",
        "# 1) RNN-style (naive L2R)\n",
        "# ------------------------------\n",
        "def tokenize_ko(text: str):\n",
        "    # Basic whitespace + punctuation split, keep simple for demo\n",
        "    text = re.sub(r\"[^\\w\\sê°€-í£]\", \" \", text)\n",
        "    toks = [t for t in text.split() if t.strip()]\n",
        "    return toks\n",
        "\n",
        "# Minimal dictionary for demo (phrase & token level)\n",
        "PHRASES = {\n",
        "    \"ë¨¹ì„ ì˜ˆì •ì…ë‹ˆë‹¤\": \"am going to eat\",\n",
        "    \"ì ì‹¬ ì‹ì‚¬ë¡œ\": \"for lunch\",\n",
        "}\n",
        "\n",
        "LEXICON = {\n",
        "    \"ë‚˜ëŠ”\": \"i\",\n",
        "    \"ì ì‹¬\": \"lunch\",\n",
        "    \"ì‹ì‚¬ë¡œ\": \"for lunch\",\n",
        "    \"íŒŒìŠ¤íƒ€ë¥¼\": \"pasta\",\n",
        "    \"ë¨¹ì„\": \"eat\",\n",
        "    \"ì˜ˆì •ì…ë‹ˆë‹¤\": \"going to\",\n",
        "}\n",
        "\n",
        "def translate_rnn_rule(text: str) -> str:\n",
        "    # Left-to-right mapping without reordering (shows RNN baseline weakness on SOVâ†’SVO)\n",
        "    # Apply phrase map first if exact phrase appears\n",
        "    out = text\n",
        "    for k, v in PHRASES.items():\n",
        "        out = out.replace(k, v)\n",
        "\n",
        "    toks = tokenize_ko(out)\n",
        "    print(f'toks : {toks}')\n",
        "\n",
        "    eng = []\n",
        "    for tok in toks:\n",
        "        if tok in LEXICON:\n",
        "            eng.append(LEXICON[tok])\n",
        "        elif tok in PHRASES:\n",
        "            eng.append(PHRASES[tok])\n",
        "        elif tok == \"íŒŒìŠ¤íƒ€\":\n",
        "            eng.append(\"pasta\")\n",
        "        else:\n",
        "            # leave as-is (unknown token)\n",
        "            eng.append(tok)\n",
        "\n",
        "    # naive L2R join\n",
        "    return pretty(\" \".join(eng))\n",
        "\n",
        "# ------------------------------\n",
        "# 2) Seq2Seq-style (encoder-decoder with reordering rules)\n",
        "# ------------------------------\n",
        "def translate_seq2seq_rule(text: str) -> str:\n",
        "    # Simulate encoder-decoder that learns typical Korean SOV â†’ English SVO reordering\n",
        "    # Pattern: \"ë‚˜ëŠ” Xë¥¼ ë¨¹ì„ ì˜ˆì •ì…ë‹ˆë‹¤\" -> \"i am going to eat X\"\n",
        "    # and attach \"for lunch\" if phrase occurs\n",
        "    s = text\n",
        "\n",
        "    # detect object phrase like \"íŒŒìŠ¤íƒ€ë¥¼\"\n",
        "    obj = None\n",
        "    m = re.search(r\"(.*?)(íŒŒìŠ¤íƒ€)ë¥¼\", s)\n",
        "    if m:\n",
        "        obj = \"pasta\"\n",
        "\n",
        "    # lunch phrase\n",
        "    lunch = \"for lunch\" if \"ì ì‹¬\" in s else None\n",
        "\n",
        "    base = \"i am going to eat\"\n",
        "    if obj:\n",
        "        out = f\"{base} {obj}\"\n",
        "    else:\n",
        "        out = base\n",
        "\n",
        "    if lunch:\n",
        "        out = f\"{out} {lunch}\"\n",
        "    return pretty(out)\n",
        "\n",
        "# ------------------------------\n",
        "# 3) Attention-style (alignment-guided + mock attention matrix)\n",
        "# ------------------------------\n",
        "def translate_attention_rule(text: str, return_attention=False):\n",
        "    # Tokenize Korean source\n",
        "    src_tokens = [\"ë‚˜ëŠ”\", \"ì ì‹¬\", \"ì‹ì‚¬ë¡œ\", \"íŒŒìŠ¤íƒ€ë¥¼\", \"ë¨¹ì„\", \"ì˜ˆì •ì…ë‹ˆë‹¤\"]\n",
        "    # Target template tokens (SVO + adjunct)\n",
        "    tgt_tokens = [\"i\", \"am\", \"going\", \"to\", \"eat\", \"pasta\", \"for\", \"lunch\"]\n",
        "\n",
        "    # Create a mock attention alignment matrix (len(tgt) x len(src))\n",
        "    A = np.zeros((len(tgt_tokens), len(src_tokens)), dtype=float)\n",
        "\n",
        "    # Rough alignments\n",
        "    align = {\n",
        "        0: [0],              # i  <- ë‚˜ëŠ”\n",
        "        1: [5],              # am <- ì˜ˆì •ì…ë‹ˆë‹¤\n",
        "        2: [5],              # going\n",
        "        3: [5],              # to\n",
        "        4: [4],              # eat <- ë¨¹ì„\n",
        "        5: [3],              # pasta <- íŒŒìŠ¤íƒ€ë¥¼\n",
        "        6: [1,2],            # for <- ì ì‹¬ ì‹ì‚¬ë¡œ\n",
        "        7: [1,2],            # lunch <- ì ì‹¬ ì‹ì‚¬ë¡œ\n",
        "    }\n",
        "    for t_idx, s_list in align.items():\n",
        "        for s_idx in s_list:\n",
        "            A[t_idx, s_idx] = 1.0 / len(s_list)\n",
        "\n",
        "    out = \"i am going to eat pasta for lunch\"\n",
        "    out = pretty(out)\n",
        "\n",
        "    if return_attention:\n",
        "        return out, A, src_tokens, tgt_tokens\n",
        "    return out\n",
        "\n",
        "# ------------------------------\n",
        "# 4) Transformer (real pretrained model if available)\n",
        "# ------------------------------\n",
        "def translate_transformer_hf(text: str) -> str:\n",
        "    try:\n",
        "        from transformers import MarianTokenizer, MarianMTModel\n",
        "        import torch\n",
        "\n",
        "        model_name = \"Helsinki-NLP/opus-mt-ko-en\"\n",
        "        tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "        model = MarianMTModel.from_pretrained(model_name)\n",
        "\n",
        "        batch = tokenizer([text], return_tensors=\"pt\", padding=True)\n",
        "        with torch.no_grad():\n",
        "            gen = model.generate(**batch, max_length=64, num_beams=5)\n",
        "        out = tokenizer.batch_decode(gen, skip_special_tokens=True)[0]\n",
        "        return pretty(out)\n",
        "    except Exception as e:\n",
        "        # Fallback: use the seq2seq rule-based output\n",
        "        return translate_seq2seq_rule(text)\n",
        "\n",
        "# ------------------------------\n",
        "# Simple scorer: token precision vs expected\n",
        "# ------------------------------\n",
        "def token_precision(pred: str, ref: str) -> float:\n",
        "    p = re.findall(r\"[a-zA-Z']+\", pred.lower())\n",
        "    r = re.findall(r\"[a-zA-Z']+\", ref.lower())\n",
        "    if not p or not r:\n",
        "        return 0.0\n",
        "    hit = sum(1 for w in p if w in r)\n",
        "    return hit / len(p)\n",
        "\n",
        "# ------------------------------\n",
        "# Main comparison\n",
        "# ------------------------------\n",
        "def main():\n",
        "    print(\"Source (KO):\", SRC_SENT)\n",
        "    print(\"Expected (EN):\", EXPECTED)\n",
        "    print(\"=\"*72)\n",
        "\n",
        "    # 1) RNN (rule)\n",
        "    rnn_out, t_rnn = timer(translate_rnn_rule, SRC_SENT)\n",
        "    # 2) Seq2Seq (rule)\n",
        "    s2s_out, t_s2s = timer(translate_seq2seq_rule, SRC_SENT)\n",
        "    # 3) Attention (rule + mock attention)\n",
        "    att_out, t_att = timer(translate_attention_rule, SRC_SENT)\n",
        "    # 4) Transformer (real model if available)\n",
        "    trf_out, t_trf = timer(translate_transformer_hf, SRC_SENT)\n",
        "\n",
        "\n",
        "    rows = [\n",
        "        (\"RNN (rule)\", rnn_out, t_rnn, token_precision(rnn_out, EXPECTED)),\n",
        "        (\"Seq2Seq (rule)\", s2s_out, t_s2s, token_precision(s2s_out, EXPECTED)),\n",
        "        (\"Attention (rule)\", att_out, t_att, token_precision(att_out, EXPECTED)),\n",
        "        (\"Transformer (HF)\", trf_out, t_trf, token_precision(trf_out, EXPECTED)),\n",
        "    ]\n",
        "\n",
        "    print()\n",
        "    print(f\"{'Model':<18} | {'Output':<45} | Time(s) | TokenPrec\")\n",
        "    print(\"-\"*90)\n",
        "    for name, out, t, prec in rows:\n",
        "        print(f\"{name:<18} | {out:<45} | {t:>6.3f} | {prec:>0.3f}\")\n",
        "\n",
        "    # Optional: visualize mock attention with matplotlib if available\n",
        "    try:\n",
        "        import matplotlib.pyplot as plt\n",
        "        att_out2, A, src, tgt = translate_attention_rule(SRC_SENT, return_attention=True)\n",
        "        plt.figure(figsize=(6,4))\n",
        "        plt.imshow(A, aspect='auto')\n",
        "        plt.yticks(range(len(tgt)), tgt)\n",
        "        plt.xticks(range(len(src)), src, rotation=45)\n",
        "        plt.title(\"Mock Attention Alignment (tgt x src)\")\n",
        "        plt.colorbar()\n",
        "        # plt.tight_layout()\n",
        "        plt.show()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "id": "9bWAaMQqn9Wh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ujvMa2MFd2L1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}