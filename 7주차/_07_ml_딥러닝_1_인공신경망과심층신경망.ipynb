{"cells":[{"cell_type":"markdown","source":["# **딥러닝(Deep Learning)**"],"metadata":{"id":"8ZEk_hwZ7qz5"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"IkE05_Pd7plu"}},{"cell_type":"markdown","source":["# **인공신경망과 심층신경망**"],"metadata":{"id":"QqvsHNjlRvp7"}},{"cell_type":"markdown","metadata":{"id":"gxUnmrjBhNip"},"source":["## **1.인공신경망 : 퍼셉트론(Perceptron)**\n","- 참고\n","    - http://neuralnetworksanddeeplearning.com/chap1.html\n","    - http://neuralnetworksanddeeplearning.com/chap2.html"]},{"cell_type":"markdown","metadata":{"id":"Pvqh8sb7_w7E"},"source":["- **퍼셉트론(Perceptron)**\n","    - 1957년 프랑크 로젠블랫(Frank Rosenblatt)에 의해 고안된 초**기 형태의 인공 신경망**\n","    - **다수의 입력을 받아 하나의 출력을 내보내는 선형 분류기**\n","    - 각 입력에는 가중치가 곱해지고, 이들의 합이 특정 임계값을 넘으면 1을, 그렇지 않으면 0(또는 -1)을 출력\n","    - 단층 퍼셉트론은 선형적으로 분리 가능한 문제(예: AND, OR 게이트)만 해결할 수 있고, 비선형 문제(예: XOR 게이트)는 해결하지 못하는 한계가 있음"]},{"cell_type":"markdown","metadata":{"id":"suROSN53AHw9"},"source":["- **활성화 함수 (Activation Function)**\n","    - 뉴런의 최종 출력값을 결정하는 함수로, 입력 신호의 총합을 받아 다음 층으로 전달할지 여부와 그 강도를 조절\n","    - 비선형 활성화 함수를 사용해야 신경망이 **비선형적인 패턴을 학습**할 수 있음\n","\n"]},{"cell_type":"markdown","source":["- 퍼셉트론 수식 요약\n","\n","| 항목           | 수식                                                                 |\n","|----------------|----------------------------------------------------------------------|\n","| **가중합**     | $  z = \\mathbf{w} \\cdot \\mathbf{x} + b$\n","| **출력 함수**  | $( \\hat{y} = f(z) )$, 여기서 f는 step function\n","| **가중치 업데이트** | \\( w_i := w_i + $\\eta$ (y - $\\hat{y}$) x_i )                          |\n","| **바이어스 업데이트** | \\( b := b + $\\eta$ (y - $\\hat{y}$) \\)   "],"metadata":{"id":"n0WjNUu_CDTs"}},{"cell_type":"markdown","source":["### **1) 퍼셉트론 기반 논리 게이트**"],"metadata":{"id":"EtXm-71S8-aj"}},{"cell_type":"code","source":["import numpy as np\n","\n","# 퍼셉트론 구현 함수\n","def perceptron(x1, x2, w1, w2, bias):\n","    threshold = 0.0\n","    tmp = x1 * w1 + x2 * w2 + bias\n","    return 1 if tmp > threshold else 0   # 임계치 값 설정\n","\n","# 논리 게이트 정의\n","def AND(x1, x2):\n","    return perceptron(x1, x2, w1=0.5, w2=0.5, bias=-0.7)\n","\n","def OR(x1, x2):\n","    return perceptron(x1, x2, w1=0.5, w2=0.5, bias=-0.2)\n","\n","def NAND(x1, x2):\n","    return perceptron(x1, x2, w1=-0.5, w2=-0.5, bias=0.7)\n","\n","def NOR(x1, x2):\n","    return perceptron(x1, x2, w1=-0.5, w2=-0.5, bias=0.2)\n","\n","# 입력 조합\n","X = [(0, 0), (0, 1), (1, 0), (1, 1)]\n","\n","# 출력 확인\n","print(\"x1 x2 | AND OR NAND NOR\")\n","print(\"-------------------------\")\n","for x1, x2 in X:\n","    print(f\" {x1}  {x2} |  {AND(x1,x2)}   {OR(x1,x2)}    {NAND(x1,x2)}    {NOR(x1,x2)}\")\n"],"metadata":{"id":"MEgTJ8hM9HFp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5WIuP_SiBABA"},"source":["### **2) (단층)퍼셉트론** (AND 게이트 예측 : 이진분류)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qYBkGjwtElbc"},"outputs":[],"source":["import numpy as np\n","\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","from sklearn.metrics import accuracy_score\n","\n","# --- 활성화 함수 및 손실 함수 정의 ---\n","\n","def sigmoid(x):\n","    \"\"\"시그모이드 함수\"\"\"\n","    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n","\n","def sigmoid_derivative(x_activated):\n","    \"\"\"시그모이드 함수의 미분 (x_activated는 시그모이드 통과한 값)\"\"\"\n","    return x_activated * (1 - x_activated)\n","\n","def softmax(x):\n","    \"\"\"소프트맥스 함수 (수치적 안정성 고려)\"\"\"\n","    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n","    return e_x / np.sum(e_x, axis=-1, keepdims=True)\n","\n","def mse_loss(y_true, y_pred):\n","    \"\"\"평균 제곱 오차 계산\"\"\"\n","    return np.mean((y_true - y_pred)**2)\n","\n","def cross_entropy_loss(y_true_one_hot, y_pred_softmax):\n","    \"\"\"교차 엔트로피 손실 함수 (다중 클래스)\"\"\"\n","    # log(0) 방지를 위한 클리핑\n","    y_pred_softmax = np.clip(y_pred_softmax, 1e-12, 1. - 1e-12)\n","    num_samples = y_true_one_hot.shape[0]\n","    loss = -np.sum(y_true_one_hot * np.log(y_pred_softmax)) / num_samples\n","    return loss\n","\n","\n","# --- 단층 퍼셉트론(SLP) 클래스 ---\n","class SLP:\n","    def __init__(self, input_size, output_size, seed=None):\n","        if seed is not None: np.random.seed(seed)\n","        self.W = np.random.uniform(-1, 1, (input_size, output_size))\n","        self.b = np.zeros((1, output_size))\n","        self.z, self.a = None, None\n","\n","    def forward(self, X):\n","        self.z = np.dot(X, self.W) + self.b\n","        self.a = sigmoid(self.z) # SLP는 주로 이진 분류에 sigmoid 사용\n","        return self.a\n","\n","    def train(self, X, y_true, epochs, learning_rate, verbose_interval=None):\n","        losses = []\n","        num_samples = X.shape[0]\n","\n","        for epoch in range(epochs):\n","            y_pred = self.forward(X)\n","            loss = mse_loss(y_true, y_pred)\n","            losses.append(loss)\n","\n","            delta_activated = (y_pred - y_true) * sigmoid_derivative(y_pred)\n","            dW = (1/num_samples) * np.dot(X.T, delta_activated)\n","            db = (1/num_samples) * np.sum(delta_activated, axis=0, keepdims=True)\n","            self.W -= learning_rate * dW\n","            self.b -= learning_rate * db\n","\n","            if verbose_interval and (epoch + 1) % verbose_interval == 0:\n","                print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.6f}\")\n","        if verbose_interval and epochs % verbose_interval != 0:\n","             print(f\"Epoch {epochs}/{epochs}, Loss: {losses[-1]:.6f}\")\n","        elif not verbose_interval:\n","            print(f\"Epoch {epochs}/{epochs}, Loss: {losses[-1]:.6f}\")\n","        return losses\n","\n","    def predict(self, X):\n","        return self.forward(X)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OgUuNzzhArS-"},"outputs":[],"source":["# --- 메인 실행 부분 ---\n","if __name__ == '__main__':\n","    # --- 1. 단층 퍼셉트론 (SLP)으로 AND 학습 예제 ---\n","    print(\"--- 단층 퍼셉트론 (SLP)으로 AND 게이트 학습 ---\")\n","    X_and = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n","    y_and = np.array([[0], [0], [0], [1]])\n","    X = X_and\n","    y = y_and\n","\n","    # print(\"--- 단층 퍼셉트론 (SLP)으로 XOR 게이트 학습 ---\")\n","    # X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n","    # y_xor = np.array([[0], [1], [1], [0]])\n","    # X = X_xor\n","    # y = y_xor\n","\n","    # 모델 파라미터\n","    input_dim = X.shape[1]     # 2\n","    output_dim = y.shape[1]    # 1\n","    learning_rate_slp = 0.1\n","    epochs_slp = 1000  # 학습횟수를 늘리거나 줄여본다.\n","\n","    # SLP 모델 생성 및 학습\n","    slp_model = SLP(input_size=input_dim, output_size=output_dim, seed=123)\n","    print(f\"초기 가중치(W):\\n{slp_model.W}\")\n","    print(f\"초기 편향(b): {slp_model.b}\\n\")\n","\n","    print(\"--모델 학습--\")\n","    training_losses_slp = slp_model.train(X, y, epochs_slp, learning_rate_slp, verbose_interval=epochs_slp // 10)\n","    print(f\"\\n학습 후 가중치(W):\\n{slp_model.W}\")\n","    print(f\"학습 후 편향(b): {slp_model.b}\\n\")\n","\n","    # 학습 후 SLP 예측 결과 출력\n","    print(\"\\n학습 후 SLP 예측 결과:\")\n","    predictions_slp = slp_model.predict(X)\n","    for i in range(len(X)):\n","        predicted_class_slp = 1 if predictions_slp[i][0] >= 0.5 else 0\n","        print(f\"Input: {X[i]}, True Output: {y[i][0]}, Predicted Output prob: {predictions_slp[i][0]:.4f} (Class: {predicted_class_slp})\")\n","\n","    print(\"-\" * 50)"]},{"cell_type":"markdown","metadata":{"id":"IQx_64cpBQzw"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"qORcwlVyhc8D"},"source":["## **2.심층신경망 : 다층퍼셉트론(MLP)**"]},{"cell_type":"markdown","metadata":{"id":"IaxkEPZmhYy_"},"source":["- **다층 퍼셉트론(Multi-Layer Perceptron, MLP)**\n","    - 입력층(input layer)과 출력층(output layer) 사이에 하나 이상의 은닉층(hidden layer)을 추가한 신경망 구조\n","    - 각 층은 여러 개의 뉴런(노드)으로 구성되며, 층간 뉴런들은 완전 연결(fully connected)되어 있음\n","    - 은닉층과 비선형 활성화 함수를 사용함으로써 MLP는 비선형 문제도 해결할 수 있게 됨"]},{"cell_type":"markdown","source":["### **1) XOR 게이트**"],"metadata":{"id":"wLL2HHLuiRNK"}},{"cell_type":"code","source":["# -----------------\n","# 다층 퍼셉트론 (XOR 추가)\n","# -----------------\n","def XOR(x1, x2):\n","    \"\"\"\n","    XOR는 단층 퍼셉트론으로 구현할 수 없으므로,\n","    기존의 NAND, OR, AND 게이트를 조합하여 다층 구조로 구현합니다.\n","    Layer 1: NAND, OR\n","    Layer 2: AND\n","    \"\"\"\n","    s1 = NAND(x1, x2)\n","    s2 = OR(x1, x2)\n","    y = AND(s1, s2)\n","    return y\n","\n","# 입력 조합\n","X = [(0, 0), (0, 1), (1, 0), (1, 1)]\n","\n","# 출력 확인 (XOR 열 추가)\n","print(\"x1 x2 | AND OR NAND NOR | XOR\")\n","print(\"-------------------------------\")\n","for x1, x2 in X:\n","    print(f\" {x1}  {x2} |  {AND(x1,x2)}   {OR(x1,x2)}   {NAND(x1,x2)}    {NOR(x1,x2)}  |  {XOR(x1,x2)}\")\n"],"metadata":{"id":"H8yXLECDY01b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sh9DFqWEBCIF"},"source":["### **2) 다층 퍼셉트론** (XOR 게이트 예측 : 이진분류)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"OYzgararAr09"},"outputs":[],"source":["import numpy as np\n","\n","# --- 1. 활성화 함수 및 손실 함수 ---\n","def sigmoid(x):\n","    \"\"\"시그모이드 함수\"\"\"\n","    return 1 / (1 + np.exp(-x))\n","\n","def sigmoid_derivative(x):\n","    \"\"\"시그모이드 함수의 도함수\"\"\"\n","    # 참고: 입력 x는 이미 시그모이드 함수를 통과한 출력값(a)이라고 가정\n","    return x * (1 - x)\n","\n","def softmax(x):\n","    \"\"\"소프트맥스 함수\"\"\"\n","    # 오버플로우 방지를 위해 입력값에서 최댓값을 빼줌\n","    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n","    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n","\n","def mse_loss(y_true, y_pred):\n","    \"\"\"평균 제곱 오차 (MSE) 손실 함수\"\"\"\n","    return np.mean((y_true - y_pred) ** 2)\n","\n","def cross_entropy_loss(y_true, y_pred):\n","    \"\"\"교차 엔트로피 손실 함수 (y_true는 원-핫 인코딩)\"\"\"\n","    num_samples = y_true.shape[0]\n","    # 아주 작은 값(epsilon)을 더해 log(0) 방지\n","    epsilon = 1e-12\n","    y_pred = np.clip(y_pred, epsilon, 1. - epsilon)\n","    loss = -np.sum(y_true * np.log(y_pred)) / num_samples\n","    return loss\n","\n","\n","# --- 2.다층 퍼셉트론 (MLP) 클래스 ---\n","class MLP:\n","    def __init__(self, input_size, hidden_size, output_size, output_activation='sigmoid', seed=None):\n","        \"\"\"\n","        MLP 모델 초기화\n","        input_size: 입력층 노드 수\n","        hidden_size: 은닉층 노드 수\n","        output_size: 출력층 노드 수\n","        seed: 가중치 초기화를 위한 랜덤 시드 (재현성을 위해)\n","        \"\"\"\n","        if seed is not None: np.random.seed(seed)\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.output_size = output_size\n","        self.output_activation = output_activation.lower() # 'sigmoid' 또는 'softmax'\n","\n","        # 가중치 초기화\n","        self.W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2. / input_size) # He 초기화 유사\n","        self.b1 = np.zeros((1, hidden_size))\n","        self.W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2. / hidden_size) # He 초기화 유사\n","        self.b2 = np.zeros((1, output_size))\n","\n","        self.z1, self.a1, self.z2, self.a2 = None, None, None, None\n","\n","    def forward(self, X):\n","        self.z1 = np.dot(X, self.W1) + self.b1\n","        self.a1 = sigmoid(self.z1) # 은닉층은 시그모이드 고정 (ReLU 등으로 변경 가능)\n","\n","        self.z2 = np.dot(self.a1, self.W2) + self.b2\n","        if self.output_activation == 'softmax':      # 다중분류\n","            self.a2 = softmax(self.z2)\n","        elif self.output_activation == 'sigmoid':    # 이진분류\n","            self.a2 = sigmoid(self.z2)\n","        else:                                        # 시그모이드(기본값)\n","            self.a2 = sigmoid(self.z2)\n","        return self.a2\n","\n","    def backward(self, X, y_true, learning_rate):\n","        num_samples = X.shape[0]\n","\n","        # 출력층 델타 계산\n","        if self.output_activation == 'softmax':\n","            # y_true는 원-핫 인코딩된 형태여야 함\n","            # 소프트맥스 + 교차 엔트로피 손실의 경우, 출력층 델타는 (예측값 - 실제값)\n","            delta2 = self.a2 - y_true\n","        elif self.output_activation == 'sigmoid':\n","            # 시그모이드 + MSE 손실의 경우\n","            delta2 = (self.a2 - y_true) * sigmoid_derivative(self.a2)\n","        else: # 기본 (시그모이드 + MSE)\n","            delta2 = (self.a2 - y_true) * sigmoid_derivative(self.a2)\n","\n","        # 은닉층 델타 계산\n","        error_hidden_layer = np.dot(delta2, self.W2.T)\n","        delta1 = error_hidden_layer * sigmoid_derivative(self.a1)\n","\n","        # 기울기 계산\n","        dW2 = (1/num_samples) * np.dot(self.a1.T, delta2)\n","        db2 = (1/num_samples) * np.sum(delta2, axis=0, keepdims=True)\n","        dW1 = (1/num_samples) * np.dot(X.T, delta1)\n","        db1 = (1/num_samples) * np.sum(delta1, axis=0, keepdims=True)\n","\n","        # 가중치 및 편향 업데이트\n","        self.W2 -= learning_rate * dW2\n","        self.b2 -= learning_rate * db2\n","        self.W1 -= learning_rate * dW1\n","        self.b1 -= learning_rate * db1\n","\n","    def train(self, X, y, epochs, learning_rate, verbose_interval=None):\n","        losses = []\n","        # y가 원-핫 인코딩이 필요한 경우 (softmax 사용 시) train 외부에서 처리하거나, 여기서 확인\n","        # 여기서는 y가 이미 적절한 형태로 들어온다고 가정\n","\n","        for epoch in range(epochs):\n","            y_pred = self.forward(X)\n","\n","            if self.output_activation == 'softmax':\n","                loss = cross_entropy_loss(y, y_pred) # y는 원-핫 인코딩된 형태\n","            elif self.output_activation == 'sigmoid':\n","                loss = mse_loss(y, y_pred) # y는 일반적인 타겟 값\n","            else: # 기본 (시그모이드 + MSE)\n","                loss = mse_loss(y, y_pred)\n","            losses.append(loss)\n","\n","            self.backward(X, y, learning_rate)\n","\n","            if verbose_interval and (epoch + 1) % verbose_interval == 0:\n","                print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.6f}\")\n","\n","        if verbose_interval and epochs > 0 and epochs % verbose_interval != 0:\n","             print(f\"Epoch {epochs}/{epochs}, Loss: {losses[-1]:.6f}\")\n","        elif not verbose_interval and epochs > 0 :\n","             print(f\"Epoch {epochs}/{epochs}, Loss: {losses[-1]:.6f}\")\n","        return losses\n","\n","    def predict_proba(self, X):\n","        \"\"\"클래스별 확률 예측 (주로 softmax와 함께 사용)\"\"\"\n","        return self.forward(X)\n","\n","    def predict(self, X):\n","        \"\"\"최종 클래스 예측\"\"\"\n","        probabilities = self.forward(X)\n","        if self.output_activation == 'softmax':\n","            return np.argmax(probabilities, axis=1) # 가장 높은 확률의 클래스 인덱스\n","        elif self.output_activation == 'sigmoid':\n","            return (probabilities > 0.5).astype(int) # 이진 분류 임계값\n","        else: # 기본\n","            return (probabilities > 0.5).astype(int)\n"]},{"cell_type":"code","source":["# --- 메인 실행 부분 ---\n","if __name__ == '__main__':\n","    # --- 2. 다층 퍼셉트론 (MLP)으로 XOR 게이트 학습 예제 ---\n","    print(\"\\n--- 다층 퍼셉트론 (MLP)으로 XOR 게이트 학습 (Sigmoid 출력) ---\")\n","    X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n","    y_xor = np.array([[0], [1], [1], [0]])\n","\n","    # 모델 파라미터\n","    input_dim_xor = X_xor.shape[1]   # 2\n","    hidden_dim_mlp_xor = 4           # 4\n","    output_dim_xor = y_xor.shape[1]  # 1\n","    learning_rate_mlp_xor = 0.1\n","    epochs_mlp_xor = 20000\n","\n","    # MLP 모델 생성\n","    mlp_model = MLP(input_size=input_dim_xor, hidden_size=hidden_dim_mlp_xor, output_size=output_dim_xor, seed=42)\n","    print(\"--- 초기 가중치 및 편향 ---\")\n","    print(f\"초기 가중치(W1):\\n{mlp_model.W1}\")\n","    print(f\"\\n초기 가중치(W2):\\n{mlp_model.W2}\")\n","    print(f\"\\n초기 편향(b1): {mlp_model.b1}\")\n","    print(f\"초기 편향(b2): {mlp_model.b2}\\n\")\n","\n","    # 모델 학습\n","    # train() 메서드는 loss 리스트를 반환합니다.\n","    # 이 값을 mlp_model 변수에 다시 할당하면 모델 객체가 덮어쓰여지므로,\n","    # 반환값은 다른 변수(mlp_losses)에 저장합니다.\n","    # train() 메서드가 실행되면 mlp_model 객체의 가중치는 내부적으로 업데이트됩니다.\n","    print(\"--- 모델 학습 시작 ---\")\n","    mlp_losses = mlp_model.train(X_xor, y_xor, epochs_mlp_xor, learning_rate_mlp_xor, verbose_interval=epochs_mlp_xor // 10)\n","    print(\"--- 모델 학습 완료 ---\\n\")\n","\n","    # 요청하신 학습 후 가중치 및 편향 출력 부분입니다.\n","    print(\"--- 학습 후 가중치 및 편향 ---\")\n","    print(f\"학습 후 가중치(W1):\\n{mlp_model.W1}\")\n","    print(f\"\\n학습 후 가중치(W2):\\n{mlp_model.W2}\")\n","    print(f\"\\n학습 후 편향(b1): {mlp_model.b1}\")\n","    print(f\"학습 후 편향(b2): {mlp_model.b2}\\n\")\n","\n","\n","    # 학습 후 MLP 예측 결과 출력 (XOR 게이트)\n","    # predict()는 최종 클래스(0 또는 1)를, predict_proba()는 확률을 반환합니다.\n","    # 두 가지 모두 출력하여 결과를 더 명확하게 확인합니다.\n","    print(\"--- XOR 게이트 학습 후 MLP 예측 결과 ---\")\n","    probabilities = mlp_model.predict_proba(X_xor)\n","    predictions = mlp_model.predict(X_xor)\n","    for i in range(len(X_xor)):\n","        print(f\"Input: {X_xor[i]}, \"\n","              f\"True Output: {y_xor[i][0]}, \"\n","              f\"Predicted Probability: {probabilities[i][0]:.4f}, \"\n","              f\"Predicted Class: {predictions[i][0]}\")\n"],"metadata":{"id":"ezq2K17ZiqSj"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-m0rAGrPExwj"},"outputs":[],"source":["# 손실 그래프 시각화\n","import matplotlib.pyplot as plt\n","\n","fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n","ax[0].plot(training_losses_slp)\n","ax[0].set_title('SLP Training Loss for AND Gate')\n","ax[0].set_xlabel('Epochs')\n","ax[0].set_ylabel('MSE Loss')\n","ax[0].grid(True)\n","\n","ax[1].plot(training_losses_mlp)\n","ax[1].set_title('MLP Training Loss for XOR Gate')\n","ax[1].set_xlabel('Epochs')\n","ax[1].set_ylabel('MSE Loss')\n","ax[1].grid(True)\n","\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"lW9r_cfnBFKj"},"source":["### **3) 다층 퍼셉트론 (iris 예측 : 다중분류)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N52Qu8JwBGOp"},"outputs":[],"source":["from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","from sklearn.metrics import accuracy_score\n","\n","print(\"\\n--- 다층 퍼셉트론 (MLP)으로 Iris 데이터셋 학습 (Softmax 출력) ---\")\n","# 데이터 로드\n","iris = load_iris()\n","X_iris = iris.data\n","y_iris_original = iris.target.reshape(-1, 1) # (n_samples, 1) 형태로\n","\n","# 데이터 전처리\n","# 1. 입력 데이터 스케일링\n","scaler = StandardScaler()\n","X_iris_scaled = scaler.fit_transform(X_iris)\n","\n","# 2. 타겟 데이터 원-핫 인코딩\n","encoder = OneHotEncoder(sparse_output=False, categories='auto') # sparse=False -> dense array\n","y_iris_one_hot = encoder.fit_transform(y_iris_original)\n","# y_iris_one_hot의 shape: (150, 3)\n","\n","# 3. 학습/테스트 데이터 분리\n","X_train_iris, X_test_iris, y_train_iris_one_hot, y_test_iris_one_hot = train_test_split(\n","    X_iris_scaled, y_iris_one_hot, test_size=0.2, random_state=42, stratify=y_iris_original\n",")\n","# 테스트용 원본 레이블 (비교용)\n","_, _, y_train_iris_orig, y_test_iris_orig = train_test_split(\n","    X_iris_scaled, y_iris_original, test_size=0.2, random_state=42, stratify=y_iris_original\n",")\n","\n","\n","# MLP 모델 파라미터 (Iris)\n","input_dim_iris = X_train_iris.shape[1]          # 4\n","hidden_dim_mlp_iris = 8                         # 8 은닉층 노드 수 (조정 가능)\n","output_dim_iris = y_train_iris_one_hot.shape[1] # 3 (클래스 개수)\n","learning_rate_mlp_iris = 0.01                   # 학습률 (조정 가능)\n","epochs_mlp_iris = 1000                          # 에포크 수 (조정 가능)\n","\n","# MLP 모델 생성 (출력 활성화: softmax)\n","mlp_model_iris = MLP(input_size=input_dim_iris,\n","                        hidden_size=hidden_dim_mlp_iris,\n","                        output_size=output_dim_iris,\n","                        output_activation='softmax',\n","                        seed=42)\n","\n","# 모델 학습\n","print(\"Iris 데이터셋 학습 시작...\")\n","mlp_model_iris.train(X_train_iris, y_train_iris_one_hot, epochs_mlp_iris, learning_rate_mlp_iris, verbose_interval=epochs_mlp_iris//10)\n","\n","# 테스트 데이터로 예측 및 평가\n","y_pred_iris_proba = mlp_model_iris.predict_proba(X_test_iris) # 확률\n","y_pred_iris_classes = mlp_model_iris.predict(X_test_iris)    # 클래스 레이블\n","\n","# 정확도 계산\n","# y_test_iris_one_hot의 argmax는 원본 레이블과 동일\n","accuracy = accuracy_score(np.argmax(y_test_iris_one_hot, axis=1), y_pred_iris_classes)\n","# 또는 accuracy_score(y_test_iris_orig.flatten(), y_pred_iris_classes)\n","\n","print(f\"\\nIris 데이터셋 테스트 정확도: {accuracy*100:.2f}%\")\n","\n","# 일부 예측 결과 출력\n","print(\"\\nIris 테스트 데이터 일부 예측 결과 (첫 5개 샘플):\")\n","for i in range(min(5, X_test_iris.shape[0])):\n","    true_class = iris.target_names[y_test_iris_orig[i][0]]\n","    predicted_class_name = iris.target_names[y_pred_iris_classes[i]]\n","    print(f\"샘플 {i+1}: 실제 클래스 = {true_class:<10}, 예측 클래스 = {predicted_class_name:<10}, 확률 = {y_pred_iris_proba[i]}\")\n","\n"]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"bR1CeNPEzmX8"}}],"metadata":{"hide_input":false,"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"vscode":{"interpreter":{"hash":"ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"}},"colab":{"provenance":[],"toc_visible":true}},"nbformat":4,"nbformat_minor":0}