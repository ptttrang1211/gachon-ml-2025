{"cells":[{"cell_type":"markdown","source":["# **LLM ìì„¸íˆ ì‚´í´ë³´ê¸°**"],"metadata":{"id":"_intVJo1FXQF"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n","\n","- ğŸ’¡ **NOTE**\n","    - ì´ ë…¸íŠ¸ë¶ì˜ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë ¤ë©´ GPUë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. êµ¬ê¸€ ì½”ë©ì—ì„œëŠ” **ëŸ°íƒ€ì„ > ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½ > í•˜ë“œì›¨ì–´ ê°€ì†ê¸° > T4 GPU**ë¥¼ ì„ íƒí•˜ì„¸ìš”.\n","\n","\n","---\n","\n"],"metadata":{"id":"81_Ybs4LI7IX"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"78YkTOy0LSXb"},"outputs":[],"source":["# Phi-3 ëª¨ë¸ê³¼ í˜¸í™˜ì„± ë•Œë¬¸ì— transformers 4.48.3 ë²„ì „ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n","!pip install transformers==4.48.3"]},{"cell_type":"code","source":["# ê¹ƒí—ˆë¸Œì—ì„œ ìœ„ì ¯ ìƒíƒœ ì˜¤ë¥˜ë¥¼ í”¼í•˜ê¸° ìœ„í•´ ì§„í–‰ í‘œì‹œì¤„ì„ ë‚˜íƒ€ë‚´ì§€ ì•Šë„ë¡ ì„¤ì •í•©ë‹ˆë‹¤.\n","from transformers.utils import logging\n","\n","logging.disable_progress_bar()"],"metadata":{"id":"8lVE_3idzVyF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W_23Z_do-faF"},"source":["## **LLM ë¡œë“œí•˜ê¸°**\n","\n","- **pipeline** : ë³µì¡í•œ ì „ì²˜ë¦¬/í›„ì²˜ë¦¬ ê³¼ì •ì„ ìë™í™”í•´ì£¼ëŠ” ê³ ìˆ˜ì¤€ API"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-5RLd6dI-Ytm"},"outputs":[],"source":["from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n","\n","# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n","tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    \"microsoft/Phi-3-mini-4k-instruct\",\n","    device_map=\"cuda\",\n","    torch_dtype=\"auto\",\n","    trust_remote_code=True,\n",")\n","\n","# Create a pipeline\n","generator = pipeline(\n","    \"text-generation\",      # ìˆ˜í–‰í•  ì‘ì—…(task) ìœ í˜•\n","    model=model,\n","    tokenizer=tokenizer,\n","    return_full_text=False, # ìƒì„±ëœ í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜í• ì§€, ì…ë ¥ í”„ë¡¬í”„íŠ¸ë¥¼ í¬í•¨í• ì§€ ê²°ì •(í”„ë¡¬í”„íŠ¸ ì œì™¸)\n","    max_new_tokens=50,      # ìƒì„±í•  ìƒˆë¡œìš´ í† í°ì˜ ìµœëŒ€ ê°œìˆ˜\n","    do_sample=False,        # ìƒ˜í”Œë§(ë¬´ì‘ìœ„ì„±) ì‚¬ìš© ì—¬ë¶€\n",")"]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"rMZqvzy97sRS"}},{"cell_type":"markdown","metadata":{"id":"REqcz-ID_XgV"},"source":["## **í›ˆë ¨ëœ íŠ¸ëœìŠ¤í¬ë¨¸ LLMì˜ ì…ë ¥ê³¼ ì¶œë ¥**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"17h6TPHluJ-i"},"outputs":[],"source":["prompt = \"Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.\"\n","\n","output = generator(prompt)\n","\n","print(output[0]['generated_text'])\n"]},{"cell_type":"markdown","source":["### **[ì‹¤ìŠµ] max_new_tokens ê°’ ì¡°ì •í•˜ê¸°**\n","max_new_tokens ê°’ì„ í¬ê²Œ ëŠ˜ë ¤ì„œ ê²°ê³¼ í™•ì¸í•˜ê¸°\n","- max_new_tokens = 200\n","- max_new_tokens = 1000 (--> OutOfMemoryError)"],"metadata":{"id":"YHbSvXnvxprg"}},{"cell_type":"code","source":["\n","\n","\n","\n","\n"],"metadata":{"id":"J_J-EuoS7nG1"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eoFkdTd6_g5o"},"outputs":[],"source":["print(model)"]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"MJzSPgvI7q6V"}},{"cell_type":"markdown","metadata":{"id":"RTrwzB67BYVY"},"source":["## **í™•ë¥  ë¶„í¬ë¡œë¶€í„° í•˜ë‚˜ì˜ í† í° ì„ íƒí•˜ê¸°(ìƒ˜í”Œë§/ë””ì½”ë”©)**"]},{"cell_type":"markdown","source":["- í™•ë¥  ë¶„í¬ì—ì„œ í•˜ë‚˜ì˜ í† í°ì„ ì„ íƒí•˜ëŠ” ë°©ë²• --> **ë””ì½”ë”© ì „ëµ**\n","- í™•ë¥  ì ìˆ˜ê°€ ê°€ì¥ ë†’ì€ í† í°ì„ ê³ ë¥´ëŠ” ê²½ìš° --> **Greedy Decoding(íƒìš•ì  ë””ì½”ë”©)**\n","    - **LLMì˜ ì˜¨ë„(temperature) ë§¤ê°œë³€ìˆ˜ë¥¼ 0ìœ¼ë¡œ ì„¤ì •í•˜ëŠ” ë°©ì‹**"],"metadata":{"id":"qpAld57j6qll"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"sEcxYgJxBYbJ"},"outputs":[],"source":["prompt = \"The capital of France is\"\n","\n","# ì…ë ¥ í”„ë¡¬í”„íŠ¸ë¥¼ í† í°í™”í•©ë‹ˆë‹¤.\n","input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n","\n","# ì…ë ¥ í† í°ì„ GPUì— ë°°ì¹˜í•©ë‹ˆë‹¤.\n","input_ids = input_ids.to(\"cuda\")\n","\n","# lm_head ì•ì— ìˆëŠ” modelì˜ ì¶œë ¥ì„ ì–»ìŠµë‹ˆë‹¤.\n","model_output = model.model(input_ids)\n","\n","# lm_headì˜ ì¶œë ¥ì„ ì–»ìŠµë‹ˆë‹¤.\n","lm_head_output = model.lm_head(model_output[0])"]},{"cell_type":"code","source":["lm_head_output"],"metadata":{"id":"pjll8vjbKtS_"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"68YUSS4GBf9Q"},"outputs":[],"source":["# ì²« ë²ˆì§¸ ë°ì´í„°(ë¬¸ì¥)ì˜ ë§ˆì§€ë§‰ í† í°(ë‹¨ì–´) ì¶”ì¶œí•˜ê¸°\n","token_id = lm_head_output[0,-1].argmax(-1) # ë§ˆì§€ë§‰ì— ìƒì„±ëœ í† í°ID\n","tokenizer.decode(token_id)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cWWrfC5oBjwp"},"outputs":[],"source":["model_output[0].shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nC1PdOnTBnxZ"},"outputs":[],"source":["lm_head_output.shape"]},{"cell_type":"markdown","metadata":{"id":"Of2_rP4QBqrZ"},"source":["## **í‚¤ì™€ ê°’ì„ ìºì‹±(kv cache)í•˜ì—¬ ìƒì„± ì†ë„ ë†’ì´ê¸°**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B0n6JhNHBrin"},"outputs":[],"source":["prompt = \"Write a very long email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.\"\n","\n","# ì…ë ¥ í”„ë¡¬í”„íŠ¸ë¥¼ í† í°í™”í•©ë‹ˆë‹¤.\n","input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n","input_ids = input_ids.to(\"cuda\")"]},{"cell_type":"markdown","source":["- ë¬¸ì¥ ìƒì„± ì†ë„ ì¸¡ì •(ìºì‹± ì‚¬ìš©)"],"metadata":{"id":"l8mS6oB0C0Dl"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"BwIvt6jSByAF"},"outputs":[],"source":["%%timeit -n 1\n","# í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n","generation_output = model.generate(\n","  input_ids=input_ids,\n","  max_new_tokens=100,\n","  use_cache=True\n",")"]},{"cell_type":"markdown","source":["- ë¬¸ì¥ ìƒì„± ì†ë„ ì¸¡ì •(ìºì‹± ì‚¬ìš© ì•ˆí•¨)"],"metadata":{"id":"wGdM6V-dC4ZW"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"dFb1dcvJByCW"},"outputs":[],"source":["%%timeit -n 1\n","# í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n","generation_output = model.generate(\n","  input_ids=input_ids,\n","  max_new_tokens=100,\n","  use_cache=False\n",")"]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"mQJopJCXWU9c"}},{"cell_type":"markdown","source":["## **ë©€í‹°í—¤ë“œ, ê·¸ë£¹ì¿¼ë¦¬, ë©€í‹°ì¿¼ë¦¬**\n","- í”„ë¡¬í”„íŠ¸ : í—ˆê¹…í˜ì´ìŠ¤ì— ìˆëŠ” ëª¨ë¸ ì¤‘ ë©€í‹°í—¤ë“œ, ê·¸ë£¹ì¿¼ë¦¬, ë©€í‹°ì¿¼ë¦¬ë¥¼ ì‚¬ìš©í•œ ëª¨ë¸ëª…ì„ ê°ê° 2ê°œì”© ì•Œë ¤ì¤˜."],"metadata":{"id":"lxbd20oaWDWF"}},{"cell_type":"markdown","source":["### **ë©€í‹°í—¤ë“œ ì–´í…ì…˜ (Multi-Head Attention, MHA)**\n","- ì…ë ¥ì„ ì—¬ëŸ¬ ê°œì˜ ë…ë¦½ì ì¸ \"í—¤ë“œ\"ë¡œ ë‚˜ëˆ„ì–´ ê°ê° ë‹¤ë¥¸ í‘œí˜„ ë¶€ë¶„ê³µê°„(representation subspace)ì„ í•™ìŠµí•˜ê²Œ í•¨.\n","- ê° í—¤ë“œëŠ” Query, Key, Valueë¥¼ ë…ë¦½ì ìœ¼ë¡œ ê°€ì§€ë©°, ì„œë¡œ ë‹¤ë¥¸ íŒ¨í„´ì„ í¬ì°©í•  ìˆ˜ ìˆìŒ"],"metadata":{"id":"8-k8-qoOWkrg"}},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n","import torch\n","\n","class MHAModelExplorer:\n","    \"\"\"\n","    Multi-Head Attention ëª¨ë¸ íƒìƒ‰ ë° ë¶„ì„\n","    \"\"\"\n","    def __init__(self):\n","        pass\n","\n","    def load_bert_mha(self):\n","        \"\"\"BERTì˜ MHA êµ¬ì¡° ë¶„ì„\"\"\"\n","        print(\"\\n\" + \"=\" * 60)\n","        print(\"ğŸ” BERT (Multi-Head Attention) ë¶„ì„\")\n","        print(\"=\" * 60)\n","\n","        # ëª¨ë¸ ë¡œë“œ\n","        model_name = 'bert-base-uncased'\n","        tokenizer = AutoTokenizer.from_pretrained(model_name)\n","        model = AutoModel.from_pretrained(model_name)\n","\n","        # ëª¨ë¸ êµ¬ì¡° ë¶„ì„\n","        print(f\"\\nğŸ“Š ëª¨ë¸: {model_name}\")\n","        print(f\"ì´ íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in model.parameters()):,}\")\n","\n","        # Attention ì„¤ì • í™•ì¸\n","        config = model.config\n","        print(f\"\\nğŸ¯ Attention ì„¤ì •:\")\n","        print(f\"   Attention í—¤ë“œ ìˆ˜: {config.num_attention_heads}\")\n","        print(f\"   Hidden size: {config.hidden_size}\")\n","        print(f\"   í—¤ë“œë‹¹ ì°¨ì›: {config.hidden_size // config.num_attention_heads}\")\n","\n","        # ê° ë ˆì´ì–´ì˜ Attention êµ¬ì¡°\n","        print(f\"\\nğŸ—ï¸  ì•„í‚¤í…ì²˜:\")\n","        print(f\"   Encoder ë ˆì´ì–´ ìˆ˜: {config.num_hidden_layers}\")\n","        print(f\"   ê° ë ˆì´ì–´ë§ˆë‹¤ {config.num_attention_heads}ê°œì˜ ë…ë¦½ì ì¸ Attention í—¤ë“œ\")\n","\n","        # Multi-Head Attention íŠ¹ì§•\n","        print(f\"\\nâœ¨ Multi-Head Attention íŠ¹ì§•:\")\n","        print(f\"   âœ… Query í—¤ë“œ: {config.num_attention_heads}ê°œ (ë…ë¦½)\")\n","        print(f\"   âœ… Key í—¤ë“œ: {config.num_attention_heads}ê°œ (ë…ë¦½)\")\n","        print(f\"   âœ… Value í—¤ë“œ: {config.num_attention_heads}ê°œ (ë…ë¦½)\")\n","        print(f\"   â†’ ì´ KV ìºì‹œ: {config.num_attention_heads} ì„¸íŠ¸\")\n","\n","        # ì‹¤ì œ ì‚¬ìš© ì˜ˆì‹œ\n","        text = \"Transformers revolutionized natural language processing.\"\n","        inputs = tokenizer(text, return_tensors='pt')\n","\n","        with torch.no_grad():\n","            outputs = model(**inputs, output_attentions=True)\n","\n","        # Attention weights ë¶„ì„\n","        attentions = outputs.attentions  # ê° ë ˆì´ì–´ì˜ attention weights\n","        print(f\"\\nğŸ“ Attention Weights í˜•íƒœ:\")\n","        print(f\"   ë ˆì´ì–´ ìˆ˜: {len(attentions)}\")\n","        print(f\"   ê° ë ˆì´ì–´ attention shape: {attentions[0].shape}\")\n","        print(f\"   â†’ [batch_size, num_heads, seq_len, seq_len]\")\n","\n","        print(\"=\" * 60)\n","\n","        return model, tokenizer\n","\n","    def load_gpt2_mha(self):\n","        \"\"\"GPT-2ì˜ MHA êµ¬ì¡° ë¶„ì„\"\"\"\n","        print(\"\\n\" + \"=\" * 60)\n","        print(\"ğŸ” GPT-2 (Multi-Head Attention) ë¶„ì„\")\n","        print(\"=\" * 60)\n","\n","        # ëª¨ë¸ ë¡œë“œ\n","        model_name = 'gpt2'\n","        tokenizer = AutoTokenizer.from_pretrained(model_name)\n","        model = AutoModelForCausalLM.from_pretrained(model_name)\n","\n","        # ëª¨ë¸ êµ¬ì¡° ë¶„ì„\n","        print(f\"\\nğŸ“Š ëª¨ë¸: {model_name}\")\n","        print(f\"ì´ íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in model.parameters()):,}\")\n","\n","        # Attention ì„¤ì • í™•ì¸\n","        config = model.config\n","        print(f\"\\nğŸ¯ Attention ì„¤ì •:\")\n","        print(f\"   Attention í—¤ë“œ ìˆ˜: {config.n_head}\")\n","        print(f\"   Hidden size: {config.n_embd}\")\n","        print(f\"   í—¤ë“œë‹¹ ì°¨ì›: {config.n_embd // config.n_head}\")\n","\n","        print(f\"\\nğŸ—ï¸  ì•„í‚¤í…ì²˜:\")\n","        print(f\"   Decoder ë ˆì´ì–´ ìˆ˜: {config.n_layer}\")\n","        print(f\"   ê° ë ˆì´ì–´ë§ˆë‹¤ {config.n_head}ê°œì˜ ë…ë¦½ì ì¸ Attention í—¤ë“œ\")\n","\n","        print(f\"\\nâœ¨ Multi-Head Attention íŠ¹ì§•:\")\n","        print(f\"   âœ… Query í—¤ë“œ: {config.n_head}ê°œ (ë…ë¦½)\")\n","        print(f\"   âœ… Key í—¤ë“œ: {config.n_head}ê°œ (ë…ë¦½)\")\n","        print(f\"   âœ… Value í—¤ë“œ: {config.n_head}ê°œ (ë…ë¦½)\")\n","        print(f\"   â†’ Causal Masking ì ìš© (ìê¸°íšŒê·€ì  ìƒì„±)\")\n","\n","        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì •\n","        head_dim = config.n_embd // config.n_head\n","        print(f\"\\nğŸ’¾ KV ìºì‹œ ë©”ëª¨ë¦¬ (ì‹œí€€ìŠ¤ ê¸¸ì´ 1024 ê¸°ì¤€):\")\n","        kv_cache_size = 2 * config.n_layer * config.n_head * 1024 * head_dim * 2  # 2 bytes (FP16)\n","        print(f\"   â†’ ì•½ {kv_cache_size / 1e6:.2f} MB\")\n","\n","        print(\"=\" * 60)\n","\n","        return model, tokenizer\n","\n","    def compare_mha_models(self):\n","        \"\"\"MHA ëª¨ë¸ë“¤ ë¹„êµ\"\"\"\n","        print(\"\\n\" + \"=\" * 60)\n","        print(\"ğŸ“Š Multi-Head Attention ëª¨ë¸ ë¹„êµ\")\n","        print(\"=\" * 60)\n","\n","        models_info = [\n","            {\n","                'name': 'BERT-base',\n","                'hf_name': 'bert-base-uncased',\n","                'heads': 12,\n","                'hidden': 768,\n","                'layers': 12,\n","                'params': '110M',\n","                'type': 'Encoder-only'\n","            },\n","            {\n","                'name': 'GPT-2',\n","                'hf_name': 'gpt2',\n","                'heads': 12,\n","                'hidden': 768,\n","                'layers': 12,\n","                'params': '124M',\n","                'type': 'Decoder-only'\n","            },\n","            {\n","                'name': 'RoBERTa-base',\n","                'hf_name': 'roberta-base',\n","                'heads': 12,\n","                'hidden': 768,\n","                'layers': 12,\n","                'params': '125M',\n","                'type': 'Encoder-only'\n","            },\n","            {\n","                'name': 'T5-base',\n","                'hf_name': 't5-base',\n","                'heads': 12,\n","                'hidden': 768,\n","                'layers': 12,\n","                'params': '220M',\n","                'type': 'Encoder-Decoder'\n","            }\n","        ]\n","\n","        print(\"\\nì¶”ê°€ MHA ëª¨ë¸ ëª©ë¡:\")\n","        print(\"-\" * 60)\n","        for model in models_info:\n","            print(f\"\\nğŸ¤– {model['name']}\")\n","            print(f\"   HF ëª¨ë¸ëª…: {model['hf_name']}\")\n","            print(f\"   í—¤ë“œ ìˆ˜: {model['heads']}, Hidden: {model['hidden']}\")\n","            print(f\"   ë ˆì´ì–´: {model['layers']}, íŒŒë¼ë¯¸í„°: {model['params']}\")\n","            print(f\"   íƒ€ì…: {model['type']}\")\n","\n","        print(\"\\n\" + \"=\" * 60)\n","        print(\"ğŸ’¡ MHAì˜ ì¥ë‹¨ì \")\n","        print(\"=\" * 60)\n","        print(\"âœ… ì¥ì :\")\n","        print(\"   - ê° í—¤ë“œê°€ ë…ë¦½ì ìœ¼ë¡œ ë‹¤ë¥¸ íŒ¨í„´ í•™ìŠµ\")\n","        print(\"   - í’ë¶€í•œ í‘œí˜„ë ¥\")\n","        print(\"   - ë³‘ë ¬ ì²˜ë¦¬ ê°€ëŠ¥\")\n","        print(\"\\nâš ï¸  ë‹¨ì :\")\n","        print(\"   - KV ìºì‹œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë†’ìŒ\")\n","        print(\"   - ê¸´ ì‹œí€€ìŠ¤ì—ì„œ ì¶”ë¡  ì†ë„ ëŠë¦¼\")\n","        print(\"   - ëŒ€í˜• ëª¨ë¸ì—ì„œ ë©”ëª¨ë¦¬ ë³‘ëª©\")\n","        print(\"=\" * 60)\n","\n","# ========================================\n","# ì‚¬ìš© ì˜ˆì‹œ\n","# ========================================\n","\n","explorer = MHAModelExplorer()\n","\n","# BERT ë¶„ì„\n","bert_model, bert_tokenizer = explorer.load_bert_mha()\n","\n","# GPT-2 ë¶„ì„\n","gpt2_model, gpt2_tokenizer = explorer.load_gpt2_mha()\n","\n","# ëª¨ë¸ ë¹„êµ\n","explorer.compare_mha_models()"],"metadata":{"id":"bg_G--F1WDg1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Grouped-Query Attention (GQA) ëª¨ë¸**\n","- Multi-Head Attentionì˜ í‘œí˜„ë ¥ê³¼ Multi-Query Attentionì˜ íš¨ìœ¨ì„±ì„ ê²°í•©í•œ ì ˆì¶©ì•ˆ\n","- Query í—¤ë“œë¥¼ ì—¬ëŸ¬ ê·¸ë£¹ìœ¼ë¡œ ë‚˜ëˆ„ê³ , ê° ê·¸ë£¹ì´ í•˜ë‚˜ì˜ Key-Value í—¤ë“œë¥¼ ê³µìœ í•¨\n","    - 2023ë…„ LLMì´ ì ì  ì»¤ì§€ë©´ì„œ ì¶”ë¡  ì‹œ KV ìºì‹œê°€ ì£¼ìš” ë³‘ëª©ì´ ë¨\n","    - MQAëŠ” ë¹ ë¥´ì§€ë§Œ í’ˆì§ˆ ì €í•˜ ìš°ë ¤\n","    - GQAëŠ” 8ê°œ ì •ë„ì˜ KV í—¤ë“œë¡œë„ MHAì™€ ìœ ì‚¬í•œ ì„±ëŠ¥ ë‹¬ì„±"],"metadata":{"id":"AsM9PYEdW7Vm"}},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForCausalLM\n","import torch\n","\n","class GQAModelExplorer:\n","    \"\"\"\n","    Grouped-Query Attention ëª¨ë¸ íƒìƒ‰ ë° ë¶„ì„\n","    \"\"\"\n","    def __init__(self):\n","        pass\n","\n","    def load_llama2_gqa(self):\n","        \"\"\"Llama 2ì˜ GQA êµ¬ì¡° ë¶„ì„\"\"\"\n","        print(\"\\n\" + \"=\" * 60)\n","        print(\"ğŸ” Llama 2 70B (Grouped-Query Attention) ë¶„ì„\")\n","        print(\"=\" * 60)\n","\n","        # ì°¸ê³ : ì‹¤ì œ 70B ëª¨ë¸ì€ ë©”ëª¨ë¦¬ê°€ ë§ì´ í•„ìš”í•˜ë¯€ë¡œ ì„¤ì •ë§Œ í™•ì¸\n","        model_name = 'meta-llama/Llama-2-70b-hf'\n","\n","        print(f\"\\nğŸ“Š ëª¨ë¸: {model_name}\")\n","        print(f\"ì´ íŒŒë¼ë¯¸í„°: 70B\")\n","\n","        # GQA ì„¤ì • (Llama 2 70B ê¸°ì¤€)\n","        num_attention_heads = 64\n","        num_key_value_heads = 8  # GQAì˜ í•µì‹¬!\n","        hidden_size = 8192\n","        num_layers = 80\n","\n","        print(f\"\\nğŸ¯ GQA Attention ì„¤ì •:\")\n","        print(f\"   Query í—¤ë“œ ìˆ˜: {num_attention_heads}\")\n","        print(f\"   KV í—¤ë“œ ìˆ˜ (ê·¸ë£¹): {num_key_value_heads}\")\n","        print(f\"   ê·¸ë£¹ ìˆ˜: {num_key_value_heads}\")\n","        print(f\"   ê·¸ë£¹ë‹¹ Query í—¤ë“œ: {num_attention_heads // num_key_value_heads}\")\n","        print(f\"   Hidden size: {hidden_size}\")\n","\n","        print(f\"\\nğŸ—ï¸  ì•„í‚¤í…ì²˜:\")\n","        print(f\"   Decoder ë ˆì´ì–´ ìˆ˜: {num_layers}\")\n","\n","        print(f\"\\nâœ¨ Grouped-Query Attention íŠ¹ì§•:\")\n","        print(f\"   âœ… Query í—¤ë“œ: {num_attention_heads}ê°œ (ë…ë¦½)\")\n","        print(f\"   âœ… Key í—¤ë“œ: {num_key_value_heads}ê°œ (ê·¸ë£¹ìœ¼ë¡œ ê³µìœ )\")\n","        print(f\"   âœ… Value í—¤ë“œ: {num_key_value_heads}ê°œ (ê·¸ë£¹ìœ¼ë¡œ ê³µìœ )\")\n","        print(f\"   â†’ ê° KV í—¤ë“œë¥¼ {num_attention_heads // num_key_value_heads}ê°œì˜ Query í—¤ë“œê°€ ê³µìœ \")\n","\n","        # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê³„ì‚°\n","        head_dim = hidden_size // num_attention_heads\n","        seq_len = 4096  # Llama 2ì˜ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´\n","\n","        # MHA vs GQA ë©”ëª¨ë¦¬ ë¹„êµ\n","        mha_kv_cache = 2 * num_layers * num_attention_heads * seq_len * head_dim * 2  # FP16\n","        gqa_kv_cache = 2 * num_layers * num_key_value_heads * seq_len * head_dim * 2  # FP16\n","\n","        print(f\"\\nğŸ’¾ KV ìºì‹œ ë©”ëª¨ë¦¬ ë¹„êµ (ì‹œí€€ìŠ¤ ê¸¸ì´ {seq_len}):\")\n","        print(f\"   MHA ë°©ì‹: {mha_kv_cache / 1e9:.2f} GB\")\n","        print(f\"   GQA ë°©ì‹: {gqa_kv_cache / 1e9:.2f} GB\")\n","        print(f\"   ì ˆê°ëŸ‰: {(1 - gqa_kv_cache / mha_kv_cache) * 100:.1f}%\")\n","        print(f\"   â†’ {num_attention_heads // num_key_value_heads}ë°° ë©”ëª¨ë¦¬ íš¨ìœ¨!\")\n","\n","        print(\"=\" * 60)\n","\n","    def load_mistral_gqa(self):\n","        \"\"\"Mistral 7Bì˜ GQA êµ¬ì¡° ë¶„ì„\"\"\"\n","        print(\"\\n\" + \"=\" * 60)\n","        print(\"ğŸ” Mistral 7B (Grouped-Query Attention) ë¶„ì„\")\n","        print(\"=\" * 60)\n","\n","        model_name = 'mistralai/Mistral-7B-v0.1'\n","\n","        print(f\"\\nğŸ“Š ëª¨ë¸: {model_name}\")\n","\n","        # Mistral 7Bì˜ GQA ì„¤ì •\n","        num_attention_heads = 32\n","        num_key_value_heads = 8  # GQA\n","        hidden_size = 4096\n","        num_layers = 32\n","\n","        print(f\"ì´ íŒŒë¼ë¯¸í„°: 7.24B\")\n","\n","        print(f\"\\nğŸ¯ GQA Attention ì„¤ì •:\")\n","        print(f\"   Query í—¤ë“œ ìˆ˜: {num_attention_heads}\")\n","        print(f\"   KV í—¤ë“œ ìˆ˜ (ê·¸ë£¹): {num_key_value_heads}\")\n","        print(f\"   ê·¸ë£¹ ìˆ˜: {num_key_value_heads}\")\n","        print(f\"   ê·¸ë£¹ë‹¹ Query í—¤ë“œ: {num_attention_heads // num_key_value_heads}\")\n","        print(f\"   Hidden size: {hidden_size}\")\n","\n","        print(f\"\\nğŸ—ï¸  ì•„í‚¤í…ì²˜:\")\n","        print(f\"   Decoder ë ˆì´ì–´ ìˆ˜: {num_layers}\")\n","        print(f\"   Sliding Window Attention: 4096 í† í°\")\n","\n","        print(f\"\\nâœ¨ Grouped-Query Attention íŠ¹ì§•:\")\n","        print(f\"   âœ… Query í—¤ë“œ: {num_attention_heads}ê°œ\")\n","        print(f\"   âœ… KV í—¤ë“œ: {num_key_value_heads}ê°œ\")\n","        print(f\"   âœ… ê·¸ë£¹í™” ë¹„ìœ¨: {num_attention_heads // num_key_value_heads}:1\")\n","\n","        # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±\n","        head_dim = hidden_size // num_attention_heads\n","        seq_len = 8192  # Mistralì˜ ìŠ¬ë¼ì´ë”© ìœˆë„ìš°\n","\n","        mha_kv_cache = 2 * num_layers * num_attention_heads * seq_len * head_dim * 2\n","        gqa_kv_cache = 2 * num_layers * num_key_value_heads * seq_len * head_dim * 2\n","\n","        print(f\"\\nğŸ’¾ KV ìºì‹œ ë©”ëª¨ë¦¬ ë¹„êµ:\")\n","        print(f\"   MHA ë°©ì‹: {mha_kv_cache / 1e9:.2f} GB\")\n","        print(f\"   GQA ë°©ì‹: {gqa_kv_cache / 1e9:.2f} GB\")\n","        print(f\"   ì ˆê°ëŸ‰: {(1 - gqa_kv_cache / mha_kv_cache) * 100:.1f}%\")\n","\n","        print(f\"\\nğŸš€ ì¶”ê°€ ìµœì í™”:\")\n","        print(f\"   âœ… Sliding Window Attention\")\n","        print(f\"   âœ… RoPE (Rotary Position Embedding)\")\n","        print(f\"   âœ… SwiGLU Activation\")\n","\n","        print(\"=\" * 60)\n","\n","    def visualize_gqa_concept(self):\n","        \"\"\"GQA ê°œë… ì‹œê°í™”\"\"\"\n","        print(\"\\n\" + \"=\" * 60)\n","        print(\"ğŸ“Š GQA vs MHA vs MQA ê°œë… ë¹„êµ\")\n","        print(\"=\" * 60)\n","\n","        print(\"\\n1ï¸âƒ£ Multi-Head Attention (MHA)\")\n","        print(\"   Query: [H1] [H2] [H3] [H4] [H5] [H6] [H7] [H8]\")\n","        print(\"   Key:   [H1] [H2] [H3] [H4] [H5] [H6] [H7] [H8]\")\n","        print(\"   Value: [H1] [H2] [H3] [H4] [H5] [H6] [H7] [H8]\")\n","        print(\"   â†’ 8ê°œì˜ ë…ë¦½ì ì¸ KV ì„¸íŠ¸\")\n","\n","        print(\"\\n2ï¸âƒ£ Grouped-Query Attention (GQA)\")\n","        print(\"   Query: [H1] [H2] [H3] [H4] [H5] [H6] [H7] [H8]\")\n","        print(\"          â””â”€â”¬â”€â”˜ â””â”€â”¬â”€â”˜ â””â”€â”¬â”€â”˜ â””â”€â”¬â”€â”˜\")\n","        print(\"   Key:     [G1]   [G2]   [G3]   [G4]\")\n","        print(\"   Value:   [G1]   [G2]   [G3]   [G4]\")\n","        print(\"   â†’ 4ê°œì˜ ê·¸ë£¹í™”ëœ KV ì„¸íŠ¸ (ê° ê·¸ë£¹ì— 2ê°œ Query)\")\n","\n","        print(\"\\n3ï¸âƒ£ Multi-Query Attention (MQA)\")\n","        print(\"   Query: [H1] [H2] [H3] [H4] [H5] [H6] [H7] [H8]\")\n","        print(\"          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n","        print(\"   Key:           [SHARED]\")\n","        print(\"   Value:         [SHARED]\")\n","        print(\"   â†’ 1ê°œì˜ ê³µìœ  KV ì„¸íŠ¸\")\n","\n","        print(\"\\n\" + \"=\" * 60)\n","        print(\"ğŸ’¡ í•µì‹¬ íŠ¸ë ˆì´ë“œì˜¤í”„\")\n","        print(\"=\" * 60)\n","\n","        comparison = [\n","            ['íŠ¹ì„±', 'MHA', 'GQA', 'MQA'],\n","            ['KV í—¤ë“œ ìˆ˜', 'ë§ìŒ (H)', 'ì¤‘ê°„ (G)', 'ì ìŒ (1)'],\n","            ['ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰', 'ë†’ìŒ', 'ì¤‘ê°„', 'ë‚®ìŒ'],\n","            ['í‘œí˜„ë ¥', 'ìµœê³ ', 'ë†’ìŒ', 'ì¤‘ê°„'],\n","            ['ì¶”ë¡  ì†ë„', 'ëŠë¦¼', 'ì¤‘ê°„', 'ë¹ ë¦„'],\n","            ['í’ˆì§ˆ', 'â­â­â­â­â­', 'â­â­â­â­', 'â­â­â­'],\n","        ]\n","\n","        for row in comparison:\n","            print(f\"{row[0]:15s} | {row[1]:15s} | {row[2]:15s} | {row[3]:15s}\")\n","\n","        print(\"=\" * 60)\n","\n","# ========================================\n","# ì‚¬ìš© ì˜ˆì‹œ\n","# ========================================\n","\n","gqa_explorer = GQAModelExplorer()\n","\n","# Llama 2 70B ë¶„ì„\n","gqa_explorer.load_llama2_gqa()\n","\n","# Mistral 7B ë¶„ì„\n","gqa_explorer.load_mistral_gqa()\n","\n","# GQA ê°œë… ì‹œê°í™”\n","gqa_explorer.visualize_gqa_concept()"],"metadata":{"id":"BDsQuHg8W9EF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Multi-Query Attention (MQA) ëª¨ë¸**"],"metadata":{"id":"QIMQL5-rgme2"}},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForCausalLM\n","import torch\n","import time\n","\n","class MQAModelExplorer:\n","    \"\"\"\n","    Multi-Query Attention ëª¨ë¸ íƒìƒ‰ ë° ë¶„ì„\n","    \"\"\"\n","    def __init__(self):\n","        pass\n","\n","    def load_falcon_mqa(self, model_size='7b'):\n","        \"\"\"Falconì˜ MQA êµ¬ì¡° ë¶„ì„\"\"\"\n","        if model_size == '40b':\n","            model_name = 'tiiuae/falcon-40b'\n","            num_heads = 64\n","            hidden_size = 8192\n","            num_layers = 60\n","            params = '40B'\n","        else:  # 7b\n","            model_name = 'tiiuae/falcon-7b'\n","            num_heads = 71\n","            hidden_size = 4544\n","            num_layers = 32\n","            params = '7B'\n","\n","        print(\"\\n\" + \"=\" * 60)\n","        print(f\"ğŸ” Falcon {model_size.upper()} (Multi-Query Attention) ë¶„ì„\")\n","        print(\"=\" * 60)\n","\n","        print(f\"\\nğŸ“Š ëª¨ë¸: {model_name}\")\n","        print(f\"ì´ íŒŒë¼ë¯¸í„°: {params}\")\n","\n","        print(f\"\\nğŸ¯ MQA Attention ì„¤ì •:\")\n","        print(f\"   Query í—¤ë“œ ìˆ˜: {num_heads}\")\n","        print(f\"   KV í—¤ë“œ ìˆ˜: 1 â­ (ëª¨ë“  Queryê°€ ê³µìœ !)\")\n","        print(f\"   Hidden size: {hidden_size}\")\n","        print(f\"   í—¤ë“œë‹¹ ì°¨ì›: {hidden_size // num_heads}\")\n","\n","        print(f\"\\nğŸ—ï¸  ì•„í‚¤í…ì²˜:\")\n","        print(f\"   Decoder ë ˆì´ì–´ ìˆ˜: {num_layers}\")\n","\n","        print(f\"\\nâœ¨ Multi-Query Attention íŠ¹ì§•:\")\n","        print(f\"   âœ… Query í—¤ë“œ: {num_heads}ê°œ (ë…ë¦½)\")\n","        print(f\"   âœ… Key í—¤ë“œ: 1ê°œ (ì „ì²´ ê³µìœ )\")\n","        print(f\"   âœ… Value í—¤ë“œ: 1ê°œ (ì „ì²´ ê³µìœ )\")\n","        print(f\"   â†’ ê·¹ë„ë¡œ íš¨ìœ¨ì ì¸ KV ìºì‹œ!\")\n","\n","        # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê³„ì‚°\n","        head_dim = hidden_size // num_heads\n","        seq_len = 2048\n","\n","        # MHA vs MQA ë©”ëª¨ë¦¬ ë¹„êµ\n","        mha_kv_cache = 2 * num_layers * num_heads * seq_len * head_dim * 2  # FP16\n","        mqa_kv_cache = 2 * num_layers * 1 * seq_len * head_dim * 2  # FP16\n","\n","        print(f\"\\nğŸ’¾ KV ìºì‹œ ë©”ëª¨ë¦¬ ë¹„êµ (ì‹œí€€ìŠ¤ ê¸¸ì´ {seq_len}):\")\n","        print(f\"   MHA ë°©ì‹: {mha_kv_cache / 1e9:.2f} GB\")\n","        print(f\"   MQA ë°©ì‹: {mqa_kv_cache / 1e9:.2f} GB\")\n","        print(f\"   ì ˆê°ëŸ‰: {(1 - mqa_kv_cache / mha_kv_cache) * 100:.1f}%\")\n","        print(f\"   â†’ {num_heads}ë°° ë©”ëª¨ë¦¬ íš¨ìœ¨! ğŸš€\")\n","\n","        print(f\"\\nğŸš€ Falconì˜ ì¶”ê°€ ìµœì í™”:\")\n","        print(f\"   âœ… FlashAttention\")\n","        print(f\"   âœ… Rotary Position Embeddings\")\n","        print(f\"   âœ… Parallel Attention/MLP\")\n","\n","        print(\"=\" * 60)\n","\n","    def compare_all_attention_mechanisms(self):\n","        \"\"\"ëª¨ë“  Attention ë©”ì»¤ë‹ˆì¦˜ ì¢…í•© ë¹„êµ\"\"\"\n","        print(\"\\n\" + \"=\" * 60)\n","        print(\"ğŸ“Š Attention ë©”ì»¤ë‹ˆì¦˜ ì¢…í•© ë¹„êµ\")\n","        print(\"=\" * 60)\n","\n","        # ì˜ˆì‹œ: 64ê°œ Query í—¤ë“œ, 32ê°œ ë ˆì´ì–´, 4096 ì‹œí€€ìŠ¤\n","        num_query_heads = 64\n","        num_layers = 32\n","        seq_len = 4096\n","        head_dim = 128\n","\n","        print(f\"\\nê°€ì •: Query í—¤ë“œ {num_query_heads}ê°œ, \"\n","              f\"ë ˆì´ì–´ {num_layers}ê°œ, ì‹œí€€ìŠ¤ {seq_len}ê°œ\")\n","\n","        # MHA\n","        mha_kv_heads = num_query_heads\n","        mha_memory = 2 * num_layers * mha_kv_heads * seq_len * head_dim * 2 / 1e9\n","\n","        # GQA (8ê°œ ê·¸ë£¹)\n","        gqa_kv_heads = 8\n","        gqa_memory = 2 * num_layers * gqa_kv_heads * seq_len * head_dim * 2 / 1e9\n","\n","        # MQA\n","        mqa_kv_heads = 1\n","        mqa_memory = 2 * num_layers * mqa_kv_heads * seq_len * head_dim * 2 / 1e9\n","\n","        print(\"\\n\" + \"=\" * 80)\n","        print(f\"{'ë©”ì»¤ë‹ˆì¦˜':<15} {'KV í—¤ë“œ':<10} {'ë©”ëª¨ë¦¬ (GB)':<15} {'ì ˆê°ë¥ ':<15} {'ì¶”ë¡ ì†ë„':<15}\")\n","        print(\"=\" * 80)\n","        print(f\"{'MHA':<15} {mha_kv_heads:<10} {mha_memory:<15.2f} {'ê¸°ì¤€':<15} {'1.0x':<15}\")\n","        print(f\"{'GQA':<15} {gqa_kv_heads:<10} {gqa_memory:<15.2f} {f'{(1-gqa_memory/mha_memory)*100:.1f}%':<15} {'~2-3x':<15}\")\n","        print(f\"{'MQA':<15} {mqa_kv_heads:<10} {mqa_memory:<15.2f} {f'{(1-mqa_memory/mha_memory)*100:.1f}%':<15} {'~4-6x':<15}\")\n","        print(\"=\" * 80)\n","\n","        print(\"\\n\" + \"=\" * 60)\n","        print(\"ğŸ¯ ì„ íƒ ê°€ì´ë“œ\")\n","        print(\"=\" * 60)\n","\n","        recommendations = [\n","            {\n","                'use_case': 'ìµœê³  í’ˆì§ˆ í•„ìš”',\n","                'recommendation': 'MHA',\n","                'examples': 'BERT, GPT-3, T5'\n","            },\n","            {\n","                'use_case': 'í’ˆì§ˆê³¼ ì†ë„ì˜ ê· í˜•',\n","                'recommendation': 'GQA',\n","                'examples': 'Llama 2, Mistral, Mixtral'\n","            },\n","            {\n","                'use_case': 'ì‹¤ì‹œê°„ ì¶”ë¡  ì¤‘ìš”',\n","                'recommendation': 'MQA',\n","                'examples': 'Falcon, PaLM, StarCoder'\n","            },\n","            {\n","                'use_case': 'ì œí•œëœ ë©”ëª¨ë¦¬ í™˜ê²½',\n","                'recommendation': 'MQA ë˜ëŠ” GQA',\n","                'examples': 'ëª¨ë°”ì¼, ì—£ì§€ ë””ë°”ì´ìŠ¤'\n","            },\n","            {\n","                'use_case': 'ëŒ€í™”í˜• AI (ë‚®ì€ ë ˆì´í„´ì‹œ)',\n","                'recommendation': 'MQA ë˜ëŠ” GQA',\n","                'examples': 'ì±—ë´‡, ì‹¤ì‹œê°„ ë²ˆì—­'\n","            }\n","        ]\n","\n","        for rec in recommendations:\n","            print(f\"\\nğŸ¯ {rec['use_case']}\")\n","            print(f\"   ê¶Œì¥: {rec['recommendation']}\")\n","            print(f\"   ì˜ˆì‹œ: {rec['examples']}\")\n","\n","        print(\"\\n\" + \"=\" * 60)\n","        print(\"ğŸ’¡ ìµœì‹  íŠ¸ë Œë“œ (2023-2025)\")\n","        print(\"=\" * 60)\n","        print(\"âœ… GQAê°€ ìƒˆë¡œìš´ í‘œì¤€ìœ¼ë¡œ ë¶€ìƒ\")\n","        print(\"   â†’ Llama 2, Mistral, Gemini ë“± ì£¼ìš” ëª¨ë¸ ì±„íƒ\")\n","        print(\"âœ… MHAëŠ” ì†Œí˜• ëª¨ë¸ì—ì„œë§Œ ì‚¬ìš©\")\n","        print(\"   â†’ BERT, GPT-2 ë“± ê¸°ì¡´ ëª¨ë¸\")\n","        print(\"âœ… MQAëŠ” íŠ¹ìˆ˜ ëª©ì  (ì†ë„ ì¤‘ì‹œ) ëª¨ë¸ì— ì‚¬ìš©\")\n","        print(\"   â†’ Falcon, StarCoder\")\n","        print(\"=\" * 60)\n","\n","    def benchmark_inference_speed(self):\n","        \"\"\"ì¶”ë¡  ì†ë„ ë¹„êµ ì‹œë®¬ë ˆì´ì…˜\"\"\"\n","        print(\"\\n\" + \"=\" * 60)\n","        print(\"âš¡ ì¶”ë¡  ì†ë„ ì‹œë®¬ë ˆì´ì…˜\")\n","        print(\"=\" * 60)\n","\n","        # ê°€ìƒì˜ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°\n","        batch_sizes = [1, 4, 16, 32]\n","\n","        # ìƒëŒ€ì  ì†ë„ (MHA ëŒ€ë¹„)\n","        mha_speed = [1.0, 1.0, 1.0, 1.0]\n","        gqa_speed = [2.2, 2.1, 1.9, 1.8]\n","        mqa_speed = [4.5, 4.2, 3.8, 3.5]\n","\n","        print(\"\\nì‹œí€€ìŠ¤ ê¸¸ì´ 2048 í† í°, ìƒì„± 512 í† í° ê°€ì •\")\n","        print(\"\\n\" + \"-\" * 60)\n","        print(f\"{'Batch Size':<15} {'MHA':<15} {'GQA':<15} {'MQA':<15}\")\n","        print(\"-\" * 60)\n","\n","        for i, bs in enumerate(batch_sizes):\n","            print(f\"{bs:<15} {mha_speed[i]:<15.1f}x {gqa_speed[i]:<15.1f}x {mqa_speed[i]:<15.1f}x\")\n","\n","        print(\"-\" * 60)\n","        print(\"\\nğŸ’¡ ì¸ì‚¬ì´íŠ¸:\")\n","        print(\"   - Batch sizeê°€ ì‘ì„ìˆ˜ë¡ MQAì˜ ì¥ì ì´ ë” í¼\")\n","        print(\"   - ì‹¤ì‹œê°„ ì„œë¹„ìŠ¤(batch=1)ì—ì„œ MQAëŠ” 4-5ë°° ë¹ ë¦„\")\n","        print(\"   - GQAëŠ” ì†ë„ì™€ í’ˆì§ˆì˜ ìµœì  ê· í˜•ì \")\n","        print(\"=\" * 60)\n","\n","# ========================================\n","# ì‚¬ìš© ì˜ˆì‹œ\n","# ========================================\n","\n","mqa_explorer = MQAModelExplorer()\n","\n","# Falcon 7B ë¶„ì„\n","mqa_explorer.load_falcon_mqa('7b')\n","\n","# Falcon 40B ë¶„ì„\n","mqa_explorer.load_falcon_mqa('40b')\n","\n","# ëª¨ë“  ë©”ì»¤ë‹ˆì¦˜ ë¹„êµ\n","mqa_explorer.compare_all_attention_mechanisms()\n","\n","# ì¶”ë¡  ì†ë„ ë²¤ì¹˜ë§ˆí¬\n","mqa_explorer.benchmark_inference_speed()"],"metadata":{"id":"qNeQ5Ux9goPc"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}