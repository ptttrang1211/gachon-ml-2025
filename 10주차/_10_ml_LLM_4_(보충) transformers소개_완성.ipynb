{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **transformers ì†Œê°œ**"
      ],
      "metadata": {
        "id": "xAPO58qbDYi8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "NAmo5uhXQ46c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ğŸ’¡ **NOTE**\n",
        "    - ì´ ë…¸íŠ¸ë¶ì˜ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë ¤ë©´ GPUë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. êµ¬ê¸€ ì½”ë©ì—ì„œëŠ” **ëŸ°íƒ€ì„ > ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½ > í•˜ë“œì›¨ì–´ ê°€ì†ê¸° > T4 GPU**ë¥¼ ì„ íƒí•˜ì„¸ìš”."
      ],
      "metadata": {
        "id": "KHIcfXn8RArA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "K-OX9xcYMp_f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.transformersë€?**\n",
        "\n",
        "\n",
        "- **Hugging Faceê°€ ê°œë°œí•œ ì˜¤í”ˆì†ŒìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬(Transformer ëª¨ë¸ ë¼ì´ë¸ŒëŸ¬ë¦¬)**\n",
        "- ìì—°ì–´ ì²˜ë¦¬, ì»´í“¨í„° ë¹„ì „, ìŒì„± ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì˜ ìµœì‹  Transformer ê¸°ë°˜ ì‚¬ì „ í•™ìŠµëœ(pretrained) ëª¨ë¸ì„ ì‰½ê³  ë¹ ë¥´ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì§€ì›í•¨\n",
        "- 3ê°œì˜ ë”¥ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì§€ì› :  PyTorch, TensorFlow, Jax\n",
        "- ìƒì„¸ì„¤ëª… : https://pypi.org/project/transformers/\n"
      ],
      "metadata": {
        "id": "cCx9g4beNOpa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- **ì§€ì› í”Œë«í¼ / ìš”êµ¬ì‚¬í•­**\n",
        "    - Python 3.9 ì´ìƒ, PyTorch 2.1+, TensorFlow 2.6+, Flax 0.4.1+\n",
        "- **ìš©ë„ / íŠ¹ì§•**\n",
        "    - í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ì˜¤ë””ì˜¤, ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ë“¤ì²˜ëŸ¼ ë‹¤ì–‘í•œ ì˜ì—­ì˜ pretrained ëª¨ë¸ì„ ê°„ë‹¨í•œ API(pipeline)ë¡œ ë¹ ë¥´ê²Œ ì‚¬ìš© ê°€ëŠ¥\n",
        "- **ì¶”ì²œ ì‚¬ìš© ì¼€ì´ìŠ¤**\n",
        "    - ë¹ ë¥´ê³  ì‰½ê²Œ NLP, ì»´í“¨í„° ë¹„ì „, ìŒì„± ë“±ì˜ ì‚¬ì „í•™ìŠµ ëª¨ë¸ì„ ì‚¬ìš©í•˜ê±°ë‚˜ fine-tuning í•˜ê³ ì í•  ë•Œ"
      ],
      "metadata": {
        "id": "2SIij8mNSmaY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.ì„¤ì¹˜ ë°©ë²•**"
      ],
      "metadata": {
        "id": "_9dDQ0-2SDH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ì„¤ì¹˜ëœ transformers ë²„ì „ í™•ì¸  -->(ex: Version: 4.55.4)\n",
        "!pip show transformers | grep Version"
      ],
      "metadata": {
        "id": "YUQOtVxdYnhM",
        "outputId": "ed6bb4f8-1a12-48b5-a171-e244a359bd7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Version: 4.57.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gioKeW2nK2zX"
      },
      "outputs": [],
      "source": [
        "# Phi-3 ëª¨ë¸ê³¼ í˜¸í™˜ì„± ë•Œë¬¸ì— transformers 4.48.3 ë²„ì „ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "!pip install transformers==4.48.3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ì„¤ì¹˜ëœ PyTorch ë²„ì „ í™•ì¸  -->(ex: Version: 2.8.0+cu126 (+cu126-->CUDA 12.6 ë²„ì „ ì§€ì›))\n",
        "!pip show torch | grep Version\n",
        "\n",
        "# pip install torch==2.8.0"
      ],
      "metadata": {
        "id": "iBbbJcLvujq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.Transformers ì‚¬ìš© ë°©ì‹(3ê°€ì§€)**\n",
        "- ğŸ’¡ ì§§ì€ ì½”ë“œë¡œ ê³¼ì œì— ë§ê²Œ ì‚¬ì „í•™ìŠµ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œ ë°›ê³  ì‚¬ìš©í•  ìˆ˜ ìˆìŒ\n",
        "- ğŸ’¡ ê°„ë‹¨ ê¸°ë³¸ ì½”ë“œ ì¤‘ì‹¬ìœ¼ë¡œ ì„¤ëª…í•¨"
      ],
      "metadata": {
        "id": "7rEqIVseOecG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1.ì €ìˆ˜ì¤€ API (AutoTokenizer + AutoModel)**\n",
        "- í† í°í™” â†’ ëª¨ë¸ í˜¸ì¶œ â†’ **ì›ì‹œ ë²¡í„°(last_hidden_state, pooler_output) ì–»ê¸°**\n",
        "- **ì—°êµ¬/ì»¤ìŠ¤í…€ íƒœìŠ¤í¬ì— ì í•©**"
      ],
      "metadata": {
        "id": "R1luVOIwjfoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch ë²„ì „\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# 1.í† í°í™”\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
        "\n",
        "# 2.ëª¨ë¸í˜¸ì¶œ\n",
        "model = AutoModel.from_pretrained(\"google-bert/bert-base-uncased\")\n",
        "\n",
        "# 3.ì›ì‹œë²¡í„° ì–»ê¸°\n",
        "inputs = tokenizer(\"Hello world!\", return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "print(outputs.last_hidden_state.shape)"
      ],
      "metadata": {
        "id": "f28jmeSrNE8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **AutoTokenizer, TFAutoModel**\n"
      ],
      "metadata": {
        "id": "bPqQJA7mskEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tensorflow ë²„ì „\n",
        "from transformers import AutoTokenizer, TFAutoModel\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
        "model = TFAutoModel.from_pretrained(\"google-bert/bert-base-uncased\")\n",
        "\n",
        "\n",
        "inputs = tokenizer(\"Hello world!\", return_tensors=\"tf\")\n",
        "outputs = model(**inputs)\n",
        "\n",
        "# Hugging Face Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ PyTorch ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ TensorFlow(TF) ëª¨ë¸ë¡œ ë³€í™˜í•  ë•Œ ì¶œë ¥ë˜ëŠ” ê²½ê³ ë¬¸ì´ ì¶œë ¥ë  ìˆ˜ ìˆë‹¤.\n",
        "#   ì´ ë©”ì‹œì§€ëŠ” PyTorchì—ì„œ í•™ìŠµëœ BERT ëª¨ë¸ì˜ ì¼ë¶€ ê°€ì¤‘ì¹˜(weight)ê°€ TFBertModel ì´ˆê¸°í™” ê³¼ì •ì—ì„œ ì‚¬ìš©ë˜ì§€ ì•Šì•˜ë‹¤ëŠ” ëœ»ìœ¼ë¡œ\n",
        "#   ë³¸ë¬¸ì˜ ê²½ìš° TFBertModelì— í•„ìš”í•œ ëª¨ë“  ê°€ì¤‘ì¹˜ëŠ” ì˜ ë¡œë“œë˜ì—ˆê³ ,\n",
        "#   ì‚¬ìš©í•˜ì§€ ì•Šì€ ê°€ì¤‘ì¹˜ëŠ” ë‹¨ì§€ ë‹¤ë¥¸ taskìš© í—¤ë“œì¼ ë¿ì´ë¯€ë¡œ,\n",
        "#   ì¶”ê°€ í•™ìŠµ ì—†ì´ ë°”ë¡œ ì˜ˆì¸¡ì— ì‚¬ìš© ê°€ëŠ¥í•˜ë‹¤ëŠ” ì–˜ê¸°"
      ],
      "metadata": {
        "id": "c_6eAISlNE-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **í† í¬ë‚˜ì´ì € ì˜µì…˜/ì†ì„± í™•ì¸ ë°©ë²•**\n",
        "ëª¨ë¸ë§ˆë‹¤ ë‹¤ì–‘í•œ ì˜µì…˜/ì†ì„±ì„ ê°–ê³  ìˆë‹¤.\n",
        "    - **print(tokenizer)** â†’ ì£¼ìš” ì„¤ì •(ëª¨ë¸ ì´ë¦„, vocab í¬ê¸°, do_lower_case, max_len ë“±) ì¶œë ¥\n",
        "    - **tokenizer.init_kwargs** â†’ ì´ˆê¸°í™” ì‹œì ì˜ ì˜µì…˜ì´ dict í˜•íƒœë¡œ ë‚˜ì˜´"
      ],
      "metadata": {
        "id": "xEKOGQ-G5LMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# í† í¬ë‚˜ì´ì €ì˜ ëª¨ë“  ì†ì„±ê³¼ ì„¤ì • ë³´ê¸°\n",
        "print(tokenizer)\n",
        "print('-'* 50)\n",
        "\n",
        "# ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ì˜µì…˜ í™•ì¸\n",
        "print(tokenizer.init_kwargs)\n",
        "print('-'* 50)\n",
        "\n",
        "print('# ë‹¨ì–´ ì§‘í•© í¬ê¸°: ', tokenizer.vocab_size)               # ë‹¨ì–´ ì§‘í•© í¬ê¸°\n",
        "print('# ì…ë ¥ í† í° ìµœëŒ€ ê¸¸ì´: ', tokenizer.model_max_length)    # ì…ë ¥ í† í° ìµœëŒ€ ê¸¸ì´\n",
        "print('# ì†Œë¬¸ì ë³€í™˜ ì—¬ë¶€: ', tokenizer.do_lower_case)          # ì†Œë¬¸ì ë³€í™˜ ì—¬ë¶€\n",
        "print('# [PAD] í† í°: ', tokenizer.pad_token)                    # [PAD] í† í°\n",
        "print('# [CLS] í† í°: ', tokenizer.cls_token)                    # [CLS] í† í°\n",
        "print('# [SEP] í† í°: ', tokenizer.sep_token)                    # [SEP] í† í°\n"
      ],
      "metadata": {
        "id": "0jNQ94je5KF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.ì¤‘ê°„ ìˆ˜ì¤€ API (Task-specific Models)**\n",
        "- **AutoModelForXXX â†’ íƒœìŠ¤í¬ë³„ í•™ìŠµ/ì¶”ë¡ ìš©**\n",
        "- AutoModelForSequenceClassification, AutoModelForQuestionAnswering ë“± íƒœìŠ¤í¬ ë§ì¶¤ í—¤ë“œê°€ ë¶™ì€ ëª¨ë¸ ì‚¬ìš©\n",
        "- ì¥ì : íŒŒì¸íŠœë‹/íƒœìŠ¤í¬ ì „ìš© í•™ìŠµì— ì í•©\n",
        "- ë‹¨ì : íƒœìŠ¤í¬ë³„ í´ë˜ìŠ¤ í•„ìš”"
      ],
      "metadata": {
        "id": "3lSZxcbXjf04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n"
      ],
      "metadata": {
        "id": "VpZCfe1_kzMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3.ê³ ìˆ˜ì¤€ API (pipeline)**\n",
        "- **pipeline â†’ ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘, ë°ëª¨ìš©**\n",
        "- í† í¬ë‚˜ì´ì €+ëª¨ë¸+í›„ì²˜ë¦¬ë¥¼ í•œ ë²ˆì— ë¬¶ì–´ ì œê³µ\n",
        "- ê°ì •ë¶„ì„, ë²ˆì—­, ìš”ì•½, QA ë“± ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥"
      ],
      "metadata": {
        "id": "BNUnXmVIjf-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "nlp = pipeline(\"sentiment-analysis\")\n",
        "print(nlp(\"I love this course!\"))"
      ],
      "metadata": {
        "id": "m4ZkU_Hklm1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## **_[ì°¸ê³ ] ëª¨ë¸ë³„ í´ë˜ìŠ¤ ì„ íƒ ì˜ˆ:**"
      ],
      "metadata": {
        "id": "0fCYMLhYuHYD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "|ëª¨ë¸êµ¬ì¡°\t| ê¸°ë³¸(í—¤ë“œ ì—†ìŒ)\t| íƒœìŠ¤í¬ë³„ í—¤ë“œ(ëŒ€í‘œ)|\t ëŒ€í‘œ ëª¨ë¸ |\n",
        "|---|---|---|---|\n",
        "|ì¸ì½”ë” ê¸°ë°˜ (Encoder-only)|\tAutoModel\t| ë§ˆìŠ¤í¬ë“œ LM<br> AutoModelForMaskedLM(ë¬¸ì„œ/ë¬¸ì¥ ë¶„ë¥˜)<br> AutoModelForSequenceClassification(í† í°ë¶„ë¥˜(NER)<br> AutoModelForTokenClassification(ì§ˆì˜ì‘ë‹µ(SQuAD))<br> AutoModelForQuestionAnswering\t| BERT, RoBERTa, DeBERTa, ELECTRA|\n",
        "|ë””ì½”ë” ê¸°ë°˜ (Decoder-only, GPT ê³„ì—´)|\tAutoModel\t|ìƒì„±(ë‹¤ìŒ í† í° ì˜ˆì¸¡)<br> AutoModelForCausalLM(ë¶„ë¥˜)<br>AutoModelForSequenceClassification(í† í°ë¶„ë¥˜)<br> AutoModelForTokenClassification\t|GPT-2/Neo/NeoX/J, Llama/Llama-2/3, MPT, Phi |\n",
        "|ì¸ì½”ë”â€“ë””ì½”ë” (Seq2Seq)|\tAutoModel (ë“œë¬¼ê²Œ ì‚¬ìš©)\t|ìƒì„±(ë²ˆì—­Â·ìš”ì•½ ë“±)<br> AutoModelForSeq2SeqLM (=â€¦ForConditionalGeneration) Â· ë¶„ë¥˜(ì¼ë¶€ ì§€ì›)<br> AutoModelForSequenceClassification\t|T5,BART, mBART, Marian |\n"
      ],
      "metadata": {
        "id": "6lgiSyXhxV3e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ydruB3m2ycKM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.ì˜ˆì œë¡œ QuickTour**"
      ],
      "metadata": {
        "id": "lNBhIvnzPGYD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ì˜ˆì œ: ê°ì •ë¶„ì„**"
      ],
      "metadata": {
        "id": "I-yv_vjYTEn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ê°ì • ë¶„ì„ íŒŒì´í”„ë¼ì¸\n",
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline('sentiment-analysis')\n",
        "print(classifier('We are very happy to introduce pipeline to the transformers repository.'))"
      ],
      "metadata": {
        "id": "kbODkmM0PL28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "nlp = pipeline(\"sentiment-analysis\")\n",
        "print(nlp(\"ì´ ê°•ì˜ëŠ” ì •ë§ ì¬ë¯¸ìˆì–´ìš”!\"))\n"
      ],
      "metadata": {
        "id": "2Dqi289eTARx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ì˜ˆì œ: ì´ë¯¸ì§€ ê°ì²´ ê°ì§€**"
      ],
      "metadata": {
        "id": "5ADhlYgBTKT8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from PIL import Image, ImageDraw\n",
        "from transformers import pipeline\n",
        "\n",
        "# ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ\n",
        "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/coco_sample.png\"\n",
        "image_data = requests.get(url, stream=True).raw\n",
        "image = Image.open(image_data)\n",
        "\n",
        "# ê°ì²´ ê°ì§€ íŒŒì´í”„ë¼ì¸\n",
        "object_detector = pipeline(\"object-detection\")\n",
        "results = object_detector(image)\n",
        "\n",
        "# ì›ë³¸ ì´ë¯¸ì§€ì— ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸°\n",
        "draw = ImageDraw.Draw(image)\n",
        "\n",
        "for result in results:\n",
        "    box = result[\"box\"]\n",
        "    label = result[\"label\"]\n",
        "    score = result[\"score\"]\n",
        "\n",
        "    # ë°•ìŠ¤ ì¢Œí‘œ\n",
        "    x1, y1, x2, y2 = box[\"xmin\"], box[\"ymin\"], box[\"xmax\"], box[\"ymax\"]\n",
        "\n",
        "    # ë°•ìŠ¤ ê·¸ë¦¬ê¸°\n",
        "    draw.rectangle(((x1, y1), (x2, y2)), outline=\"red\", width=3)\n",
        "    # ë ˆì´ë¸” + ì ìˆ˜ í‘œì‹œ\n",
        "    draw.text((x1, y1 - 10), f\"{label} {score:.2f}\", fill=\"yellow\")\n",
        "\n",
        "# ê²°ê³¼ ì¶œë ¥ (Colab í™˜ê²½ì—ì„œ ì‹œê°í™”)\n",
        "image.show()   # ë¡œì»¬ í™˜ê²½ì´ë©´ ìƒˆ ì°½\n",
        "display(image) # Colab/Jupyter í™˜ê²½ì´ë©´ ë…¸íŠ¸ë¶ ì•ˆì— í‘œì‹œ\n"
      ],
      "metadata": {
        "id": "8GYQrUE5pBmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  **5.huggingface Access Token ì ìš© ìœ ë¬´**"
      ],
      "metadata": {
        "id": "O89lFdMSmASC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Access Token ì—†ì´ ê³µê°œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ**"
      ],
      "metadata": {
        "id": "cJDOLfJQ8vkU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "inputs = tokenizer(\"Hello world\", return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "print(outputs.last_hidden_state.shape)"
      ],
      "metadata": {
        "id": "hWEKGelDlAWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFBertModel\n",
        "\n",
        "# PyTorch ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "model = TFBertModel.from_pretrained(\"bert-base-uncased\", from_pt=True)\n",
        "\n",
        "# ì •ìƒ ë™ì‘ (ê²½ê³ ëŠ” ë‚˜ì˜¬ ìˆ˜ ìˆìŒ)\n",
        "outputs = model(**tokenizer(\"Hello world\", return_tensors=\"tf\"))\n",
        "print(outputs.last_hidden_state.shape)\n"
      ],
      "metadata": {
        "id": "O6peF7JXdmLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " #### **Access Token ë„£ê³  ë¹„ê³µê°œ ëª¨ë¸ ì ‘ê·¼**\n",
        "- https://huggingface.co/settings/tokens  ë¡œê·¸ì¸ í›„ ì‚¬ìš©í•˜ê³ ì í•˜ëŠ” ëª¨ë¸ì— ëŒ€í•œ accessìš© í† í° ë°œê¸‰"
      ],
      "metadata": {
        "id": "yPeGS1yncJm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1ë‹¨ê³„: Hugging Face í† í° ìƒì„± ë° ì„¤ì •\n",
        "from huggingface_hub import login\n",
        "import os\n",
        "\n",
        "# Google Colabì—ì„œ í† í° ì…ë ¥ (í•œ ë²ˆë§Œ ì‹¤í–‰)\n",
        "# https://huggingface.co/settings/tokens ì—ì„œ í† í° ìƒì„± í›„ ì…ë ¥\n",
        "login()\n",
        "\n",
        "# ë˜ëŠ” ì§ì ‘ í† í° ì…ë ¥ (ë³´ì•ˆìƒ ê¶Œì¥í•˜ì§€ ì•ŠìŒ)\n",
        "# os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = \"your_token_here\""
      ],
      "metadata": {
        "id": "BVqNfPWtvC2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from huggingface_hub import login\n",
        "# login(\"hf_xxxxxxxxxxxxx\")\n",
        "\n",
        "# from transformers import AutoModel\n",
        "# model = AutoModel.from_pretrained(\"username/private-model\")\n"
      ],
      "metadata": {
        "id": "j8RA42UEcJy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "FVvKuqY1tKG9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6.BERT ëª¨ë¸ë¡œ ê¸°ë³¸ ì½”ë“œ ìƒì„¸ ì„¤ëª…**\n",
        "- ğŸ’¡ Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ê¸°ë³¸ ì‚¬ìš©ë²• - BERT ëª¨ë¸ ì˜ˆì œ\n"
      ],
      "metadata": {
        "id": "BZK4AjHIij89"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. í† í¬ë‚˜ì´ì €ì˜ ì£¼ìš” ê¸°ëŠ¥:\n",
        "    - í…ìŠ¤íŠ¸ë¥¼ í† í°ìœ¼ë¡œ ë¶„í•  (ì„œë¸Œì›Œë“œ í† í°í™”)\n",
        "    - í† í°ì„ ìˆ«ì IDë¡œ ë³€í™˜\n",
        "    - íŠ¹ìˆ˜ í† í° ì¶”ê°€ ([CLS], [SEP], [PAD] ë“±)\n",
        "    - ì–´í…ì…˜ ë§ˆìŠ¤í¬ ìƒì„±\n",
        "2. ëª¨ë¸ì˜ ì£¼ìš” êµ¬ì„±ìš”ì†Œ:\n",
        "    - Embedding Layer: í† í° IDë¥¼ ë²¡í„°ë¡œ ë³€í™˜\n",
        "    - Transformer Layers: 12ê°œì˜ ì¸ì½”ë” ì¸µ\n",
        "    - Pooler: [CLS] í† í°ì˜ í‘œí˜„ì„ ê°€ê³µ (ë¶„ë¥˜ ì‘ì—…ìš©)\n",
        "3. ì…ë ¥ í…ìŠ¤íŠ¸ í† í°í™” ë‹¨ê³„ë³„ ê³¼ì •:\n",
        "    - í…ìŠ¤íŠ¸ ì •ê·œí™”: \"Hello world!\" â†’ \"hello world!\"\n",
        "    - ì„œë¸Œì›Œë“œ í† í°í™”: [\"hello\", \"world\", \"!\"]\n",
        "    - íŠ¹ìˆ˜ í† í° ì¶”ê°€: [\"[CLS]\", \"hello\", \"world\", \"!\", \"[SEP]\"]\n",
        "    - ID ë³€í™˜: [101, 7592, 2088, 999, 102] (ì˜ˆì‹œ)\n",
        "    - í…ì„œ ìƒì„± ë° íŒ¨ë”©/ë§ˆìŠ¤í‚¹\n",
        "4. ëª¨ë¸ ì¶”ë¡ (BERT ëª¨ë¸ ë‚´ë¶€ ì²˜ë¦¬ ê³¼ì •):\n",
        "    - Embedding: í† í° ID â†’ 768ì°¨ì› ë²¡í„°\n",
        "    - Position Encoding: ìœ„ì¹˜ ì •ë³´ ì¶”ê°€\n",
        "    - 12ê°œ Transformer Layer í†µê³¼:\n",
        "        - Multi-Head Self-Attention\n",
        "        - Feed-Forward Network\n",
        "        - Layer Normalization\n",
        "        - Residual Connection\n",
        "    - ìµœì¢… hidden state ìƒì„±"
      ],
      "metadata": {
        "id": "PKY4z-qVqRt7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ê¸°ë³¸ ì½”ë“œ ìƒì„¸ : ì €ìˆ˜ì¤€ API (AutoTokenizer + AutoModel)"
      ],
      "metadata": {
        "id": "oOqZ4l6rr3oQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ê¸°ë³¸ ì‚¬ìš©ë²• - BERT ëª¨ë¸ ì˜ˆì œ\n",
        "# =====================================================\n",
        "\n",
        "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "# AutoTokenizer: ìë™ìœ¼ë¡œ ì ì ˆí•œ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•˜ëŠ” í´ë˜ìŠ¤\n",
        "# AutoModel: ìë™ìœ¼ë¡œ ì ì ˆí•œ ëª¨ë¸ ì•„í‚¤í…ì²˜ë¥¼ ë¡œë“œí•˜ëŠ” í´ë˜ìŠ¤\n",
        "\n",
        "# =====================================================\n",
        "# 1. í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”\n",
        "# =====================================================\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
        "\"\"\"\n",
        "í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” ìƒì„¸ ì„¤ëª…:\n",
        "- from_pretrained(): Hugging Face Hubì—ì„œ ì‚¬ì „ í›ˆë ¨ëœ í† í¬ë‚˜ì´ì €ë¥¼ ë‹¤ìš´ë¡œë“œ\n",
        "- \"google-bert/bert-base-uncased\": ëª¨ë¸ ì‹ë³„ì\n",
        "  * google-bert: Googleì—ì„œ ê°œë°œí•œ BERT ëª¨ë¸\n",
        "  * bert-base: ê¸°ë³¸ í¬ê¸° ëª¨ë¸ (12ì¸µ, 768 hidden size)\n",
        "  * uncased: ëŒ€ì†Œë¬¸ìë¥¼ êµ¬ë¶„í•˜ì§€ ì•ŠìŒ (ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ì†Œë¬¸ìë¡œ ë³€í™˜)\n",
        "\"\"\"\n",
        "\n",
        "# =====================================================\n",
        "# 2. ëª¨ë¸ ì´ˆê¸°í™”\n",
        "# =====================================================\n",
        "model = AutoModel.from_pretrained(\"google-bert/bert-base-uncased\")\n",
        "\"\"\"\n",
        "ëª¨ë¸ ì´ˆê¸°í™” ìƒì„¸ ì„¤ëª…:\n",
        "- ê°™ì€ ëª¨ë¸ ì‹ë³„ìë¥¼ ì‚¬ìš©í•˜ì—¬ í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ì˜ ì¼ì¹˜ì„± ë³´ì¥\n",
        "- BERT-base ëª¨ë¸ êµ¬ì¡°:\n",
        "  * 12ê°œì˜ Transformer ì¸ì½”ë” ì¸µ\n",
        "  * 768ì°¨ì› íˆë“  ìŠ¤í…Œì´íŠ¸\n",
        "  * 12ê°œì˜ ì–´í…ì…˜ í—¤ë“œ\n",
        "  * 30,522ê°œì˜ ì–´íœ˜ í¬ê¸°\n",
        "  * ì•½ 110Mê°œì˜ íŒŒë¼ë¯¸í„°\n",
        "\"\"\"\n",
        "\n",
        "# =====================================================\n",
        "# 3. ì…ë ¥ í…ìŠ¤íŠ¸ í† í°í™”\n",
        "# =====================================================\n",
        "inputs = tokenizer(\"Hello world!\", return_tensors=\"pt\")\n",
        "\"\"\"\n",
        "í† í°í™” ê³¼ì • ìƒì„¸ ì„¤ëª…:\n",
        "- ì…ë ¥: \"Hello world!\" (ì¼ë°˜ í…ìŠ¤íŠ¸ ë¬¸ìì—´)\n",
        "- return_tensors=\"pt\": PyTorch í…ì„œ í˜•íƒœë¡œ ë°˜í™˜ ì§€ì •\n",
        "\n",
        "í…ìŠ¤íŠ¸ ì •ê·œí™”: \"Hello world!\" â†’ \"hello world!\"\n",
        "- ì„œë¸Œì›Œë“œ í† í°í™”: [\"hello\", \"world\", \"!\"]\n",
        "- íŠ¹ìˆ˜ í† í° ì¶”ê°€: [\"[CLS]\", \"hello\", \"world\", \"!\", \"[SEP]\"]\n",
        "- ID ë³€í™˜: [101, 7592, 2088, 999, 102] (ì˜ˆì‹œ)\n",
        "- í…ì„œ ìƒì„± ë° íŒ¨ë”©/ë§ˆìŠ¤í‚¹\n",
        "\n",
        "ë°˜í™˜ë˜ëŠ” ë”•ì…”ë„ˆë¦¬ êµ¬ì¡°:\n",
        "- input_ids: í† í° ID ì‹œí€€ìŠ¤ [1, 5]\n",
        "- token_type_ids: ë¬¸ì¥ êµ¬ë¶„ì (ë‹¨ì¼ ë¬¸ì¥ì´ë¯€ë¡œ ëª¨ë‘ 0) [1, 5]\n",
        "- attention_mask: ì‹¤ì œ í† í° ìœ„ì¹˜ í‘œì‹œ (ëª¨ë‘ 1) [1, 5]\n",
        "\"\"\"\n",
        "\n",
        "# =====================================================\n",
        "# 4. ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰\n",
        "# =====================================================\n",
        "outputs = model(**inputs)\n",
        "\"\"\"\n",
        "ëª¨ë¸ ì¶”ë¡  ìƒì„¸ ì„¤ëª…:\n",
        "- **inputs: ë”•ì…”ë„ˆë¦¬ë¥¼ í‚¤ì›Œë“œ ì¸ìë¡œ ì „ë‹¬\n",
        "- model()ì€ forward() ë©”ì„œë“œë¥¼ í˜¸ì¶œ\n",
        "\n",
        "BERT ëª¨ë¸ ë‚´ë¶€ ì²˜ë¦¬ ê³¼ì •:\n",
        "1. Embedding: í† í° ID â†’ 768ì°¨ì› ë²¡í„°\n",
        "2. Position Encoding: ìœ„ì¹˜ ì •ë³´ ì¶”ê°€\n",
        "3. 12ê°œ Transformer Layer í†µê³¼:\n",
        "   - Multi-Head Self-Attention\n",
        "   - Feed-Forward Network\n",
        "   - Layer Normalization\n",
        "   - Residual Connection\n",
        "4. ìµœì¢… hidden state ìƒì„±\n",
        "\n",
        "ë°˜í™˜ë˜ëŠ” ê°ì²´ (BaseModelOutputWithPoolingAndCrossAttentions):\n",
        "- last_hidden_state: ë§ˆì§€ë§‰ ì¸µì˜ ëª¨ë“  í† í° í‘œí˜„ [batch_size, seq_len, hidden_size]\n",
        "- pooler_output: [CLS] í† í°ì˜ í’€ë§ëœ í‘œí˜„ [batch_size, hidden_size]\n",
        "- hidden_states: ëª¨ë“  ì¸µì˜ hidden state (output_hidden_states=Trueì¼ ë•Œ)\n",
        "- attentions: ì–´í…ì…˜ ê°€ì¤‘ì¹˜ (output_attentions=Trueì¼ ë•Œ)\n",
        "\"\"\"\n",
        "\n",
        "# =====================================================\n",
        "# 5. ì¶œë ¥ ê²°ê³¼ í™•ì¸\n",
        "# =====================================================\n",
        "print(outputs.last_hidden_state.shape)\n",
        "\"\"\"\n",
        "ì¶œë ¥ ê²°ê³¼ ë¶„ì„:\n",
        "- ì˜ˆìƒ ê²°ê³¼: torch.Size([1, 5, 768])\n",
        "  * 1: ë°°ì¹˜ í¬ê¸° (ë‹¨ì¼ ë¬¸ì¥)\n",
        "  * 5: ì‹œí€€ìŠ¤ ê¸¸ì´ (í† í° ê°œìˆ˜)\n",
        "    - [CLS] + hello + world + ! + [SEP] = 5ê°œ\n",
        "  * 768: BERT-baseì˜ íˆë“  ì°¨ì›\n",
        "\n",
        "ê° ì°¨ì›ì˜ ì˜ë¯¸:\n",
        "- ì²« ë²ˆì§¸ ì°¨ì› (1): ë°°ì¹˜ì—ì„œ ì²˜ë¦¬ëœ ë¬¸ì¥ ìˆ˜\n",
        "- ë‘ ë²ˆì§¸ ì°¨ì› (5): ê° í† í°ì˜ ìœ„ì¹˜\n",
        "- ì„¸ ë²ˆì§¸ ì°¨ì› (768): ê° í† í°ì˜ 768ì°¨ì› ë²¡í„° í‘œí˜„\n",
        "\n",
        "í† í°ë³„ í‘œí˜„ ì ‘ê·¼:\n",
        "- outputs.last_hidden_state[0, 0, :]: [CLS] í† í° í‘œí˜„\n",
        "- outputs.last_hidden_state[0, 1, :]: \"hello\" í† í° í‘œí˜„\n",
        "- outputs.last_hidden_state[0, 2, :]: \"world\" í† í° í‘œí˜„\n",
        "- outputs.last_hidden_state[0, 3, :]: \"!\" í† í° í‘œí˜„\n",
        "- outputs.last_hidden_state[0, 4, :]: [SEP] í† í° í‘œí˜„\n",
        "\"\"\"\n",
        "print()"
      ],
      "metadata": {
        "id": "M73QeBZXjMRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ê¸°ë³¸ ì½”ë“œ í™œìš© ì˜ˆì œ"
      ],
      "metadata": {
        "id": "u9Se8Uegrxso"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# ì¶”ê°€ í™œìš© ì˜ˆì œ\n",
        "# =====================================================\n",
        "from typing import List, Tuple\n",
        "import torch        # tensorì™€ ê·¸ë˜ë””ì–¸íŠ¸ ì œì–´ë¥¼ ìœ„í•´ ì‚¬ìš©\n",
        "from transformers import AutoTokenizer, AutoModel, BatchEncoding\n",
        "\n",
        "# ===== í™˜ê²½/ë¦¬ì†ŒìŠ¤ ì„¤ì • =====\n",
        "DEVICE = (\n",
        "    \"cuda\" if torch.cuda.is_available()\n",
        "    else (\"mps\" if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available() else \"cpu\")\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
        "model = AutoModel.from_pretrained(\"google-bert/bert-base-uncased\").to(DEVICE).eval()\n",
        "\n",
        "\n",
        "# ===== 1) í† í°í™” ë¶„ì„ =====\n",
        "def analyze_tokenization(text: str, tokenizer: AutoTokenizer) -> None:\n",
        "    \"\"\"\n",
        "    ë‹¨ì¼ í…ìŠ¤íŠ¸ë¥¼ í•œ ë²ˆì˜ ì¸ì½”ë”©ìœ¼ë¡œ ë¶„ì„(íŠ¹ìˆ˜ í† í°/ids/í† í°/ë³µì› í…ìŠ¤íŠ¸).\n",
        "    \"\"\"\n",
        "    enc: BatchEncoding = tokenizer(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        return_tensors=None,\n",
        "        return_attention_mask=False,\n",
        "        return_offsets_mapping=False\n",
        "    )\n",
        "\n",
        "    input_ids = enc[\"input_ids\"]\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "    decoded = tokenizer.decode(input_ids, skip_special_tokens=False)\n",
        "\n",
        "    print(f\"ì›ë³¸ í…ìŠ¤íŠ¸ : {text}\")\n",
        "    print(f\"í† í°ë“¤      : {tokens}\")\n",
        "    print(f\"í† í° IDë“¤   : {input_ids}\")\n",
        "    print(f\"ë””ì½”ë”© ê²°ê³¼ : {decoded}\")\n",
        "\n",
        "\n",
        "# ===== 2) í‘œí˜„ ì¶”ì¶œ(Mean/Max/CLS) =====\n",
        "def extract_representations(\n",
        "    outputs: BaseException,  # ì‹¤ì œ íƒ€ì…ì€ ModelOutputì´ì§€ë§Œ, ëŸ°íƒ€ì„ ì œë„¤ë¦­ íšŒí”¼\n",
        "    attention_mask: torch.Tensor\n",
        ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    ëª¨ë¸ ì¶œë ¥ì—ì„œ (CLS, mean-pool, max-pool) ë°˜í™˜.\n",
        "    - CLS: ê° ë°°ì¹˜ì˜ ì²« í† í° ì„ë² ë”©\n",
        "    - Mean: attention_maskë¥¼ ë°˜ì˜í•´ PAD ì œì™¸ í‰ê· \n",
        "    - Max: ì‹œí€€ìŠ¤ ì°¨ì› ìµœëŒ€ê°’\n",
        "    ë°˜í™˜ shape: (B, H), (B, H), (B, H)\n",
        "    \"\"\"\n",
        "    last_hidden = outputs.last_hidden_state  # (B, L, H)\n",
        "    # B (Batch size) : í•œ ë²ˆì— ì²˜ë¦¬í•˜ëŠ” ë¬¸ì¥(ë˜ëŠ” ì…ë ¥ ìƒ˜í”Œ)ì˜ ê°œìˆ˜\n",
        "    # L (Sequence length) : ê° ì…ë ¥ ë¬¸ì¥ì˜ í† í° ê°œìˆ˜ (íŒ¨ë”© í¬í•¨)\n",
        "    # H (Hidden size / Hidden dimension) : ê° í† í°ì„ ë²¡í„°ë¡œ í‘œí˜„í•œ ì°¨ì› ìˆ˜, bert-base-uncasedì˜ ê²½ìš° H = 768, bert-largeëŠ” H = 1024\n",
        "\n",
        "    # CLS\n",
        "    cls_repr = last_hidden[:, 0, :]  # (B, H)\n",
        "\n",
        "    # Mean pooling (PAD ì œì™¸). mask -> (B, L, 1), ê°€ì¤‘ í•©/í•©ê³„\n",
        "    mask = attention_mask.unsqueeze(-1).type_as(last_hidden)  # (B, L, 1)\n",
        "    summed = (last_hidden * mask).sum(dim=1)                   # (B, H)\n",
        "    counts = mask.sum(dim=1).clamp(min=1e-9)                  # (B, 1) zero-division ë°©ì§€\n",
        "    mean_repr = summed / counts                               # (B, H)\n",
        "\n",
        "    # Max pooling (PADëŠ” -infë¡œ ì±„ì›Œì„œ ë¬´ì‹œ)\n",
        "    masked_hidden = last_hidden.masked_fill(mask == 0, float(\"-inf\"))\n",
        "    max_repr = masked_hidden.max(dim=1).values                # (B, H)\n",
        "\n",
        "    return cls_repr, mean_repr, max_repr\n",
        "\n",
        "# ===== 3) ë°°ì¹˜ ì²˜ë¦¬ =====\n",
        "def batch_processing(\n",
        "    texts: List[str],\n",
        "    tokenizer: AutoTokenizer,\n",
        "    model: AutoModel,\n",
        "    max_length: int = 512\n",
        ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    ì—¬ëŸ¬ í…ìŠ¤íŠ¸ë¥¼ ë°°ì¹˜ë¡œ ì²˜ë¦¬í•˜ê³  (CLS/Mean/Max) ì„ë² ë”©ì„ ë°˜í™˜.\n",
        "    \"\"\"\n",
        "    enc: BatchEncoding = tokenizer(\n",
        "        texts,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\",\n",
        "        return_attention_mask=True\n",
        "    )\n",
        "\n",
        "\n",
        "    # ë””ë°”ì´ìŠ¤ ì´ë™\n",
        "    enc = {k: v.to(DEVICE) for k, v in enc.items()}\n",
        "    print(enc)\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        outputs = model(**enc)\n",
        "\n",
        "    cls_repr, mean_repr, max_repr = extract_representations(outputs, enc[\"attention_mask\"])\n",
        "    return cls_repr, mean_repr, max_repr\n",
        "\n",
        "\n",
        "# ===== ì‚¬ìš© ì˜ˆì‹œ =====\n",
        "if __name__ == \"__main__\":\n",
        "    print(f\"[INFO] Using device: {DEVICE}\")\n",
        "\n",
        "    # 1) í† í° ì •ë³´ ìì„¸íˆ ë³´ê¸°\n",
        "    analyze_tokenization(\"Hello world!\", tokenizer)\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # 2) ë°°ì¹˜ ì²˜ë¦¬ + í‘œí˜„ ì¶”ì¶œ\n",
        "    sample_texts = [\n",
        "        \"Hello world!\",\n",
        "        \"How are you today?\",\n",
        "        \"This is a longer sentence for demonstration.\"\n",
        "    ]\n",
        "    cls_vec, mean_vec, max_vec = batch_processing(sample_texts, tokenizer, model)\n",
        "\n",
        "    print(f\"CLS í‘œí˜„     : {cls_vec.shape}\")   # (B, 768)\n",
        "    print(f\"Mean í’€ë§    : {mean_vec.shape}\")  # (B, 768)\n",
        "    print(f\"Max  í’€ë§    : {max_vec.shape}\")   # (B, 768)\n",
        "    print('-' * 50)\n"
      ],
      "metadata": {
        "id": "_kh9hYgSjVws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **íŠ¹ìˆ˜ í† í° ì„¤ëª…** : BERT [CLS], [SEP]"
      ],
      "metadata": {
        "id": "jffyGqil6qrn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "####  **BERT ì†Œê°œ**\n",
        "\n",
        "- **BERT**(Bidirectional Encoder Representations from Transformers)\n",
        "    - êµ¬ê¸€(2018)**ì—ì„œ ë°œí‘œí•œ ì‚¬ì „í•™ìŠµ(pre-trained) ì–¸ì–´ëª¨ë¸\n",
        "    - ì–‘ë°©í–¥(Bidirectional) Transformer ì¸ì½”ë” êµ¬ì¡°ë¥¼ ì‚¬ìš© â†’ ë¬¸ë§¥ì„ ì•ë’¤ ëª¨ë‘ ë°˜ì˜\n",
        "    - ì‚¬ì „í•™ìŠµ(Pre-training) ë‘ ê°€ì§€ ë°©ì‹\n",
        "        - Masked Language Modeling (MLM): ë¬¸ì¥ ì† ë‹¨ì–´ë¥¼ [MASK]ë¡œ ê°€ë¦¬ê³  ì˜ˆì¸¡\n",
        "        - Next Sentence Prediction (NSP): ë‘ ë¬¸ì¥ì´ ì—°ì†ë˜ëŠ”ì§€ ì—¬ë¶€ ì˜ˆì¸¡\n",
        "    - í•™ìŠµ í›„ ë‹¤ì–‘í•œ NLP íƒœìŠ¤í¬(ë¶„ë¥˜, ê°œì²´ëª… ì¸ì‹, ì§ˆì˜ì‘ë‹µ ë“±)ì— íŒŒì¸íŠœë‹(fine-tuning) ê°€ëŠ¥\n",
        "    - BERTëŠ” êµ¬ì¡°ì ìœ¼ë¡œ ë‘ ê°œì˜ ë¬¸ì¥(A, B)ê¹Œì§€ë§Œ ëª…ì‹œì ìœ¼ë¡œ ì…ë ¥(êµ¬ë¶„) ê°€ëŠ¥í•¨ -->(ì„¸ê·¸ë¨¼íŠ¸ IDëŠ” 0ê³¼ 1ë§Œ ì§€ì› ê°€ëŠ¥)\n",
        "        - ë¬¸ì¥ì´ 3ê°œ ì´ìƒì¼ ê²½ìš°: ë³´í†µ í•˜ë‚˜ì˜ ê¸´ ì‹œí€€ìŠ¤ë¡œ í•©ì¹˜ê³  [SEP] í† í°ìœ¼ë¡œ ë¶„ë¦¬í•¨\n",
        "    - ì„±ëŠ¥ í˜ì‹ -->ë‹¤ë¥¸ íŒŒìƒ ëª¨ë¸ ë“±ì¥\n",
        "        - RoBERTa: NSP ì œê±°, ë¬¸ì¥ êµ¬ë¶„ì„ ë‹¤ë¥´ê²Œ ì²˜ë¦¬\n",
        "        - Longformer, BigBird: ê¸´ ë¬¸ì¥/ë¬¸ì„œ ì²˜ë¦¬\n",
        "        - ALBERT: Sentence Order Prediction (SOP) ì‚¬ìš©\n",
        "- [ì°¸ê³ ] **GPT** : Transformerì˜ Decoder ë¸”ë¡ë§Œ ì‚¬ìš©í•˜ëŠ” êµ¬ì¡°\n",
        "    - ë‹¨ë°©í–¥ (ì™¼ìª½ â†’ ì˜¤ë¥¸ìª½ë§Œ ë´„), ì˜¤ë¥¸ìª½(ê³¼ê±°) ë¬¸ë§¥ë§Œ ë³´ê³  ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ìê¸°íšŒê·€(auto-regressive) ì–¸ì–´ëª¨ë¸\n",
        "    - ì¦‰, GPTëŠ” ë¬¸ì¥ì„ ì…ë ¥ë°›ìœ¼ë©´:\n",
        "        1. ë¬¸ì¥ì„ í† í°í™” (ì˜ˆ: \"Hello world\" â†’ [Hello, world])\n",
        "        2. ê° í† í°ì„ ì„ë² ë”© + ìœ„ì¹˜ ì„ë² ë”©\n",
        "        3. Causal Masked Self-Attention ì‚¬ìš©\n",
        "            - í˜„ì¬ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•  ë•Œ ì•ìª½ ë‹¨ì–´ë§Œ ì°¸ì¡° ê°€ëŠ¥\n",
        "            - ì˜ˆ: \"The cat sat on the\" â†’ ë‹¤ìŒ ë‹¨ì–´ mat ì˜ˆì¸¡\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PZX8c8BslW9B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  **[CLS] í† í°**\n",
        "- **[CLS] í† í°ì˜ ì—­í• ê³¼ íŠ¹ì§•**\n",
        "    1. ìœ„ì¹˜: í•­ìƒ ì…ë ¥ ì‹œí€€ìŠ¤ì˜ ë§¨ ì•ì— ìœ„ì¹˜\n",
        "    2. ëª©ì : ì „ì²´ ë¬¸ì¥ì˜ ì§‘ì•½ëœ í‘œí˜„(aggregate representation) ìƒì„±\n",
        "    3. í™œìš©: ë¬¸ì¥ ìˆ˜ì¤€ì˜ ë¶„ë¥˜ ì‘ì—… (ê°ì • ë¶„ì„, ë¬¸ì„œ ë¶„ë¥˜ ë“±)ì— ì£¼ë¡œ ì‚¬ìš©\n",
        "    4. í•™ìŠµ: NSP(Next Sentence Prediction) ê³¼ì œë¥¼ í†µí•´ ë¬¸ì¥ ê°„ ê´€ê³„ í•™ìŠµ\n",
        "    - **[CLS] í† í°ì˜ ì„ë² ë”©ì€ ë¬¸ì¥ ì „ì²´ì˜ ì˜ë¯¸ë¥¼ ì••ì¶•í•œ 768ì°¨ì› ë²¡í„°**ì…ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "qiREd0zVz0TA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **[SEP] í† í°**\n",
        "\n",
        "- **[SEP] í† í°ì˜ ì—­í• ê³¼ íŠ¹ì§•**\n",
        "    1. ìœ„ì¹˜: ê° ë¬¸ì¥ì˜ ëì— ìœ„ì¹˜\n",
        "    2. ëª©ì : ë¬¸ì¥ ê°„ ê²½ê³„ë¥¼ ëª…í™•íˆ êµ¬ë¶„\n",
        "    3. í™œìš©:\n",
        "        - ë‹¨ì¼ ë¬¸ì¥: ë¬¸ì¥ ëì„ í‘œì‹œ\n",
        "        - ë¬¸ì¥ ìŒ: ë‘ ë¬¸ì¥ì„ êµ¬ë¶„ (ì§ˆì˜ì‘ë‹µ, ë¬¸ì¥ ê´€ê³„ ë¶„ì„)\n",
        "    4. í† í° íƒ€ì… ID: [SEP] í† í°ì„ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ì¥ êµ¬ë¶„ (0, 1ë¡œ í‘œì‹œ)\n",
        "    - **[SEP] í† í°ì€ BERTê°€ ë¬¸ì¥ì˜ êµ¬ì¡°ë¥¼ ì´í•´í•˜ëŠ” ë° í•µì‹¬ì ì¸ ì—­í• **ì„ í•©ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "tK8pOdi2ypXH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **[CLS], [SEP] í† í° í™•ì¸**\n",
        "- [ì°¸ê³ ] Claude ì•„í‹°íŒ©íŠ¸ : https://claude.ai/public/artifacts/8b6bd242-86a5-48e9-b7dd-457dba23781d"
      ],
      "metadata": {
        "id": "Bm1dMhANG6nJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ ë¡œë“œ\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
        "model = AutoModel.from_pretrained(\"google-bert/bert-base-uncased\")\n",
        "\n",
        "\n",
        "# ê°ì • ë¶„ì„ìš© ì˜ˆì œ ë¬¸ì¥ë“¤\n",
        "sentences = [\n",
        "    \"I love this product! It's amazing!\",           # ë§¤ìš° ê¸ì •\n",
        "    \"This is okay, nothing special.\",               # ì¤‘ë¦½\n",
        "    \"I hate this. Worst purchase ever.\",           # ë§¤ìš° ë¶€ì •\n",
        "    \"The service was good but could be better.\",   # ì•½ê°„ ê¸ì •\n",
        "    \"Today is a beautiful day! ğŸ˜Š\",                # ì´ëª¨ì§€ í¬í•¨ ê¸ì •\n",
        "    \"I'm not sure how I feel about this...\",       # ì• ë§¤í•œ ê°ì •\n",
        "]\n",
        "\n",
        "# =====================================================\n",
        "# 1. [CLS] í† í° (Classification Token)\n",
        "# =====================================================\n",
        "def demonstrate_cls_token():\n",
        "\n",
        "    cls_embeddings = []\n",
        "\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        # í† í°í™” ë° ëª¨ë¸ ì¶”ë¡ \n",
        "        inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "        # [CLS] í† í°ì˜ ì„ë² ë”© ì¶”ì¶œ (ì¸ë±ìŠ¤ 0)\n",
        "        cls_embedding = outputs.last_hidden_state[0, 0, :]  # [ë°°ì¹˜, ìœ„ì¹˜, ì°¨ì›]\n",
        "        cls_embeddings.append(cls_embedding)\n",
        "\n",
        "        print()\n",
        "        print(f\"ë¬¸ì¥ {i+1}: {sentence}\")\n",
        "        print(f\"[CLS] ì„ë² ë”© í˜•íƒœ: {cls_embedding.shape}\")\n",
        "        print(f\"[CLS] ì„ë² ë”© ì¼ë¶€: {cls_embedding[:5].tolist()}\")  # ì²˜ìŒ 5ê°œ ê°’ë§Œ ì¶œë ¥\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "    # [CLS] í† í° ê°„ ìœ ì‚¬ë„ ê³„ì‚° (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)\n",
        "    print(\"\\n=== [CLS] í† í° ê°„ ìœ ì‚¬ë„ ë¶„ì„ ===\")\n",
        "    for i in range(len(cls_embeddings)):\n",
        "        for j in range(i+1, len(cls_embeddings)):\n",
        "            similarity = F.cosine_similarity(\n",
        "                cls_embeddings[i].unsqueeze(0),\n",
        "                cls_embeddings[j].unsqueeze(0)\n",
        "            )\n",
        "            print(f\"ë¬¸ì¥ {i+1} vs ë¬¸ì¥ {j+1}: {similarity.item():.4f}\")\n",
        "\n",
        "# =====================================================\n",
        "# 2. [SEP] í† í° (Separator Token)\n",
        "# =====================================================\n",
        "def demonstrate_sep_token():\n",
        "\n",
        "    # 1.ë‹¨ì¼ ë¬¸ì¥ ì˜ˆì œ\n",
        "    single_sentence = \"BERT is a powerful language model.\"\n",
        "    inputs_single = tokenizer(single_sentence, return_tensors=\"pt\")\n",
        "\n",
        "    print(\"1. ë‹¨ì¼ ë¬¸ì¥ì—ì„œì˜ [SEP] í† í°:\")\n",
        "    print(f\"ì›ë³¸ ë¬¸ì¥: {single_sentence}\")\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs_single.input_ids[0])\n",
        "    print(f\"í† í°ë“¤: {tokens}\")\n",
        "    print(f\"í† í° íƒ€ì… ID: {inputs_single.token_type_ids[0].tolist()}\")\n",
        "    print(\"-\" * 50)\n",
        "    print()\n",
        "\n",
        "\n",
        "    # 2.ë¬¸ì¥ ìŒ ì˜ˆì œ (ì§ˆì˜ì‘ë‹µ í˜•íƒœ)\n",
        "    question = \"What is BERT?\"\n",
        "    answer = \"BERT is a bidirectional encoder representation from transformers.\"\n",
        "\n",
        "    # ë¬¸ì¥ ìŒ í† í°í™”\n",
        "    inputs_pair = tokenizer(question, answer, return_tensors=\"pt\")\n",
        "\n",
        "    print(\"2. ë¬¸ì¥ ìŒì—ì„œì˜ [SEP] í† í°:\")\n",
        "    print(f\"ì§ˆë¬¸: {question}\")\n",
        "    print(f\"ë‹µë³€: {answer}\")\n",
        "    tokens_pair = tokenizer.convert_ids_to_tokens(inputs_pair.input_ids[0])\n",
        "    print(f\"í† í°ë“¤: {tokens_pair}\")\n",
        "    print(f\"í† í° íƒ€ì… ID: {inputs_pair.token_type_ids[0].tolist()}\")\n",
        "    print(\"-\" * 50)\n",
        "    print()\n",
        "\n",
        "\n",
        "    # [SEP] í† í° ìœ„ì¹˜ ì°¾ê¸°\n",
        "    sep_token_id = tokenizer.sep_token_id\n",
        "    sep_positions = (inputs_pair.input_ids[0] == sep_token_id).nonzero().flatten()\n",
        "    print(f\"[SEP] í† í° ID: {sep_token_id}\")\n",
        "    print(f\"[SEP] í† í° ìœ„ì¹˜: {sep_positions.tolist()}\")\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# 3. íŠ¹ìˆ˜ í† í° í™œìš© íŒ¨í„´ ë¹„êµ\n",
        "# =====================================================\n",
        "def compare_token_patterns():\n",
        "    \"\"\"ë‹¤ì–‘í•œ ì…ë ¥ íŒ¨í„´ì—ì„œ íŠ¹ìˆ˜ í† í° ì‚¬ìš© ë¹„êµ\"\"\"\n",
        "\n",
        "    print(\"\\n=== íŠ¹ìˆ˜ í† í° ì‚¬ìš© íŒ¨í„´ ë¹„êµ ===\")\n",
        "\n",
        "    examples = [\n",
        "        # ì¼€ì´ìŠ¤ 1: ë‹¨ì¼ ì§§ì€ ë¬¸ì¥\n",
        "        (\"Hello world!\", \"ë‹¨ì¼ ì§§ì€ ë¬¸ì¥\"),\n",
        "\n",
        "        # ì¼€ì´ìŠ¤ 2: ë‹¨ì¼ ê¸´ ë¬¸ì¥\n",
        "        (\"Natural language processing is a fascinating field that combines linguistics and computer science.\", \"ë‹¨ì¼ ê¸´ ë¬¸ì¥\"),\n",
        "\n",
        "        # ì¼€ì´ìŠ¤ 3: ë¬¸ì¥ ìŒ\n",
        "        ((\"Is this a good movie?\", \"Yes, it's excellent!\"), \"ë¬¸ì¥ ìŒ\"),\n",
        "    ]\n",
        "\n",
        "    for example, description in examples:\n",
        "        print(f\"\\n--- {description} ---\")\n",
        "\n",
        "        if isinstance(example, tuple):  # ë¬¸ì¥ ìŒ\n",
        "            inputs = tokenizer(example[0], example[1], return_tensors=\"pt\")\n",
        "            print(f\"ë¬¸ì¥ 1: {example[0]}\")\n",
        "            print(f\"ë¬¸ì¥ 2: {example[1]}\")\n",
        "        else:  # ë‹¨ì¼ ë¬¸ì¥\n",
        "            inputs = tokenizer(example, return_tensors=\"pt\")\n",
        "            print(f\"ë¬¸ì¥: {example}\")\n",
        "\n",
        "        # í† í° ë¶„ì„\n",
        "        tokens = tokenizer.convert_ids_to_tokens(inputs.input_ids[0])\n",
        "        token_types = inputs.token_type_ids[0].tolist()\n",
        "\n",
        "        print(f\"í† í° ê°œìˆ˜: {len(tokens)}\")\n",
        "        print(f\"í† í°ë“¤: {tokens}\")\n",
        "        print(f\"í† í° íƒ€ì…: {token_types}\")\n",
        "\n",
        "        # íŠ¹ìˆ˜ í† í° ìœ„ì¹˜ í™•ì¸\n",
        "        cls_pos = tokens.index('[CLS]') if '[CLS]' in tokens else -1\n",
        "        sep_positions = [i for i, token in enumerate(tokens) if token == '[SEP]']\n",
        "\n",
        "        print(f\"[CLS] ìœ„ì¹˜: {cls_pos}\")\n",
        "        print(f\"[SEP] ìœ„ì¹˜: {sep_positions}\")\n",
        "\n",
        "def print_token_id():\n",
        "    # ì‹¤ì œ í† í° ID ê°’ í™•ì¸\n",
        "    print(f\"\\n=== í† í° ID ê°’ === \")\n",
        "    print(f\"   [CLS] í† í° ID: {tokenizer.cls_token_id}\")\n",
        "    print(f\"   [SEP] í† í° ID: {tokenizer.sep_token_id}\")\n",
        "    print(f\"   [PAD] í† í° ID: {tokenizer.pad_token_id}\")\n",
        "    print(f\"   [UNK] í† í° ID: {tokenizer.unk_token_id}\")\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# ì‹¤í–‰\n",
        "# =====================================================\n",
        "\n",
        "demonstrate_cls_token()     # [CLS] í† í°ì˜ ì—­í• ê³¼ íŠ¹ì§•\n",
        "demonstrate_sep_token()     # [SEP] í† í°ì˜ ì—­í• ê³¼ íŠ¹ì§•\n",
        "compare_token_patterns()    # íŠ¹ìˆ˜ í† í° í™œìš© íŒ¨í„´ ë¹„êµ"
      ],
      "metadata": {
        "id": "BeG6xCUc2Os_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **=== íŠ¹ìˆ˜ í† í° ì‚¬ìš© ì£¼ì˜ì‚¬í•­ ===**\n",
        "    1. [CLS] í† í° ì£¼ì˜ì‚¬í•­:\n",
        "        - í•­ìƒ ì‹œí€€ìŠ¤ì˜ ì²« ë²ˆì§¸ ìœ„ì¹˜(ì¸ë±ìŠ¤ 0)ì— ìœ„ì¹˜\n",
        "        - ë¬¸ì¥ ìˆ˜ì¤€ ì‘ì—…ì—ë§Œ ì‚¬ìš© (í† í° ìˆ˜ì¤€ ì‘ì—…ì—ëŠ” ë¶€ì í•©)\n",
        "        - íŒŒì¸íŠœë‹ì‹œ [CLS] í† í°ì— ë¶„ë¥˜ í—¤ë“œ ì—°ê²°\n",
        "    2. [SEP] í† í° ì£¼ì˜ì‚¬í•­:\n",
        "        - ë¬¸ì¥ ìŒ ì²˜ë¦¬ì‹œ ë°˜ë“œì‹œ í•„ìš”\n",
        "        - í† í° íƒ€ì… IDì™€ í•¨ê»˜ ë¬¸ì¥ êµ¬ë¶„ì— ì‚¬ìš©\n",
        "        - ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ ê³„ì‚°ì‹œ [SEP] í† í°ë„ í¬í•¨í•˜ì—¬ ê³„ì‚°\n",
        "    3. ì¼ë°˜ì ì¸ ì‹¤ìˆ˜ë“¤:\n",
        "        - [CLS] í† í°ì„ í† í° ìˆ˜ì¤€ ë¶„ë¥˜ì— ì‚¬ìš©\n",
        "        - ë¬¸ì¥ ìŒì—ì„œ [SEP] í† í° ëˆ„ë½\n",
        "        - í† í° íƒ€ì… ID ë¬´ì‹œ"
      ],
      "metadata": {
        "id": "LmO_uvi663pq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7.ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ì‚¬ìš©**"
      ],
      "metadata": {
        "id": "eau2XiCJqlJr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**=== ì‚¬ì „ í›ˆë ¨ ëª¨ë¸ vs ëœë¤ ë¶„ë¥˜ê¸° ë¹„êµ ===**\n",
        "- âœ… ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì˜ ì¥ì :\n",
        "    - â€¢ ì‹¤ì œ ê°ì • ë¶„ì„ ë°ì´í„°ì…‹ìœ¼ë¡œ í›ˆë ¨ë˜ì–´ ë†’ì€ ì •í™•ë„\n",
        "    - â€¢ ë‹¤ì–‘í•œ í‘œí˜„ê³¼ ë¬¸ë§¥ì„ ì´í•´\n",
        "    - â€¢ ì´ëª¨ì§€, ì†ì–´, ì€ìœ ì  í‘œí˜„ë„ ì²˜ë¦¬ ê°€ëŠ¥\n",
        "    - â€¢ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” í™•ë¥  ë¶„í¬ ì œê³µ\n",
        "\n",
        "- âŒ ëœë¤ ê°€ì¤‘ì¹˜ ë¶„ë¥˜ê¸°ì˜ í•œê³„:\n",
        "    - â€¢ ë¬´ì‘ìœ„ ì˜ˆì¸¡ìœ¼ë¡œ ì •í™•ë„ ì•½ 33% (3í´ë˜ìŠ¤ ê¸°ì¤€)\n",
        "    - â€¢ ë¬¸ë§¥ì„ ì „í˜€ ì´í•´í•˜ì§€ ëª»í•¨\n",
        "    - â€¢ ì¼ê´€ì„± ì—†ëŠ” ê²°ê³¼\n",
        "\n",
        "\n",
        "- **ğŸ“š í•µì‹¬ í•™ìŠµ í¬ì¸íŠ¸**:\n",
        "   1. [CLS] í† í°ì€ ë¬¸ì¥ ì „ì²´ì˜ í‘œí˜„ì„ ë‹´ê³  ìˆìŠµë‹ˆë‹¤\n",
        "   2. ì‹¤ì œ ê°ì • ë¶„ì„ì€ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì„ ì‚¬ìš©í•´ì•¼ ì •í™•í•©ë‹ˆë‹¤\n",
        "   3. ëœë¤ ê°€ì¤‘ì¹˜ë¡œëŠ” ì˜ë¯¸ìˆëŠ” ë¶„ë¥˜ê°€ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤\n",
        "\n",
        "- [ì°¸ê³ ] Claude ì•„í‹°íŒ©íŠ¸ : https://claude.ai/public/artifacts/f019b7d3-01de-467e-a01d-00ca8bf4fdb4"
      ],
      "metadata": {
        "id": "SgpMcFQs7W4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ ë¡œë“œ\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
        "model = AutoModel.from_pretrained(\"google-bert/bert-base-uncased\")\n",
        "\n",
        "\n",
        "# ê°ì • ë¶„ì„ìš© ì˜ˆì œ ë¬¸ì¥ë“¤\n",
        "sentences = [\n",
        "    \"I love this product! It's amazing!\",           # ë§¤ìš° ê¸ì •\n",
        "    \"This is okay, nothing special.\",               # ì¤‘ë¦½\n",
        "    \"I hate this. Worst purchase ever.\",           # ë§¤ìš° ë¶€ì •\n",
        "    \"The service was good but could be better.\",   # ì•½ê°„ ê¸ì •\n",
        "]\n",
        "\n",
        "# =====================================================\n",
        "# ë°©ë²• 1: ëœë¤ ê°€ì¤‘ì¹˜ ë¶„ë¥˜ê¸° (êµìœ¡ìš©)\n",
        "# =====================================================\n",
        "def sentiment_analysis_with_cls():\n",
        "    \"\"\"[CLS] í† í°ì„ í™œìš©í•œ ê°„ë‹¨í•œ ê°ì • ë¶„ì„ ì˜ˆì œ\"\"\"\n",
        "\n",
        "    print(\"\\n=== ëœë¤ ê°€ì¤‘ì¹˜ ë¶„ë¥˜ê¸°(í† í° í™œìš© ê°ì • ë¶„ì„) ===\")\n",
        "\n",
        "    cls_embeddings = []\n",
        "\n",
        "    # ê°„ë‹¨í•œ ê°ì • ë¶„ë¥˜ë¥¼ ìœ„í•œ ì„ í˜• ë¶„ë¥˜ê¸° (3ê°œ í´ë˜ìŠ¤: ë¶€ì •, ì¤‘ë¦½, ê¸ì •)\n",
        "    # ì‹¤ì œë¡œëŠ” í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•´ì•¼ í•˜ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” ì‹œì—°ìš©ìœ¼ë¡œ ëœë¤ ê°€ì¤‘ì¹˜ ì‚¬ìš©\n",
        "    torch.manual_seed(42)  # ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ìœ„í•œ ì‹œë“œ ì„¤ì •\n",
        "    classifier_head = torch.nn.Linear(768, 3)  # 768ì°¨ì› ì…ë ¥ â†’ 3ê°œ í´ë˜ìŠ¤ ì¶œë ¥ # â† ì´ ë¶€ë¶„ì´ ëœë¤ ê°€ì¤‘ì¹˜ ìƒì„±!(ë‚´ë¶€ì ìœ¼ë¡œ ê°€ì¤‘ì¹˜ ìë™ ì´ˆê¸°í™”)\n",
        "\n",
        "    # ì‹¤ì œ ê°ì • ë¼ë²¨ (ì´ ëª¨ë¸ì˜ ê²½ìš°)\n",
        "    sentiment_labels = [\"ë¶€ì •ì  (NEGATIVE)\", \"ì¤‘ë¦½ì  (NEUTRAL)\", \"ê¸ì •ì  (POSITIVE)\"]\n",
        "\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "            # [CLS] í† í° ì„ë² ë”© ì¶”ì¶œ\n",
        "            cls_embedding = outputs.last_hidden_state[0, 0, :]\n",
        "            cls_embeddings.append(cls_embedding)\n",
        "\n",
        "            # ê°ì • ë¶„ë¥˜ ì˜ˆì¸¡ ìˆ˜í–‰\n",
        "            logits = classifier_head(cls_embedding.unsqueeze(0))  # ë°°ì¹˜ ì°¨ì› ì¶”ê°€\n",
        "            probabilities = F.softmax(logits, dim=-1)  # í™•ë¥ ë¡œ ë³€í™˜\n",
        "            predicted_class = torch.argmax(probabilities, dim=-1).item()\n",
        "            confidence = probabilities[0, predicted_class].item()\n",
        "\n",
        "            print(f\"ë¬¸ì¥ {i+1}: {sentence}\")\n",
        "            print(f\"[CLS] ì„ë² ë”© í¬ê¸°: {cls_embedding.shape}\")\n",
        "            print(f\"ì˜ˆì¸¡ëœ ê°ì •: {sentiment_labels[predicted_class]}\")\n",
        "            print(f\"ì‹ ë¢°ë„: {confidence:.4f} ({confidence*100:.1f}%)\")\n",
        "\n",
        "            # ê° ê°ì •ë³„ í™•ë¥  ë¶„í¬ ì¶œë ¥\n",
        "            print(\"ê°ì •ë³„ í™•ë¥  ë¶„í¬:\")\n",
        "            for j, (label, prob) in enumerate(zip(sentiment_labels, probabilities[0])):\n",
        "                    bar_length = int(prob.item() * 20)  # ì‹œê°ì  í‘œí˜„ì„ ìœ„í•œ ë°”\n",
        "                    bar = \"â–ˆ\" * bar_length + \"â–‘\" * (20 - bar_length)\n",
        "                    print(f\"  {label}: {prob.item():.4f} ({prob.item()*100:.1f}%) {bar}\")\n",
        "            print(\"-\" * 60)\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# # ë°©ë²• 2: ì‚¬ì „ í›ˆë ¨ëœ ê°ì • ë¶„ì„ ëª¨ë¸ (ì‹¤ìš©ì )\n",
        "# =====================================================\n",
        "def sentiment_analysis_with_cls_pretrainedmodel():\n",
        "    \"\"\"ì‚¬ì „ í›ˆë ¨ëœ ê°ì • ë¶„ì„ ëª¨ë¸ì„ í™œìš©í•œ ì‹¤ì œ ê°ì • ë¶„ì„ ì˜ˆì œ\"\"\"\n",
        "\n",
        "    print(\"\\n=== ì‚¬ì „ í›ˆë ¨ëœ ê°ì • ë¶„ì„ ëª¨ë¸ í™œìš© ===\")\n",
        "\n",
        "    try:\n",
        "        # ì‹¤ì œ ê°ì • ë¶„ì„ìš© ì‚¬ì „ í›ˆë ¨ ëª¨ë¸ ë¡œë“œ\n",
        "        from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "        model_name = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "        print(f\"ëª¨ë¸ ë¡œë”© ì¤‘: {model_name}\")\n",
        "        print(\"(ì²˜ìŒ ì‹¤í–‰ì‹œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œë¡œ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤...)\")\n",
        "\n",
        "        sentiment_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        sentiment_model = AutoModelForSequenceClassification.from_pretrained(model_name) # â† ì´ ë¶€ë¶„ì´ ì‚¬ì „ í›ˆë ¨ëœ ê°€ì¤‘ì¹˜\n",
        "\n",
        "        print(\"ëª¨ë¸ ë¡œë”© ì™„ë£Œ! ê°ì • ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤.\\n\")\n",
        "\n",
        "        # ì‹¤ì œ ê°ì • ë¼ë²¨ (ì´ ëª¨ë¸ì˜ ê²½ìš°)\n",
        "        sentiment_labels = [\"ë¶€ì •ì  (NEGATIVE)\", \"ì¤‘ë¦½ì  (NEUTRAL)\", \"ê¸ì •ì  (POSITIVE)\"]\n",
        "\n",
        "        for i, sentence in enumerate(sentences):\n",
        "            # ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì˜ í† í¬ë‚˜ì´ì € ì‚¬ìš© (ì¤‘ìš”!)\n",
        "            inputs = sentiment_tokenizer(sentence, return_tensors=\"pt\",\n",
        "                                       truncation=True, padding=True, max_length=512)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                # ğŸ”§ ìˆ˜ì •ëœ ë¶€ë¶„: output_hidden_states=True ëª…ì‹œì ìœ¼ë¡œ ì„¤ì •\n",
        "                outputs = sentiment_model(**inputs, output_hidden_states=True)\n",
        "\n",
        "                # ì˜ˆì¸¡ ê²°ê³¼ ì²˜ë¦¬\n",
        "                logits = outputs.logits\n",
        "                probabilities = F.softmax(logits, dim=-1)\n",
        "                predicted_class = torch.argmax(probabilities, dim=-1).item()\n",
        "                confidence = probabilities[0, predicted_class].item()\n",
        "\n",
        "                # ğŸ”§ ìˆ˜ì •ëœ ë¶€ë¶„: ì•ˆì „í•œ [CLS] í† í° ì„ë² ë”© ì¶”ì¶œ\n",
        "                cls_embedding = None\n",
        "                if hasattr(outputs, 'hidden_states') and outputs.hidden_states is not None:\n",
        "                    try:\n",
        "                        # RoBERTaëŠ” <s> í† í°ì„ ì‚¬ìš© (BERTì˜ [CLS]ì™€ ë™ì¼í•œ ì—­í• )\n",
        "                        cls_embedding = outputs.hidden_states[-1][0, 0, :]  # ë§ˆì§€ë§‰ ë ˆì´ì–´, ì²« ë²ˆì§¸ í† í°\n",
        "                        cls_shape = cls_embedding.shape\n",
        "                    except (IndexError, TypeError) as e:\n",
        "                        print(f\"   âš ï¸ [CLS] í† í° ì¶”ì¶œ ì‹¤íŒ¨: {e}\")\n",
        "                        cls_embedding = None\n",
        "                else:\n",
        "                    print(\"   âš ï¸ hidden_statesë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "                print(f\"ë¬¸ì¥ {i+1}: {sentence}\")\n",
        "                if cls_embedding is not None:\n",
        "                    print(f\"[CLS] ì„ë² ë”© í¬ê¸°: {cls_shape}\")\n",
        "                    print(f\"[CLS] ì„ë² ë”© ìƒ˜í”Œ: {cls_embedding[:5].detach().numpy()}\")\n",
        "                print(f\"ì˜ˆì¸¡ëœ ê°ì •: {sentiment_labels[predicted_class]}\")\n",
        "                print(f\"ì‹ ë¢°ë„: {confidence:.4f} ({confidence*100:.1f}%)\")\n",
        "\n",
        "                # ê° ê°ì •ë³„ í™•ë¥  ë¶„í¬ ì¶œë ¥\n",
        "                print(\"ê°ì •ë³„ í™•ë¥  ë¶„í¬:\")\n",
        "                for j, (label, prob) in enumerate(zip(sentiment_labels, probabilities[0])):\n",
        "                    bar_length = int(prob.item() * 20)  # ì‹œê°ì  í‘œí˜„ì„ ìœ„í•œ ë°”\n",
        "                    bar = \"â–ˆ\" * bar_length + \"â–‘\" * (20 - bar_length)\n",
        "                    print(f\"  {label}: {prob.item():.4f} ({prob.item()*100:.1f}%) {bar}\")\n",
        "\n",
        "                print(\"-\" * 80)\n",
        "\n",
        "\n",
        "    except ImportError as e:\n",
        "        print(f\"âš ï¸  í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}\")\n",
        "        print(\"ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”:\")\n",
        "        print(\"pip install transformers torch\")\n",
        "        print(\"ëŒ€ì•ˆìœ¼ë¡œ í‚¤ì›Œë“œ ê¸°ë°˜ ê°ì • ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤...\\n\")\n",
        "        sentiment_analysis_with_keywords(sentences)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸  ëª¨ë¸ ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
        "        print(\"ğŸ”§ ë¬¸ì œ í•´ê²° ë°©ë²•:\")\n",
        "        print(\"   1. ì¸í„°ë„· ì—°ê²° í™•ì¸ (ëª¨ë¸ ë‹¤ìš´ë¡œë“œ í•„ìš”)\")\n",
        "        print(\"   2. transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ë²„ì „ í™•ì¸: pip install --upgrade transformers\")\n",
        "        print(\"   3. torch ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜: pip install torch\")\n",
        "        print(\"   4. ë””ìŠ¤í¬ ê³µê°„ í™•ì¸ (ëª¨ë¸ íŒŒì¼ì´ ì•½ 500MB)\")\n",
        "        print(\"\\nëŒ€ì•ˆìœ¼ë¡œ í‚¤ì›Œë“œ ê¸°ë°˜ ê°ì • ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤...\\n\")\n",
        "        sentiment_analysis_with_keywords(sentences)\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# ì‹¤í–‰\n",
        "# =====================================================\n",
        "\n",
        "# ê°ì • ë¶„ì„ ë¹„êµ (ë‘ ê°€ì§€ ë°©ë²•)\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ê°ì • ë¶„ì„ ë°©ë²• ë¹„êµ\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# ë°©ë²• 1: ëœë¤ ê°€ì¤‘ì¹˜ ë¶„ë¥˜ê¸° (êµìœ¡ìš©)\n",
        "sentiment_analysis_with_cls()\n",
        "\n",
        "\n",
        "# ë°©ë²• 2: ì‚¬ì „ í›ˆë ¨ëœ ê°ì • ë¶„ì„ ëª¨ë¸ (ì‹¤ìš©ì )\n",
        "sentiment_analysis_with_cls_pretrainedmodel()\n"
      ],
      "metadata": {
        "id": "qewQ-4IrK9V5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentiment_analysis_with_keywords():\n",
        "    \"\"\"í‚¤ì›Œë“œ ê¸°ë°˜ ê°„ë‹¨í•œ ê°ì • ë¶„ì„ (í´ë°± ë°©ë²•)\"\"\"\n",
        "\n",
        "    print(\"=== í‚¤ì›Œë“œ ê¸°ë°˜ ê°„ë‹¨ ê°ì • ë¶„ì„ ===\")\n",
        "\n",
        "    # ê°„ë‹¨í•œ ê°ì • í‚¤ì›Œë“œ ì‚¬ì „\n",
        "    positive_words = {\n",
        "        'love', 'amazing', 'great', 'good', 'excellent', 'wonderful',\n",
        "        'fantastic', 'awesome', 'perfect', 'best', 'beautiful', 'happy'\n",
        "    }\n",
        "\n",
        "    negative_words = {\n",
        "        'hate', 'terrible', 'awful', 'bad', 'worst', 'horrible',\n",
        "        'disgusting', 'sad', 'angry', 'disappointed', 'annoying'\n",
        "    }\n",
        "\n",
        "    sentences = [\n",
        "        \"I love this product! It's amazing!\",           # ë§¤ìš° ê¸ì •\n",
        "        \"This is okay, nothing special.\",               # ì¤‘ë¦½\n",
        "        \"I hate this. Worst purchase ever.\",           # ë§¤ìš° ë¶€ì •\n",
        "        \"The service was good but could be better.\",   # ì•½ê°„ ê¸ì •\n",
        "    ]\n",
        "\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        words = sentence.lower().replace('!', '').replace('.', '').replace(',', '').split()\n",
        "\n",
        "        positive_count = sum(1 for word in words if word in positive_words)\n",
        "        negative_count = sum(1 for word in words if word in negative_words)\n",
        "\n",
        "        if positive_count > negative_count:\n",
        "            sentiment = \"ê¸ì •ì \"\n",
        "            confidence = min(0.9, 0.6 + (positive_count - negative_count) * 0.1)\n",
        "        elif negative_count > positive_count:\n",
        "            sentiment = \"ë¶€ì •ì \"\n",
        "            confidence = min(0.9, 0.6 + (negative_count - positive_count) * 0.1)\n",
        "        else:\n",
        "            sentiment = \"ì¤‘ë¦½ì \"\n",
        "            confidence = 0.5\n",
        "\n",
        "        print(f\"ë¬¸ì¥ {i+1}: {sentence}\")\n",
        "        print(f\"ê¸ì • í‚¤ì›Œë“œ: {positive_count}ê°œ, ë¶€ì • í‚¤ì›Œë“œ: {negative_count}ê°œ\")\n",
        "        print(f\"ì˜ˆì¸¡ëœ ê°ì •: {sentiment}\")\n",
        "        print(f\"ì‹ ë¢°ë„: {confidence:.4f} ({confidence*100:.1f}%)\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "# =====================================================\n",
        "# ì‹¤í–‰\n",
        "# =====================================================\n",
        "# í‚¤ì›Œë“œ ê¸°ë°˜ ê°„ë‹¨ ê°ì • ë¶„ì„\n",
        "sentiment_analysis_with_keywords(sentences)"
      ],
      "metadata": {
        "id": "efyIiSlWJBQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "nF_03YnsiwPi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Phiâ€‘3**\n",
        "\n",
        "- **Microsoftê°€ ê°œë°œí•œ ì†Œí˜• ì–¸ì–´ ëª¨ë¸(SLM) ì‹œë¦¬ì¦ˆ**\n",
        "    - Phiâ€‘3â€‘mini (3.8B íŒŒë¼ë¯¸í„°), Phiâ€‘3â€‘small (ì•½ 7B), Phiâ€‘3â€‘medium (14B) ë“± ë‹¤ì–‘í•œ ë³€í˜•ì„ í¬í•¨\n",
        "- íŒŒë¼ë¯¸í„° & ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°\n",
        "    - Phi-3-mini: 3.8B íŒŒë¼ë¯¸í„°, 4K ë° í™•ì¥ëœ 128K ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ì§€ì›\n",
        "    - Phiâ€‘3â€‘small: ì•½ 7B, ê¸°ë³¸ 8K ì»¨í…ìŠ¤íŠ¸\n",
        "    - Phiâ€‘3â€‘medium: 14B ê¸°ëŠ¥, ë” ë„“ì€ ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬ ëŠ¥ë ¥\n",
        "- í•™ìŠµ ë°ì´í„° & ë¯¸ì„¸ì¡°ì •\n",
        "    - ì´ 3.3ì¡° í† í° ì´ìƒìœ¼ë¡œ êµ¬ì„±ëœ ê³ í’ˆì§ˆ í•„í„°ë§ ì›¹ ë°ì´í„°, í•©ì„± ë°ì´í„° ë“±ì„ í™œìš©. ì´í›„ **Supervised Fine-Tuning (SFT)**ê³¼ Direct Preference Optimization (DPO) ê¸°ë²•ì„ í†µí•´ ì¸ê°„ ì„ í˜¸ë„ ë° ì•ˆì „ ê¸°ì¤€ì— ë§ì¶° ë¯¸ì„¸ ì¡°ì •ë¨\n",
        "- ì„±ëŠ¥ ì§€í‘œ\n",
        "    - ìµœì†Œí˜• ëª¨ë¸(Phi-3-mini)ë„ MMLU 69%, MT-bench 8.38 ë“± ì„±ëŠ¥ìœ¼ë¡œ Mixtral 8x7B, GPT-3.5ì™€ ìœ ì‚¬í•œ ìˆ˜ì¤€\n",
        "    - Phiâ€‘3â€‘small ë° mediumëŠ” ê°ê° MMLU 75% ë° 78%, MTâ€‘bench 8.7 ë° 8.9ë¥¼ ë‹¬ì„±\n",
        "    - ì—¬ëŸ¬ ë²¤ì¹˜ë§ˆí¬(HellaSwag, WinoGrande, TruthfulQA, HumanEval ë“±)ì—ì„œ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë³´ì„\n",
        "- ì´ìš© ê°€ëŠ¥ í”Œë«í¼ ë° ìµœì í™”\n",
        "    - Azure AI Studio, Hugging Face, Ollamaì—ì„œ ì‚¬ìš© ê°€ëŠ¥\n",
        "    - ONNX Runtimeê³¼ Windows DirectMLì„ í†µí•œ GPU/CPU ë° ëª¨ë°”ì¼ ê¸°ê¸° ìµœì í™” ì§€ì›\n",
        "    - Ollamaë¥¼ í†µí•´ ë¡œì»¬ì—ì„œë„ ì‹¤í–‰ ê°€ëŠ¥í•˜ë©°, ê¸´ ë¬¸ë§¥ ì²˜ë¦¬ê°€ íŠ¹ì§•\n",
        "- í™•ì¥ ë° ì‘ìš©ì„±\n",
        "    - ë¦¬ì†ŒìŠ¤ ì œì•½ í™˜ê²½, ì €ì§€ì—° ìš”êµ¬ ì‹œë‚˜ë¦¬ì˜¤, ê°•ë ¥í•œ ì¶”ë¡ /ìˆ˜í•™/ë¡œì§ ì²˜ë¦¬, ê¸´ ì»¨í…ìŠ¤íŠ¸ í•„ìš” ì‘ì—…ì— ì í•©\n",
        "    - Strathweb Phi Engineì²˜ëŸ¼ ë‹¤ì–‘í•œ í”Œë«í¼(C#, Swift, Kotlin ë“±)ì—ì„œ ë¡œì»¬ ì‹¤í–‰ì„ ì‰½ê²Œ í•´ ì£¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ë„ ì¡´ì¬\n",
        "- ì±…ì„ìˆëŠ” AI ì„¤ê³„\n",
        "    - Microsoftì˜ Responsible AI ê¸°ì¤€ì— ë”°ë¼ ì„¤ê³„ë¨. RLHF, ìë™í™” í…ŒìŠ¤íŠ¸, red-teaming ë“±ì„ í¬í•¨í•œ ì•ˆì „ì„± ê²€ì¦ í”„ë¡œì„¸ìŠ¤ê°€ ì ìš©ë¨\n"
      ],
      "metadata": {
        "id": "olZXCZdcWh_x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### QuickTour"
      ],
      "metadata": {
        "id": "GYYPM8LKYKtD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ëª¨ë¸ ë¡œë“œ ë° í† í° ìƒì„±\n"
      ],
      "metadata": {
        "id": "iiJCX0VgX5F3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ì¤‘ê°„ìˆ˜ì¤€ API(Task-specific Models) ë°©ì‹"
      ],
      "metadata": {
        "id": "ye9p2FAuak2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Phi-3-mini ë¡œë“œ ë° ê°„ë‹¨ ìƒì„± (Transformers)\n",
        "#   ì„ í˜•ì  ìƒì„± íë¦„ì„ ì²´í—˜í•˜ë©° ëª¨ë¸ ì‘ë™ ì›ë¦¬ë¥¼ ì„¤ëª…í•˜ê¸° ì¢‹ë‹¤.\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n"
      ],
      "metadata": {
        "id": "BZfCMj2BYK4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ê³ ìˆ˜ì¤€ API(pipeline) ë°©ì‹"
      ],
      "metadata": {
        "id": "2c9r0tymaW76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# íŒŒì´í”„ë¼ì¸ì„ ë§Œë“­ë‹ˆë‹¤.\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=False,\n",
        "    max_new_tokens=500,\n",
        "    do_sample=False\n",
        ")"
      ],
      "metadata": {
        "id": "5Foxafz3aXEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### í”„ë¡¬í”„íŠ¸ë¥¼ ì‘ì„±í•˜ê³  ëª¨ë¸ì— ì£¼ì…"
      ],
      "metadata": {
        "id": "sxUo-N0cbC_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# í”„ë¡¬í”„íŠ¸ (ì‚¬ìš©ì ì…ë ¥ / ì¿¼ë¦¬)\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Create a funny joke about chickens.\"}\n",
        "]\n",
        "\n",
        "# ì¶œë ¥ ìƒì„±\n",
        "output = generator(messages)\n",
        "print(output[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "hJOl9GZIbDJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ìˆ˜í•™ì  ì¶”ë¡  í…ŒìŠ¤íŠ¸"
      ],
      "metadata": {
        "id": "-x5CFg5mYDhy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ìˆ˜í•™ì  ì¶”ë¡  í…ŒìŠ¤íŠ¸\n",
        "#   ì§ì ‘ ì•Œê³ ë¦¬ì¦˜ ì‚¬ê³ ê°€ ë‚˜ì˜¤ëŠ” ê³¼ì •ì„ ë³¼ ìˆ˜ ìˆì–´ êµìœ¡ì ìœ¼ë¡œ ìœ ìµ(?)\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
        "\n",
        "\n",
        "# í”„ë¡¬í”„íŠ¸ (ì‚¬ìš©ì ì…ë ¥ / ì¿¼ë¦¬)\n",
        "prompt = \"ì‹œê³—ë°”ëŠ˜ì´ 15ë¶„ ë™ì•ˆ ì–¼ë§ˆë‚˜ ì›€ì§ì¼ê¹Œìš”?\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "output = model.generate(**inputs, max_new_tokens=50)\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "c32AwHKIYmPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [ì‹¤ìŠµ] Phi-3 4K vs 128K ì»¨í…ìŠ¤íŠ¸ ë¹„êµ ì‹¤ìŠµ\n",
        "**[ì£¼ì˜!]** ëª¨ë¸ í¬ê¸°ê°€ í¬ê¸° ë•Œë¬¸ì— 128K ëª¨ë¸ì€ ì‹¤í–‰ ì†ë„ê°€ ëŠë¦´ ìˆ˜ ìˆë‹¤."
      ],
      "metadata": {
        "id": "IezMGMJIY6Lb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ì‹¤í—˜ í¬ì¸íŠ¸**\n",
        "\n",
        "1. 4K ë²„ì „ (Phi-3-mini-4k)\n",
        "    - 4,096 í† í°ê¹Œì§€ë§Œ ì¸ì‹\n",
        "    - ê¸´ ë¬¸ë§¥(30K í† í°)ì„ ë„£ìœ¼ë©´ ì•ë¶€ë¶„ì€ ì˜ë¼ë²„ë¦¼ â†’ â€œì •ë³´ ì†ì‹¤â€ ë°œìƒ\n",
        "    - ë”°ë¼ì„œ ë‹µë³€ì´ í‹€ë¦¬ê±°ë‚˜ ë¶ˆì™„ì „í•  ìˆ˜ ìˆìŒ\n",
        "2. 128K ë²„ì „ (Phi-3-mini-128k)\n",
        "    - ìµœëŒ€ 128,000 í† í°ê¹Œì§€ ì¸ì‹\n",
        "    - ê¸´ ë¬¸ë§¥ ì „ì²´ë¥¼ ìœ ì§€\n",
        "    - ë‹µë³€ì—ì„œ ì •í™•í•œ ë¬¸ì¥ ê°œìˆ˜ë‚˜ ë” ê¹Šì€ ë¬¸ë§¥ ì´í•´ê°€ ê°€ëŠ¥"
      ],
      "metadata": {
        "id": "KAmWuompZN_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# 4K ë²„ì „ê³¼ 128K ë²„ì „ ê°ê° ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "models = {\n",
        "    \"4k\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    \"128k\": \"microsoft/Phi-3-mini-128k-instruct\"\n",
        "}\n",
        "\n",
        "tokenizers = {}\n",
        "loaded_models = {}\n",
        "\n",
        "for key, model_name in models.items():\n",
        "    tokenizers[key] = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "    loaded_models[key] = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name, trust_remote_code=True, device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "# ê¸´ í…ìŠ¤íŠ¸ ìƒì„± (í…ŒìŠ¤íŠ¸ìš© ë¬¸ë§¥)\n",
        "long_text = \"ì´ ë¬¸ì¥ì€ í…ŒìŠ¤íŠ¸ ë¬¸ì¥ì…ë‹ˆë‹¤. \" * 3000  # ì•½ 30K í† í° ìˆ˜ì¤€\n",
        "\n",
        "prompt = long_text + \"\\n\\nì§ˆë¬¸: ìœ„ ë¬¸ì¥ì˜ ê°œìˆ˜ëŠ” ëª‡ ê°œì¸ê°€ìš”?\"\n",
        "\n",
        "# ë‘ ëª¨ë¸ì— ê°ê° ì…ë ¥\n",
        "for key in [\"4k\", \"128k\"]:\n",
        "    tokenizer = tokenizers[key]\n",
        "    model = loaded_models[key]\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    print(f\"\\n===== Phi-3 {key.upper()} ê²°ê³¼ =====\")\n",
        "    outputs = model.generate(**inputs, max_new_tokens=50)\n",
        "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "wgA_FG5IZFYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- ### [ë¯¸ì…˜] Phi ë¹„êµí•˜ê¸°\n",
        "1. ë¬¸í•™ ì‘í’ˆ(ì˜ˆ: í—Œë²• ì „ë¬¸ + ê¸´ ì†Œì„¤ ì¼ë¶€)ì„ í†µì§¸ë¡œ ë„£ì–´ ë³´ê³ , ìš”ì•½ì„ ë¹„êµí•˜ê¸°\n",
        "2. íšŒì˜ë¡ 20í˜ì´ì§€ vs 100í˜ì´ì§€ë¥¼ ë„£ì–´ ë‘ ë²„ì „ì—ì„œ ìš”ì•½ ê²°ê³¼ ì°¨ì´ë¥¼ ì§ì ‘ í™•ì¸\n",
        "3. ìˆ«ì ì„¸ê¸°, ë¬¸ë§¥ ìš”ì•½ ê°™ì€ ë‹¨ìˆœ ê³¼ì œë¥¼ í†µí•´ ì°¨ì´ë¥¼ ëˆˆìœ¼ë¡œ ë³´ì—¬ì£¼ê¸° -->"
      ],
      "metadata": {
        "id": "RAnHmaGmZoPd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ufq_HCjKZ-C5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "0g36MizoMox8"
      }
    }
  ]
}