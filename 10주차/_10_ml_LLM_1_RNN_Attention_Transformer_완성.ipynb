{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **\"I love llamas\"ë¥¼ \"Ik hou van lama's\"ë¡œ ë²ˆì—­í•˜ëŠ” ì˜ˆì œ**"
      ],
      "metadata": {
        "id": "Kt3UZ2gRcqqm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "p6ajhPyPdN3b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ğŸ’¡ **NOTE**\n",
        "    - ì´ ë…¸íŠ¸ë¶ì˜ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë ¤ë©´ GPUë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. êµ¬ê¸€ ì½”ë©ì—ì„œëŠ” **ëŸ°íƒ€ì„ > ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½ > í•˜ë“œì›¨ì–´ ê°€ì†ê¸° > T4 GPU**ë¥¼ ì„ íƒí•˜ì„¸ìš”."
      ],
      "metadata": {
        "id": "LIKiFsVSnVMl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "QGGN9MLEna-L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. RNN(Recurrent Neural Network) ì•„í‚¤í…ì²˜ ì‚¬ìš© : Seq2Seq**\n",
        "- (ì‹¤í–‰ì„ ìœ„í•´ ì• ì½”ë“œì—ì„œ ì¶”ê°€í•œ ì‚¬í•­)\n",
        "    - **ì–´íœ˜ ì‚¬ì „** : Vocabulary í´ë˜ìŠ¤ â†’ ë‹¨ì–´-ì¸ë±ìŠ¤ ë§¤í•‘ ìë™í™”\n",
        "    - **íŠ¹ìˆ˜ í† í°** : `<SOS>`, `<EOS>`, `<PAD>`, `<UNK>` â†’ë¬¸ì¥ ê²½ê³„ ë° íŒ¨ë”© ì²˜ë¦¬\n",
        "    - **Teacher Forcing** : í™•ë¥ ì  ì ìš© â†’í›ˆë ¨ ì•ˆì •ì„± í–¥ìƒ\n",
        "    - **ë“œë¡­ì•„ì›ƒ** : 0.2 ë¹„ìœ¨ ì ìš© â†’ ê³¼ì í•© ë°©ì§€"
      ],
      "metadata": {
        "id": "2a6-GJV4Ykeq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import random\n",
        "\n",
        "# 1. ë°ì´í„° ì¤€ë¹„ ë° ì „ì²˜ë¦¬\n",
        "class Vocabulary:\n",
        "    def __init__(self):\n",
        "        self.word2idx = {'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3}\n",
        "        self.idx2word = {0: '<PAD>', 1: '<SOS>', 2: '<EOS>', 3: '<UNK>'}\n",
        "        self.word_count = 4\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            self.word2idx[word] = self.word_count\n",
        "            self.idx2word[self.word_count] = word\n",
        "            self.word_count += 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.word_count\n",
        "\n",
        "def create_datasets():\n",
        "    \"\"\"ì˜ì–´-ë„¤ëœë€ë“œì–´ ë²ˆì—­ ë°ì´í„°ì…‹ ìƒì„±\"\"\"\n",
        "    # ê°„ë‹¨í•œ í›ˆë ¨ ë°ì´í„° (ì‹¤ì œë¡œëŠ” ë” ë§ì€ ë°ì´í„°ê°€ í•„ìš”)\n",
        "    english_sentences = [\n",
        "        \"I love llamas\",\n",
        "        \"I like cats\",\n",
        "        \"I eat apples\",\n",
        "        \"I drink water\",\n",
        "        \"I read books\",\n",
        "        \"I watch movies\",\n",
        "        \"I play games\",\n",
        "        \"I write code\",\n",
        "        \"I love programming\",\n",
        "        \"I study machine learning\"\n",
        "    ]\n",
        "\n",
        "    dutch_sentences = [\n",
        "        \"Ik hou van lama's\",\n",
        "        \"Ik hou van katten\",\n",
        "        \"Ik eet appels\",\n",
        "        \"Ik drink water\",\n",
        "        \"Ik lees boeken\",\n",
        "        \"Ik kijk naar films\",\n",
        "        \"Ik speel spelletjes\",\n",
        "        \"Ik schrijf code\",\n",
        "        \"Ik hou van programmeren\",\n",
        "        \"Ik studeer machine learning\"\n",
        "    ]\n",
        "\n",
        "    return english_sentences, dutch_sentences\n",
        "\n",
        "def preprocess_data(english_sentences, dutch_sentences):\n",
        "    \"\"\"ë°ì´í„° ì „ì²˜ë¦¬ ë° ì–´íœ˜ ì‚¬ì „ ìƒì„±\"\"\"\n",
        "    en_vocab = Vocabulary()\n",
        "    nl_vocab = Vocabulary()\n",
        "\n",
        "    # ì–´íœ˜ ì‚¬ì „ êµ¬ì¶•\n",
        "    for sentence in english_sentences:\n",
        "        for word in sentence.lower().split():\n",
        "            en_vocab.add_word(word)\n",
        "\n",
        "    for sentence in dutch_sentences:\n",
        "        for word in sentence.lower().split():\n",
        "            nl_vocab.add_word(word)\n",
        "\n",
        "    return en_vocab, nl_vocab\n",
        "\n",
        "def sentence_to_indices(sentence, vocab, max_len=10):\n",
        "    \"\"\"ë¬¸ì¥ì„ ì¸ë±ìŠ¤ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜\"\"\"\n",
        "    words = sentence.lower().split()\n",
        "    indices = [vocab.word2idx.get(word, vocab.word2idx['<UNK>']) for word in words]\n",
        "\n",
        "    # ìµœëŒ€ ê¸¸ì´ ì œí•œ (EOS í† í° ê³µê°„ í™•ë³´)\n",
        "    if len(indices) >= max_len:\n",
        "        indices = indices[:max_len-1]\n",
        "\n",
        "    # íŒ¨ë”© ì¶”ê°€\n",
        "    while len(indices) < max_len:\n",
        "        indices.append(vocab.word2idx['<PAD>'])\n",
        "\n",
        "    return indices\n",
        "\n",
        "# 2. ëª¨ë¸ ì •ì˜ (ê¸°ì¡´ ì½”ë“œ ê°œì„ )\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.dropout(self.embedding(x))\n",
        "        output, (hidden, cell) = self.lstm(embedded)\n",
        "        return hidden, cell\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x, hidden, cell):\n",
        "        embedded = self.dropout(self.embedding(x))\n",
        "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
        "        prediction = self.fc(output)\n",
        "        return prediction, hidden, cell\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, source, target, teacher_forcing_ratio=0.5):\n",
        "        batch_size = source.size(0)\n",
        "        target_len = target.size(1)\n",
        "        target_vocab_size = self.decoder.fc.out_features\n",
        "\n",
        "        # ì¸ì½”ë”©\n",
        "        hidden, cell = self.encoder(source)\n",
        "\n",
        "        # ë””ì½”ë”© ê²°ê³¼ ì €ì¥\n",
        "        outputs = torch.zeros(batch_size, target_len, target_vocab_size)\n",
        "\n",
        "        # ì²« ë²ˆì§¸ ì…ë ¥ì€ <SOS> í† í°\n",
        "        input_token = target[:, 0:1]\n",
        "\n",
        "        for i in range(1, target_len):\n",
        "            output, hidden, cell = self.decoder(input_token, hidden, cell)\n",
        "            outputs[:, i:i+1] = output\n",
        "\n",
        "            # Teacher forcing ì ìš©\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.argmax(-1)\n",
        "            input_token = target[:, i:i+1] if teacher_force else top1\n",
        "\n",
        "        return outputs\n",
        "\n",
        "# 3. í›ˆë ¨ í•¨ìˆ˜\n",
        "def train_model(model, train_data, en_vocab, nl_vocab, num_epochs=100):\n",
        "    \"\"\"ëª¨ë¸ í›ˆë ¨\"\"\"\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=0)  # íŒ¨ë”© ë¬´ì‹œ\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    model.train()\n",
        "    max_len = 10\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "\n",
        "        for en_sentence, nl_sentence in train_data:\n",
        "            # ë°ì´í„° ì¤€ë¹„ - ê¸¸ì´ í†µì¼\n",
        "            en_indices = sentence_to_indices(en_sentence, en_vocab, max_len)\n",
        "            nl_words = sentence_to_indices(nl_sentence, nl_vocab, max_len-1)  # SOS/EOS ê³µê°„ í™•ë³´\n",
        "\n",
        "            # ë””ì½”ë” ì…ë ¥: <SOS> + ë¬¸ì¥ (ë§ˆì§€ë§‰ ì œì™¸)\n",
        "            nl_input = [nl_vocab.word2idx['<SOS>']] + nl_words[:-1]\n",
        "            # íƒ€ê²Ÿ: ë¬¸ì¥ + <EOS>\n",
        "            nl_target = nl_words + [nl_vocab.word2idx['<EOS>']]\n",
        "\n",
        "            # ê¸¸ì´ ë§ì¶”ê¸°\n",
        "            nl_input = nl_input[:max_len]\n",
        "            nl_target = nl_target[:max_len]\n",
        "\n",
        "            # íŒ¨ë”©ìœ¼ë¡œ ê¸¸ì´ í†µì¼\n",
        "            while len(nl_input) < max_len:\n",
        "                nl_input.append(nl_vocab.word2idx['<PAD>'])\n",
        "            while len(nl_target) < max_len:\n",
        "                nl_target.append(nl_vocab.word2idx['<PAD>'])\n",
        "\n",
        "            en_tensor = torch.LongTensor([en_indices])\n",
        "            nl_tensor = torch.LongTensor([nl_input])\n",
        "            target_tensor = torch.LongTensor([nl_target])\n",
        "\n",
        "            # ìˆœì „íŒŒ\n",
        "            optimizer.zero_grad()\n",
        "            output = model(en_tensor, nl_tensor)\n",
        "\n",
        "            # ì†ì‹¤ ê³„ì‚° - ì°¨ì› ë§ì¶”ê¸°\n",
        "            output = output.reshape(-1, output.shape[-1])  # [batch*seq, vocab]\n",
        "            target_tensor = target_tensor.reshape(-1)       # [batch*seq]\n",
        "            loss = criterion(output, target_tensor)\n",
        "\n",
        "            # ì—­ì „íŒŒ\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        if (epoch + 1) % 20 == 0:\n",
        "            avg_loss = total_loss / len(train_data)\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
        "\n",
        "# 4. ë²ˆì—­ í•¨ìˆ˜\n",
        "def translate(model, sentence, en_vocab, nl_vocab, max_len=10):\n",
        "    \"\"\"ë¬¸ì¥ ë²ˆì—­\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # ì…ë ¥ ë¬¸ì¥ ì „ì²˜ë¦¬\n",
        "        en_indices = sentence_to_indices(sentence, en_vocab)\n",
        "        en_tensor = torch.LongTensor([en_indices])\n",
        "\n",
        "        # ì¸ì½”ë”©\n",
        "        hidden, cell = model.encoder(en_tensor)\n",
        "\n",
        "        # ë””ì½”ë”©\n",
        "        result = []\n",
        "        input_token = torch.LongTensor([[nl_vocab.word2idx['<SOS>']]])\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            output, hidden, cell = model.decoder(input_token, hidden, cell)\n",
        "            predicted_id = output.argmax(-1).item()\n",
        "\n",
        "            if predicted_id == nl_vocab.word2idx['<EOS>']:\n",
        "                break\n",
        "\n",
        "            if predicted_id != nl_vocab.word2idx['<PAD>']:\n",
        "                word = nl_vocab.idx2word[predicted_id]\n",
        "                result.append(word)\n",
        "\n",
        "            input_token = torch.LongTensor([[predicted_id]])\n",
        "\n",
        "        return ' '.join(result)\n",
        "\n",
        "# 5. ë©”ì¸ ì‹¤í–‰ ì½”ë“œ\n",
        "def main():\n",
        "    \"\"\"ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜\"\"\"\n",
        "    print(\"=\" * 50)\n",
        "    print(\"ì˜ì–´-ë„¤ëœë€ë“œì–´ Seq2Seq ë²ˆì—­ ëª¨ë¸\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # ë°ì´í„° ì¤€ë¹„\n",
        "    english_sentences, dutch_sentences = create_datasets()\n",
        "    en_vocab, nl_vocab = preprocess_data(english_sentences, dutch_sentences)\n",
        "\n",
        "    print(f\"ì˜ì–´ ì–´íœ˜ ìˆ˜: {len(en_vocab)}\")\n",
        "    print(f\"ë„¤ëœë€ë“œì–´ ì–´íœ˜ ìˆ˜: {len(nl_vocab)}\")\n",
        "    print()\n",
        "\n",
        "    # í›ˆë ¨ ë°ì´í„°\n",
        "    train_data = list(zip(english_sentences, dutch_sentences))\n",
        "\n",
        "    # ëª¨ë¸ ìƒì„±\n",
        "    embed_size = 128\n",
        "    hidden_size = 256\n",
        "\n",
        "    encoder = Encoder(len(en_vocab), embed_size, hidden_size)\n",
        "    decoder = Decoder(len(nl_vocab), embed_size, hidden_size)\n",
        "    model = Seq2Seq(encoder, decoder)\n",
        "\n",
        "    print(\"ëª¨ë¸ êµ¬ì¡°:\")\n",
        "    print(f\"Encoder: ì–´íœ˜ í¬ê¸° {len(en_vocab)}, ì„ë² ë”© {embed_size}, ì€ë‹‰ {hidden_size}\")\n",
        "    print(f\"Decoder: ì–´íœ˜ í¬ê¸° {len(nl_vocab)}, ì„ë² ë”© {embed_size}, ì€ë‹‰ {hidden_size}\")\n",
        "    print()\n",
        "\n",
        "    # ëª¨ë¸ í›ˆë ¨\n",
        "    print(\"ëª¨ë¸ í›ˆë ¨ ì‹œì‘...\")\n",
        "    train_model(model, train_data, en_vocab, nl_vocab, num_epochs=200)\n",
        "    print()\n",
        "\n",
        "    # ë²ˆì—­ í…ŒìŠ¤íŠ¸\n",
        "    print(\"ë²ˆì—­ í…ŒìŠ¤íŠ¸:\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    test_sentences = [\n",
        "        \"I love llamas\",\n",
        "        \"I like cats\",\n",
        "        \"I eat apples\",\n",
        "        \"I drink water\"\n",
        "    ]\n",
        "\n",
        "    for sentence in test_sentences:\n",
        "        translation = translate(model, sentence, en_vocab, nl_vocab)\n",
        "        print(f\"ì˜ì–´: {sentence}\")\n",
        "        print(f\"ë„¤ëœë€ë“œì–´: {translation}\")\n",
        "        print()\n",
        "\n",
        "    # ìƒˆë¡œìš´ ë¬¸ì¥ ë²ˆì—­ (ëª©í‘œ ë¬¸ì¥)\n",
        "    print(\"=\" * 30)\n",
        "    print(\"ëª©í‘œ ë²ˆì—­:\")\n",
        "    target_sentence = \"I love llamas\"\n",
        "    translation = translate(model, target_sentence, en_vocab, nl_vocab)\n",
        "    print(f\"ì˜ì–´: {target_sentence}\")\n",
        "    print(f\"ë„¤ëœë€ë“œì–´ ë²ˆì—­: {translation}\")\n",
        "    print(f\"ì •ë‹µ: Ik hou van lama's\")\n",
        "\n",
        "# 6. ëª¨ë¸ ì„±ëŠ¥ ë¶„ì„\n",
        "def analyze_model_performance():\n",
        "    \"\"\"ëª¨ë¸ ì„±ëŠ¥ ë¶„ì„\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 40)\n",
        "    print(\"ëª¨ë¸ ì„±ëŠ¥ ë¶„ì„\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    print(\"1. Seq2Seq ëª¨ë¸ì˜ íŠ¹ì§•:\")\n",
        "    print(\"   - ì¸ì½”ë”: ì…ë ¥ ë¬¸ì¥ì„ ê³ ì • í¬ê¸° ë²¡í„°ë¡œ ì••ì¶•\")\n",
        "    print(\"   - ë””ì½”ë”: ì••ì¶•ëœ ë²¡í„°ë¡œë¶€í„° ì¶œë ¥ ë¬¸ì¥ ìƒì„±\")\n",
        "    print(\"   - Teacher Forcing: í›ˆë ¨ ì‹œ ì •ë‹µì„ ë‹¤ìŒ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©\")\n",
        "\n",
        "    print(\"\\n2. í•œê³„ì :\")\n",
        "    print(\"   - ê¸´ ë¬¸ì¥ì—ì„œ ì •ë³´ ì†ì‹¤ ë°œìƒ ê°€ëŠ¥\")\n",
        "    print(\"   - ì…ë ¥ ë¬¸ì¥ì˜ ëª¨ë“  ì •ë³´ë¥¼ í•˜ë‚˜ì˜ ë²¡í„°ì— ì••ì¶•\")\n",
        "    print(\"   - ë‹¨ì–´ ìˆœì„œê°€ ë‹¤ë¥¸ ì–¸ì–´ìŒì—ì„œ ì„±ëŠ¥ ì œí•œ\")\n",
        "\n",
        "    print(\"\\n3. ê°œì„  ë°©ì•ˆ:\")\n",
        "    print(\"   - Attention ë©”ì»¤ë‹ˆì¦˜ ì¶”ê°€\")\n",
        "    print(\"   - ë” ë§ì€ í›ˆë ¨ ë°ì´í„° ì‚¬ìš©\")\n",
        "    print(\"   - Transformer ëª¨ë¸ ë„ì…\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "    analyze_model_performance()"
      ],
      "metadata": {
        "id": "QcXuZ49lYdq8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aec6524b-a220-421a-85e0-8da642c609d4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "ì˜ì–´-ë„¤ëœë€ë“œì–´ Seq2Seq ë²ˆì—­ ëª¨ë¸\n",
            "==================================================\n",
            "ì˜ì–´ ì–´íœ˜ ìˆ˜: 25\n",
            "ë„¤ëœë€ë“œì–´ ì–´íœ˜ ìˆ˜: 26\n",
            "\n",
            "ëª¨ë¸ êµ¬ì¡°:\n",
            "Encoder: ì–´íœ˜ í¬ê¸° 25, ì„ë² ë”© 128, ì€ë‹‰ 256\n",
            "Decoder: ì–´íœ˜ í¬ê¸° 26, ì„ë² ë”© 128, ì€ë‹‰ 256\n",
            "\n",
            "ëª¨ë¸ í›ˆë ¨ ì‹œì‘...\n",
            "Epoch [20/200], Loss: 0.8251\n",
            "Epoch [40/200], Loss: 0.7602\n",
            "Epoch [60/200], Loss: 0.7377\n",
            "Epoch [80/200], Loss: 0.7356\n",
            "Epoch [100/200], Loss: 0.7348\n",
            "Epoch [120/200], Loss: 0.7342\n",
            "Epoch [140/200], Loss: 0.7339\n",
            "Epoch [160/200], Loss: 0.7337\n",
            "Epoch [180/200], Loss: 0.7336\n",
            "Epoch [200/200], Loss: 0.7335\n",
            "\n",
            "ë²ˆì—­ í…ŒìŠ¤íŠ¸:\n",
            "------------------------------\n",
            "ì˜ì–´: I love llamas\n",
            "ë„¤ëœë€ë“œì–´: hou van lama's lama's lama's lama's\n",
            "\n",
            "ì˜ì–´: I like cats\n",
            "ë„¤ëœë€ë“œì–´: hou van katten katten katten\n",
            "\n",
            "ì˜ì–´: I eat apples\n",
            "ë„¤ëœë€ë“œì–´: eet appels appels appels films\n",
            "\n",
            "ì˜ì–´: I drink water\n",
            "ë„¤ëœë€ë“œì–´: drink water water water\n",
            "\n",
            "==============================\n",
            "ëª©í‘œ ë²ˆì—­:\n",
            "ì˜ì–´: I love llamas\n",
            "ë„¤ëœë€ë“œì–´ ë²ˆì—­: hou van lama's lama's lama's lama's\n",
            "ì •ë‹µ: Ik hou van lama's\n",
            "\n",
            "========================================\n",
            "ëª¨ë¸ ì„±ëŠ¥ ë¶„ì„\n",
            "========================================\n",
            "1. Seq2Seq ëª¨ë¸ì˜ íŠ¹ì§•:\n",
            "   - ì¸ì½”ë”: ì…ë ¥ ë¬¸ì¥ì„ ê³ ì • í¬ê¸° ë²¡í„°ë¡œ ì••ì¶•\n",
            "   - ë””ì½”ë”: ì••ì¶•ëœ ë²¡í„°ë¡œë¶€í„° ì¶œë ¥ ë¬¸ì¥ ìƒì„±\n",
            "   - Teacher Forcing: í›ˆë ¨ ì‹œ ì •ë‹µì„ ë‹¤ìŒ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©\n",
            "\n",
            "2. í•œê³„ì :\n",
            "   - ê¸´ ë¬¸ì¥ì—ì„œ ì •ë³´ ì†ì‹¤ ë°œìƒ ê°€ëŠ¥\n",
            "   - ì…ë ¥ ë¬¸ì¥ì˜ ëª¨ë“  ì •ë³´ë¥¼ í•˜ë‚˜ì˜ ë²¡í„°ì— ì••ì¶•\n",
            "   - ë‹¨ì–´ ìˆœì„œê°€ ë‹¤ë¥¸ ì–¸ì–´ìŒì—ì„œ ì„±ëŠ¥ ì œí•œ\n",
            "\n",
            "3. ê°œì„  ë°©ì•ˆ:\n",
            "   - Attention ë©”ì»¤ë‹ˆì¦˜ ì¶”ê°€\n",
            "   - ë” ë§ì€ í›ˆë ¨ ë°ì´í„° ì‚¬ìš©\n",
            "   - Transformer ëª¨ë¸ ë„ì…\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ë¬¸ì œì \n",
        "    - ë„¤ëœë€ë“œì–´ ë²ˆì—­: hou van lama's lama's lama's lama's\n",
        "    - lama's ê°€ ë°˜ë³µì ìœ¼ë¡œ ë‚˜ì˜´.\n",
        "    - ì…ë ¥ ë¬¸ì¥ì„ ê³ ì • í¬ê¸° ë²¡í„°ì¸í•´ ì¥ê¸° ê¸°ì–µ ì†ì‹¤ì´ ë°œìƒí•¨\n",
        "    - Decoderê°€ \"I love llamas\"ê¹Œì§€ ë²ˆì—­í•œ í›„, \"ê·¸ë‹¤ìŒì—” ë­˜ ë²ˆì—­í•´ì•¼ í•˜ì§€?\"ë¼ê³  ë¬¸ë§¥ ë²¡í„°ë¥¼ ë´¤ì„ ë•Œ, ì´ë¯¸ ì •ë³´ê°€ ì†ì‹¤ë˜ì–´ \"ì›ë˜ ë¬¸ì¥ì´ ëë‚¬ëŠ”ì§€\", \"ë‹¤ìŒ ë‹¨ì–´ê°€ ë¬´ì—‡ì¸ì§€\" ì•Œ ìˆ˜ ì—†ê²Œ ë©ë‹ˆë‹¤.(ëª¨ë¸ì€ \"ë‚´ê°€ ë°©ê¸ˆ ì „ì— 'lama's'ë¥¼ ë§í–ˆë‹¤\"ëŠ” ì‚¬ì‹¤ì€ ê¸°ì–µí•˜ì§€ë§Œ, \"ë‚´ê°€ ì•„ì£¼ ì˜¤ë˜ì „ì— 'I love'ë¥¼ ë§í–ˆë‹¤\"ëŠ” ì‚¬ì‹¤ì€ ìŠì–´ë²„ë¦¬ê²Œ ë©ë‹ˆë‹¤.)\n"
      ],
      "metadata": {
        "id": "-Ly035xho3bt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "UA805xi7wZAx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Attention ì•„í‚¤í…ì²˜ ì ìš© : Seq2Seq + Attention**"
      ],
      "metadata": {
        "id": "GoCX3bMpc-YI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "|ê°œì„  ì˜ì—­|ê¸°ì¡´ Seq2Seq|Attention Seq2Seq|ê°œì„ íš¨ê³¼|\n",
        "|---|---|---|---|\n",
        "|ì¸ì½”ë”|ë‹¨ë°©í–¥ LSTM|ì–‘ë°©í–¥ LSTM|ë¬¸ë§¥ ì •ë³´ í’ë¶€í™”|\n",
        "|ì •ë³´ ì „ë‹¬|ë§ˆì§€ë§‰ ìƒíƒœë§Œ ì‚¬ìš©|ëª¨ë“  ìƒíƒœ í™œìš©|ì •ë³´ ì†ì‹¤ ë°©ì§€|\n",
        "|ë””ì½”ë”| ì…ë ¥ì´ì „ ì¶œë ¥ + ê³ ì • ì»¨í…ìŠ¤íŠ¸|ì´ì „ ì¶œë ¥ + ë™ì  ì»¨í…ìŠ¤íŠ¸|ì ì‘ì  ë²ˆì—­|\n",
        "|ë©”ëª¨ë¦¬|O(1) ì»¨í…ìŠ¤íŠ¸|O(n) ëª¨ë“  ìƒíƒœ ì €ì¥|ê¸´ ë¬¸ì¥ ì²˜ë¦¬ ê°€ëŠ¥|\n",
        "|í•´ì„ ê°€ëŠ¥ì„±|ë¸”ë™ë°•ìŠ¤|Attention ê°€ì¤‘ì¹˜ ì‹œê°í™”|ë²ˆì—­ ê³¼ì • ì´í•´|"
      ],
      "metadata": {
        "id": "B4CgvrvdeCGO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **ê¸°ì¡´ Seq2Seqì™€ ì°¨ì´ì (encoder, decoder, Seq2Seq Class)**\n",
        "    - **Encoder vs ImprovedEncode**r EncoderëŠ” ë§ˆì§€ë§‰ ì‹œì ì˜ ì€ë‹‰ ìƒíƒœ(context vector)ë§Œ ë°˜í™˜í•˜ëŠ” ë°˜ë©´, ImprovedEncoderëŠ” ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì— ì‚¬ìš©ë  ì¸ì½”ë”ì˜ **ëª¨ë“  ì‹œì  ì¶œë ¥ê°’**(outputs)ê³¼ **íŒ¨ë”© ë§ˆìŠ¤í¬**(mask)ë¥¼ ì¶”ê°€ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
        "    - **Decoder vs ImprovedDecode**r DecoderëŠ” LSTMì˜ ì€ë‹‰ ìƒíƒœë§Œìœ¼ë¡œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ì§€ë§Œ, ImprovedDecoderëŠ” **ì–´í…ì…˜*8(Attention)ì„ ë„ì…í•˜ì—¬ ì¸ì½”ë”ì˜ ëª¨ë“  ì¶œë ¥(context)ê³¼ í˜„ì¬ LSTM ì¶œë ¥ì„ **ê²°í•©**(concatenate)í•œ ë’¤ ìµœì¢… ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
        "    - **Seq2Seq vs ImprovedSeq2Seq** Seq2SeqëŠ” ì¸ì½”ë”ì˜ ë§ˆì§€ë§‰ ì€ë‹‰ ìƒíƒœë§Œ ë””ì½”ë”ì˜ ì´ˆê¸° ìƒíƒœë¡œ ì „ë‹¬í•˜ëŠ” ë°˜ë©´, ImprovedSeq2SeqëŠ” ì–´í…ì…˜ì„ ìœ„í•´ ì¸ì½”ë”ì˜ ëª¨ë“  ì¶œë ¥(encoder_outputs)ê³¼ ë§ˆìŠ¤í¬ë¥¼ ë””ì½”ë”ì˜ ë§¤ ì‹œì ì— ì „ë‹¬í•˜ë©°, **ì–´í…ì…˜ ê°€ì¤‘ì¹˜(attention_weights)ë¥¼ í•¨ê»˜ ë°˜í™˜**í•©ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "DHW9By58rsbS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import random\n",
        "\n",
        "# ì‹œë“œ ì„¤ì •ìœ¼ë¡œ ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼ ë³´ì¥\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# 1. ë°ì´í„° ì¤€ë¹„ ë° ì „ì²˜ë¦¬\n",
        "class Vocabulary:\n",
        "    def __init__(self):\n",
        "        self.word2idx = {'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3}\n",
        "        self.idx2word = {0: '<PAD>', 1: '<SOS>', 2: '<EOS>', 3: '<UNK>'}\n",
        "        self.word_count = 4\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            self.word2idx[word] = self.word_count\n",
        "            self.idx2word[self.word_count] = word\n",
        "            self.word_count += 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.word_count\n",
        "\n",
        "def create_datasets():\n",
        "    \"\"\"ì˜ì–´-ë„¤ëœë€ë“œì–´ ë²ˆì—­ ë°ì´í„°ì…‹ ìƒì„±\"\"\"\n",
        "    english_sentences = [\n",
        "        \"I love llamas\",\n",
        "        \"I like cats\",\n",
        "        \"I eat apples\",\n",
        "        \"I drink water\",\n",
        "        \"I read books\",\n",
        "        \"I watch movies\",\n",
        "        \"I play games\",\n",
        "        \"I write code\",\n",
        "        \"I love programming\",\n",
        "        \"I study machine learning\"\n",
        "    ]\n",
        "\n",
        "    dutch_sentences = [\n",
        "        \"Ik hou van lama's\",\n",
        "        \"Ik hou van katten\",\n",
        "        \"Ik eet appels\",\n",
        "        \"Ik drink water\",\n",
        "        \"Ik lees boeken\",\n",
        "        \"Ik kijk naar films\",\n",
        "        \"Ik speel spelletjes\",\n",
        "        \"Ik schrijf code\",\n",
        "        \"Ik hou van programmeren\",\n",
        "        \"Ik studeer machine learning\"\n",
        "    ]\n",
        "\n",
        "    return english_sentences, dutch_sentences\n",
        "\n",
        "def preprocess_data(english_sentences, dutch_sentences):\n",
        "    \"\"\"ë°ì´í„° ì „ì²˜ë¦¬ ë° ì–´íœ˜ ì‚¬ì „ ìƒì„±\"\"\"\n",
        "    en_vocab = Vocabulary()\n",
        "    nl_vocab = Vocabulary()\n",
        "\n",
        "    for sentence in english_sentences:\n",
        "        for word in sentence.lower().split():\n",
        "            en_vocab.add_word(word)\n",
        "\n",
        "    for sentence in dutch_sentences:\n",
        "        for word in sentence.lower().split():\n",
        "            nl_vocab.add_word(word)\n",
        "\n",
        "    return en_vocab, nl_vocab\n",
        "\n",
        "def sentence_to_indices(sentence, vocab, max_len=10):\n",
        "    \"\"\"ë¬¸ì¥ì„ ì¸ë±ìŠ¤ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜\"\"\"\n",
        "    words = sentence.lower().split()\n",
        "    indices = [vocab.word2idx.get(word, vocab.word2idx['<UNK>']) for word in words]\n",
        "\n",
        "    if len(indices) >= max_len:\n",
        "        indices = indices[:max_len-1]\n",
        "\n",
        "    while len(indices) < max_len:\n",
        "        indices.append(vocab.word2idx['<PAD>'])\n",
        "\n",
        "    return indices\n",
        "\n",
        "\n",
        "# 2. ìˆ˜ì •ëœ Attention ë©”ì»¤ë‹ˆì¦˜\n",
        "class ImprovedAttention(nn.Module):\n",
        "    \"\"\"ê°œì„ ëœ Dot-Product Attention (Luong-style),\n",
        "        - ì´ˆê¸° ì´ˆê¸° RNN ê¸°ë°˜ Seq2Seq ëª¨ë¸ì˜ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ ì¢…ë¥˜, Bahdanau-style\"\"\"\n",
        "\n",
        "    def __init__(self, hidden_size):\n",
        "        super(ImprovedAttention, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Luong Attention - ë” ê°„ë‹¨í•˜ê³  íš¨ê³¼ì \n",
        "        self.W_a = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "\n",
        "    def forward(self, decoder_hidden, encoder_outputs, mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            decoder_hidden: [batch_size, hidden_size] - í˜„ì¬ ë””ì½”ë” ìƒíƒœ\n",
        "            encoder_outputs: [batch_size, seq_len, hidden_size] - ì¸ì½”ë” ëª¨ë“  ì¶œë ¥\n",
        "            mask: [batch_size, seq_len] - íŒ¨ë”© ë§ˆìŠ¤í¬\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, hidden_size = encoder_outputs.size()\n",
        "\n",
        "        # Luong Attention: score = decoder_hidden^T * W_a * encoder_outputs\n",
        "        # decoder_hidden: [batch_size, hidden_size]\n",
        "        # W_a(encoder_outputs): [batch_size, seq_len, hidden_size]\n",
        "\n",
        "        transformed_encoder = self.W_a(encoder_outputs)  # [batch_size, seq_len, hidden_size]\n",
        "        decoder_hidden = decoder_hidden.unsqueeze(2)     # [batch_size, hidden_size, 1]\n",
        "\n",
        "        # ë°°ì¹˜ í–‰ë ¬ ê³±ì…ˆìœ¼ë¡œ attention score ê³„ì‚°\n",
        "        attention_scores = torch.bmm(transformed_encoder, decoder_hidden)  # [batch_size, seq_len, 1]\n",
        "        attention_scores = attention_scores.squeeze(2)  # [batch_size, seq_len]\n",
        "\n",
        "        # íŒ¨ë”© ë§ˆìŠ¤í‚¹\n",
        "        if mask is not None:\n",
        "            attention_scores = attention_scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        # Softmaxë¡œ ì •ê·œí™”\n",
        "        attention_weights = F.softmax(attention_scores, dim=1)  # [batch_size, seq_len]\n",
        "\n",
        "        # Context vector ê³„ì‚°\n",
        "        context_vector = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)  # [batch_size, 1, hidden_size]\n",
        "        context_vector = context_vector.squeeze(1)  # [batch_size, hidden_size]\n",
        "\n",
        "        return attention_weights, context_vector\n",
        "\n",
        "# 3. ê°œì„ ëœ Encoder\n",
        "class ImprovedEncoder(nn.Module):\n",
        "    \"\"\"ê°œì„ ëœ ì¸ì½”ë”\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=1, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # ë‹¨ë°©í–¥ LSTM ì‚¬ìš© (ë” ì•ˆì •ì )\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers,\n",
        "                           batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # íŒ¨ë”© ë§ˆìŠ¤í¬ ìƒì„±\n",
        "        mask = (x != 0).float()  # [batch_size, seq_len]\n",
        "\n",
        "        embedded = self.dropout(self.embedding(x))  # [batch_size, seq_len, embed_size]\n",
        "\n",
        "        # LSTM forward pass\n",
        "        outputs, (hidden, cell) = self.lstm(embedded)\n",
        "        # outputs: [batch_size, seq_len, hidden_size]\n",
        "        # hidden: [num_layers, batch_size, hidden_size]\n",
        "\n",
        "        return outputs, (hidden, cell), mask\n",
        "\n",
        "# 4. ê°œì„ ëœ Decoder\n",
        "class ImprovedDecoder(nn.Module):\n",
        "    \"\"\"ê°œì„ ëœ ë””ì½”ë”\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=1, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
        "        self.attention = ImprovedAttention(hidden_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # LSTM: embeddingë§Œ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers,\n",
        "                           batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
        "\n",
        "        # ì¶œë ¥ì¸µ: LSTM output + context vector ê²°í•©\n",
        "        self.output_projection = nn.Linear(hidden_size * 2, vocab_size)\n",
        "\n",
        "    def forward(self, input_token, hidden, cell, encoder_outputs, encoder_mask):\n",
        "        batch_size = input_token.size(0)\n",
        "\n",
        "        # ì„ë² ë”©\n",
        "        embedded = self.dropout(self.embedding(input_token))  # [batch_size, 1, embed_size]\n",
        "\n",
        "        # LSTM\n",
        "        lstm_output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
        "        # lstm_output: [batch_size, 1, hidden_size]\n",
        "\n",
        "        # Attention ê³„ì‚°\n",
        "        decoder_hidden = lstm_output.squeeze(1)  # [batch_size, hidden_size]\n",
        "        attention_weights, context_vector = self.attention(\n",
        "            decoder_hidden, encoder_outputs, encoder_mask\n",
        "        )\n",
        "\n",
        "        # LSTM ì¶œë ¥ê³¼ ì»¨í…ìŠ¤íŠ¸ ë²¡í„° ê²°í•©\n",
        "        combined = torch.cat([decoder_hidden, context_vector], dim=1)  # [batch_size, hidden_size * 2]\n",
        "\n",
        "        # ìµœì¢… ì¶œë ¥\n",
        "        output = self.output_projection(combined)  # [batch_size, vocab_size]\n",
        "        output = output.unsqueeze(1)  # [batch_size, 1, vocab_size]\n",
        "\n",
        "        return output, hidden, cell, attention_weights\n",
        "\n",
        "# 5. ê°œì„ ëœ Seq2Seq ëª¨ë¸\n",
        "class ImprovedSeq2Seq(nn.Module):\n",
        "    \"\"\"ê°œì„ ëœ Seq2Seq ëª¨ë¸\"\"\"\n",
        "\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "        # ì¸ì½”ë”ì™€ ë””ì½”ë”ì˜ ì€ë‹‰ ìƒíƒœ í¬ê¸°ê°€ ë‹¤ë¥¼ ê²½ìš°ë¥¼ ìœ„í•œ ë³€í™˜ì¸µ\n",
        "        if encoder.hidden_size != decoder.hidden_size:\n",
        "            self.hidden_projection = nn.Linear(encoder.hidden_size, decoder.hidden_size)\n",
        "            self.cell_projection = nn.Linear(encoder.hidden_size, decoder.hidden_size)\n",
        "        else:\n",
        "            self.hidden_projection = None\n",
        "            self.cell_projection = None\n",
        "\n",
        "    def forward(self, source, target, teacher_forcing_ratio=0.8):\n",
        "        batch_size = source.size(0)\n",
        "        target_len = target.size(1)\n",
        "        target_vocab_size = self.decoder.vocab_size\n",
        "\n",
        "        # ì¸ì½”ë”©\n",
        "        encoder_outputs, (hidden, cell), encoder_mask = self.encoder(source)\n",
        "\n",
        "        # ì¸ì½”ë”ì˜ ë§ˆì§€ë§‰ ì€ë‹‰ ìƒíƒœë¥¼ ë””ì½”ë” ì´ˆê¸° ìƒíƒœë¡œ ì‚¬ìš©\n",
        "        if self.hidden_projection is not None:\n",
        "            hidden = self.hidden_projection(hidden)\n",
        "            cell = self.cell_projection(cell)\n",
        "\n",
        "        # ê²°ê³¼ ì €ì¥\n",
        "        outputs = torch.zeros(batch_size, target_len, target_vocab_size)\n",
        "        attention_weights_list = []\n",
        "\n",
        "        # ì²« ë²ˆì§¸ ì…ë ¥ì€ <SOS> í† í°\n",
        "        input_token = target[:, 0:1]\n",
        "\n",
        "        for i in range(1, target_len):\n",
        "            output, hidden, cell, attention_weights = self.decoder(\n",
        "                input_token, hidden, cell, encoder_outputs, encoder_mask\n",
        "            )\n",
        "\n",
        "            outputs[:, i:i+1] = output\n",
        "            attention_weights_list.append(attention_weights)\n",
        "\n",
        "            # Teacher forcing\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.argmax(-1)\n",
        "            input_token = target[:, i:i+1] if teacher_force else top1\n",
        "\n",
        "        return outputs, attention_weights_list\n",
        "\n",
        "# 6. ê°œì„ ëœ í›ˆë ¨ í•¨ìˆ˜\n",
        "def train_improved_model(model, train_data, en_vocab, nl_vocab, num_epochs=200):\n",
        "    \"\"\"ê°œì„ ëœ ëª¨ë¸ í›ˆë ¨\"\"\"\n",
        "    # ë” ì ê·¹ì ì¸ í•™ìŠµë¥ ê³¼ ìŠ¤ì¼€ì¤„ë§\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=0, label_smoothing=0.1)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=0.002, weight_decay=0.01)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "    model.train()\n",
        "    max_len = 10\n",
        "\n",
        "    # í›ˆë ¨ ë£¨í”„\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        epoch_teacher_forcing = 0.9 - (epoch / num_epochs) * 0.3  # ì ì§„ì ìœ¼ë¡œ ê°ì†Œ\n",
        "\n",
        "        # ë°ì´í„° ì…”í”Œ\n",
        "        shuffled_data = train_data.copy()\n",
        "        random.shuffle(shuffled_data)\n",
        "\n",
        "        for en_sentence, nl_sentence in shuffled_data:\n",
        "            # ë°ì´í„° ì¤€ë¹„\n",
        "            en_indices = sentence_to_indices(en_sentence, en_vocab, max_len)\n",
        "            nl_words = sentence_to_indices(nl_sentence, nl_vocab, max_len-1)\n",
        "\n",
        "            nl_input = [nl_vocab.word2idx['<SOS>']] + nl_words[:-1]\n",
        "            nl_target = nl_words + [nl_vocab.word2idx['<EOS>']]\n",
        "\n",
        "            # ê¸¸ì´ ì¡°ì •\n",
        "            nl_input = nl_input[:max_len]\n",
        "            nl_target = nl_target[:max_len]\n",
        "\n",
        "            while len(nl_input) < max_len:\n",
        "                nl_input.append(nl_vocab.word2idx['<PAD>'])\n",
        "            while len(nl_target) < max_len:\n",
        "                nl_target.append(nl_vocab.word2idx['<PAD>'])\n",
        "\n",
        "            en_tensor = torch.LongTensor([en_indices])\n",
        "            nl_tensor = torch.LongTensor([nl_input])\n",
        "            target_tensor = torch.LongTensor([nl_target])\n",
        "\n",
        "            # ìˆœì „íŒŒ\n",
        "            optimizer.zero_grad()\n",
        "            output, attention_weights = model(en_tensor, nl_tensor, epoch_teacher_forcing)\n",
        "\n",
        "            # ì†ì‹¤ ê³„ì‚°\n",
        "            output = output.reshape(-1, output.shape[-1])\n",
        "            target_tensor = target_tensor.reshape(-1)\n",
        "            loss = criterion(output, target_tensor)\n",
        "\n",
        "            # ì—­ì „íŒŒ\n",
        "            loss.backward()\n",
        "\n",
        "            # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
        "\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        if (epoch + 1) % 25 == 0:\n",
        "            avg_loss = total_loss / len(train_data)\n",
        "            current_lr = scheduler.get_last_lr()[0]\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, LR: {current_lr:.6f}')\n",
        "\n",
        "# 7. ë²ˆì—­ ë° ì‹œê°í™” í•¨ìˆ˜ (ë™ì¼)\n",
        "def translate_improved(model, sentence, en_vocab, nl_vocab, max_len=10):\n",
        "    \"\"\"ê°œì„ ëœ ëª¨ë¸ë¡œ ë²ˆì—­\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        en_indices = sentence_to_indices(sentence, en_vocab, max_len)\n",
        "        en_tensor = torch.LongTensor([en_indices])\n",
        "\n",
        "        encoder_outputs, (hidden, cell), encoder_mask = model.encoder(en_tensor)\n",
        "\n",
        "        # ì€ë‹‰ ìƒíƒœ ë³€í™˜\n",
        "        if model.hidden_projection is not None:\n",
        "            hidden = model.hidden_projection(hidden)\n",
        "            cell = model.cell_projection(cell)\n",
        "\n",
        "        result = []\n",
        "        input_token = torch.LongTensor([[nl_vocab.word2idx['<SOS>']]])\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            output, hidden, cell, attention_weights = model.decoder(\n",
        "                input_token, hidden, cell, encoder_outputs, encoder_mask\n",
        "            )\n",
        "            predicted_id = output.argmax(-1).item()\n",
        "\n",
        "            if predicted_id == nl_vocab.word2idx['<EOS>']:\n",
        "                break\n",
        "            if predicted_id == nl_vocab.word2idx['<PAD>']:\n",
        "                break\n",
        "\n",
        "            word = nl_vocab.idx2word[predicted_id]\n",
        "            result.append(word)\n",
        "            input_token = torch.LongTensor([[predicted_id]])\n",
        "\n",
        "        return ' '.join(result)\n",
        "\n",
        "def get_attention_weights(model, sentence, en_vocab, nl_vocab, max_len=10):\n",
        "    \"\"\"Attention ê°€ì¤‘ì¹˜ ë°˜í™˜ (ì‹œê°í™” ì—†ì´)\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        en_indices = sentence_to_indices(sentence, en_vocab, max_len)\n",
        "        en_tensor = torch.LongTensor([en_indices])\n",
        "\n",
        "        encoder_outputs, (hidden, cell), encoder_mask = model.encoder(en_tensor)\n",
        "\n",
        "        if model.hidden_projection is not None:\n",
        "            hidden = model.hidden_projection(hidden)\n",
        "            cell = model.cell_projection(cell)\n",
        "\n",
        "        result_words = []\n",
        "        attention_matrix = []\n",
        "        input_token = torch.LongTensor([[nl_vocab.word2idx['<SOS>']]])\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            output, hidden, cell, attention_weights = model.decoder(\n",
        "                input_token, hidden, cell, encoder_outputs, encoder_mask\n",
        "            )\n",
        "\n",
        "            predicted_id = output.argmax(-1).item()\n",
        "\n",
        "            if predicted_id == nl_vocab.word2idx['<EOS>']:\n",
        "                break\n",
        "            if predicted_id == nl_vocab.word2idx['<PAD>']:\n",
        "                break\n",
        "\n",
        "            word = nl_vocab.idx2word[predicted_id]\n",
        "            result_words.append(word)\n",
        "            attention_matrix.append(attention_weights.squeeze().cpu().numpy())\n",
        "\n",
        "            input_token = torch.LongTensor([[predicted_id]])\n",
        "\n",
        "        # ì…ë ¥ ë‹¨ì–´ë“¤\n",
        "        input_words = []\n",
        "        for idx in en_indices:\n",
        "            if idx == en_vocab.word2idx['<PAD>']:\n",
        "                break\n",
        "            input_words.append(en_vocab.idx2word[idx])\n",
        "\n",
        "        return ' '.join(result_words), attention_matrix, input_words, result_words\n",
        "\n",
        "# 8. ì„±ëŠ¥ ë¹„êµë¥¼ ìœ„í•œ ê¸°ë³¸ Seq2Seq ëª¨ë¸\n",
        "class BasicSeq2Seq(nn.Module):\n",
        "    \"\"\"ê¸°ë³¸ Seq2Seq ëª¨ë¸ (ë¹„êµìš©)\"\"\"\n",
        "\n",
        "    def __init__(self, input_vocab_size, output_vocab_size, embed_size, hidden_size):\n",
        "        super().__init__()\n",
        "\n",
        "        # ì¸ì½”ë”\n",
        "        self.encoder_embedding = nn.Embedding(input_vocab_size, embed_size, padding_idx=0)\n",
        "        self.encoder_lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
        "\n",
        "        # ë””ì½”ë”\n",
        "        self.decoder_embedding = nn.Embedding(output_vocab_size, embed_size, padding_idx=0)\n",
        "        self.decoder_lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
        "        self.output_projection = nn.Linear(hidden_size, output_vocab_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, source, target, teacher_forcing_ratio=0.8):\n",
        "        batch_size = source.size(0)\n",
        "        target_len = target.size(1)\n",
        "        vocab_size = self.output_projection.out_features\n",
        "\n",
        "        # ì¸ì½”ë”©\n",
        "        embedded_src = self.dropout(self.encoder_embedding(source))\n",
        "        _, (hidden, cell) = self.encoder_lstm(embedded_src)\n",
        "\n",
        "        # ë””ì½”ë”©\n",
        "        outputs = torch.zeros(batch_size, target_len, vocab_size)\n",
        "        input_token = target[:, 0:1]\n",
        "\n",
        "        for i in range(1, target_len):\n",
        "            embedded_tgt = self.dropout(self.decoder_embedding(input_token))\n",
        "            output, (hidden, cell) = self.decoder_lstm(embedded_tgt, (hidden, cell))\n",
        "            prediction = self.output_projection(output)\n",
        "            outputs[:, i:i+1] = prediction\n",
        "\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = prediction.argmax(-1)\n",
        "            input_token = target[:, i:i+1] if teacher_force else top1\n",
        "\n",
        "        return outputs\n",
        "\n",
        "def train_basic_model(model, train_data, en_vocab, nl_vocab, num_epochs=200):\n",
        "    \"\"\"ê¸°ë³¸ ëª¨ë¸ í›ˆë ¨\"\"\"\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    model.train()\n",
        "    max_len = 10\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "\n",
        "        for en_sentence, nl_sentence in train_data:\n",
        "            en_indices = sentence_to_indices(en_sentence, en_vocab, max_len)\n",
        "            nl_words = sentence_to_indices(nl_sentence, nl_vocab, max_len-1)\n",
        "\n",
        "            nl_input = [nl_vocab.word2idx['<SOS>']] + nl_words[:-1]\n",
        "            nl_target = nl_words + [nl_vocab.word2idx['<EOS>']]\n",
        "\n",
        "            nl_input = nl_input[:max_len]\n",
        "            nl_target = nl_target[:max_len]\n",
        "\n",
        "            while len(nl_input) < max_len:\n",
        "                nl_input.append(nl_vocab.word2idx['<PAD>'])\n",
        "            while len(nl_target) < max_len:\n",
        "                nl_target.append(nl_vocab.word2idx['<PAD>'])\n",
        "\n",
        "            en_tensor = torch.LongTensor([en_indices])\n",
        "            nl_tensor = torch.LongTensor([nl_input])\n",
        "            target_tensor = torch.LongTensor([nl_target])\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(en_tensor, nl_tensor)\n",
        "\n",
        "            output = output.reshape(-1, output.shape[-1])\n",
        "            target_tensor = target_tensor.reshape(-1)\n",
        "            loss = criterion(output, target_tensor)\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        if (epoch + 1) % 50 == 0:\n",
        "            avg_loss = total_loss / len(train_data)\n",
        "            print(f'Basic Model - Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
        "\n",
        "def translate_basic(model, sentence, en_vocab, nl_vocab, max_len=10):\n",
        "    \"\"\"ê¸°ë³¸ ëª¨ë¸ë¡œ ë²ˆì—­\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        en_indices = sentence_to_indices(sentence, en_vocab, max_len)\n",
        "        en_tensor = torch.LongTensor([en_indices])\n",
        "\n",
        "        embedded_src = model.encoder_embedding(en_tensor)\n",
        "        _, (hidden, cell) = model.encoder_lstm(embedded_src)\n",
        "\n",
        "        result = []\n",
        "        input_token = torch.LongTensor([[nl_vocab.word2idx['<SOS>']]])\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            embedded_tgt = model.decoder_embedding(input_token)\n",
        "            output, (hidden, cell) = model.decoder_lstm(embedded_tgt, (hidden, cell))\n",
        "            prediction = model.output_projection(output)\n",
        "            predicted_id = prediction.argmax(-1).item()\n",
        "\n",
        "            if predicted_id == nl_vocab.word2idx['<EOS>']:\n",
        "                break\n",
        "            if predicted_id == nl_vocab.word2idx['<PAD>']:\n",
        "                break\n",
        "\n",
        "            word = nl_vocab.idx2word[predicted_id]\n",
        "            result.append(word)\n",
        "            input_token = torch.LongTensor([[predicted_id]])\n",
        "\n",
        "        return ' '.join(result)\n",
        "\n",
        "# 9. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜\n",
        "def main():\n",
        "    print(\"=\" * 70)\n",
        "    print(\"ê¸°ë³¸ Seq2Seq vs ê°œì„ ëœ Attention Seq2Seq ì„±ëŠ¥ ë¹„êµ\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # ë°ì´í„° ì¤€ë¹„\n",
        "    english_sentences, dutch_sentences = create_datasets()\n",
        "    en_vocab, nl_vocab = preprocess_data(english_sentences, dutch_sentences)\n",
        "    train_data = list(zip(english_sentences, dutch_sentences))\n",
        "\n",
        "    print(f\"ì˜ì–´ ì–´íœ˜ ìˆ˜: {len(en_vocab)}\")\n",
        "    print(f\"ë„¤ëœë€ë“œì–´ ì–´íœ˜ ìˆ˜: {len(nl_vocab)}\")\n",
        "    print(f\"í›ˆë ¨ ë°ì´í„° ìˆ˜: {len(train_data)}\")\n",
        "    print()\n",
        "\n",
        "    # ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
        "    embed_size = 128\n",
        "    hidden_size = 256\n",
        "\n",
        "    # 1. ê¸°ë³¸ Seq2Seq ëª¨ë¸ í›ˆë ¨\n",
        "    print(\"1. ê¸°ë³¸ Seq2Seq ëª¨ë¸ í›ˆë ¨...\")\n",
        "    basic_model = BasicSeq2Seq(len(en_vocab), len(nl_vocab), embed_size, hidden_size)\n",
        "    train_basic_model(basic_model, train_data, en_vocab, nl_vocab, num_epochs=200)\n",
        "    print()\n",
        "\n",
        "    # 2. ê°œì„ ëœ Attention ëª¨ë¸ í›ˆë ¨\n",
        "    print(\"2. ê°œì„ ëœ Attention Seq2Seq ëª¨ë¸ í›ˆë ¨...\")\n",
        "    encoder = ImprovedEncoder(len(en_vocab), embed_size, hidden_size, dropout=0.1)\n",
        "    decoder = ImprovedDecoder(len(nl_vocab), embed_size, hidden_size, dropout=0.1)\n",
        "    attention_model = ImprovedSeq2Seq(encoder, decoder)\n",
        "    train_improved_model(attention_model, train_data, en_vocab, nl_vocab, num_epochs=200)\n",
        "    print()\n",
        "\n",
        "    # 3. ì„±ëŠ¥ ë¹„êµ\n",
        "    print(\"=\" * 50)\n",
        "    print(\"ë²ˆì—­ ì„±ëŠ¥ ë¹„êµ\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    test_sentences = [\n",
        "        \"I love llamas\",\n",
        "        \"I like cats\",\n",
        "        \"I eat apples\",\n",
        "        \"I drink water\",\n",
        "        \"I study machine learning\"\n",
        "    ]\n",
        "\n",
        "    for sentence in test_sentences:\n",
        "        basic_translation = translate_basic(basic_model, sentence, en_vocab, nl_vocab)\n",
        "        attention_translation = translate_improved(attention_model, sentence, en_vocab, nl_vocab)\n",
        "\n",
        "        print(f\"ì…ë ¥: {sentence}\")\n",
        "        print(f\"ê¸°ë³¸ ëª¨ë¸: {basic_translation}\")\n",
        "        print(f\"Attention ëª¨ë¸: {attention_translation}\")\n",
        "        print()\n",
        "\n",
        "    # 4. ì£¼ìš” ë¬¸ì¥ Attention ë¶„ì„ (í…ìŠ¤íŠ¸ë¡œë§Œ)\n",
        "    print(\"=\" * 50)\n",
        "    print(\"'I love llamas' Attention ë¶„ì„\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    target_sentence = \"I love llamas\"\n",
        "    translation, attention_matrix, input_words, result_words = get_attention_weights(\n",
        "        attention_model, target_sentence, en_vocab, nl_vocab\n",
        "    )\n",
        "    print(f\"ë²ˆì—­ ê²°ê³¼: {translation}\")\n",
        "    print(f\"ì •ë‹µ: Ik hou van lama's\")\n",
        "\n",
        "    # Attention ê°€ì¤‘ì¹˜ë¥¼ í…ìŠ¤íŠ¸ë¡œ ì¶œë ¥\n",
        "    if attention_matrix and result_words:\n",
        "        print(f\"\\nAttention ê°€ì¤‘ì¹˜ ë¶„ì„:\")\n",
        "        attention_matrix = np.array(attention_matrix)\n",
        "        attention_matrix = attention_matrix[:, :len(input_words)]\n",
        "\n",
        "        for i, target_word in enumerate(result_words):\n",
        "            print(f\"\\n'{target_word}' ìƒì„± ì‹œ:\")\n",
        "            for j, source_word in enumerate(input_words):\n",
        "                weight = attention_matrix[i, j]\n",
        "                bar = 'â–ˆ' * int(weight * 20)\n",
        "                print(f\"  {source_word:8}: {weight:.3f} {bar}\")\n",
        "\n",
        "    return basic_model, attention_model, en_vocab, nl_vocab\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    basic_model, attention_model, en_vocab, nl_vocab = main()\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"ê°œì„ ì‚¬í•­ ìš”ì•½\")\n",
        "    print(\"=\" * 50)\n",
        "    print(\"1. Luong Attentionìœ¼ë¡œ ë³€ê²½ (ë” íš¨ìœ¨ì )\")\n",
        "    print(\"2. ë‹¨ë°©í–¥ LSTM ì‚¬ìš© (ì•ˆì •ì„± í–¥ìƒ)\")\n",
        "    print(\"3. ê°œì„ ëœ í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§ (CosineAnnealingLR)\")\n",
        "    print(\"4. Label Smoothing ì ìš© (ì¼ë°˜í™” í–¥ìƒ)\")\n",
        "    print(\"5. ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ ê°•í™” (ì•ˆì •ì„± í–¥ìƒ)\")\n",
        "    print(\"6. Teacher Forcing ì ì§„ì  ê°ì†Œ\")\n",
        "    print(\"7. ë” ë‚˜ì€ ì •ê·œí™” (AdamW + Weight Decay)\")\n",
        "\n",
        "    print(\"\\nì˜ˆìƒ ê°œì„  íš¨ê³¼:\")\n",
        "    print(\"- ë²ˆì—­ ì •í™•ë„ í–¥ìƒ\")\n",
        "    print(\"- ë” ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ ìƒì„±\")\n",
        "    print(\"- ì•ˆì •ì ì¸ í›ˆë ¨ ê³¼ì •\")\n",
        "    print(\"- ëª…í™•í•œ Attention íŒ¨í„´\")"
      ],
      "metadata": {
        "id": "AnfYBufnhpJa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a351692-3709-490e-ffb9-a5881305d14f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "ê¸°ë³¸ Seq2Seq vs ê°œì„ ëœ Attention Seq2Seq ì„±ëŠ¥ ë¹„êµ\n",
            "======================================================================\n",
            "ì˜ì–´ ì–´íœ˜ ìˆ˜: 25\n",
            "ë„¤ëœë€ë“œì–´ ì–´íœ˜ ìˆ˜: 26\n",
            "í›ˆë ¨ ë°ì´í„° ìˆ˜: 10\n",
            "\n",
            "1. ê¸°ë³¸ Seq2Seq ëª¨ë¸ í›ˆë ¨...\n",
            "Basic Model - Epoch [50/200], Loss: 0.7354\n",
            "Basic Model - Epoch [100/200], Loss: 0.7338\n",
            "Basic Model - Epoch [150/200], Loss: 0.7334\n",
            "Basic Model - Epoch [200/200], Loss: 0.7333\n",
            "\n",
            "2. ê°œì„ ëœ Attention Seq2Seq ëª¨ë¸ í›ˆë ¨...\n",
            "Epoch [25/200], Loss: 1.2385, LR: 0.001924\n",
            "Epoch [50/200], Loss: 1.2241, LR: 0.001707\n",
            "Epoch [75/200], Loss: 1.2199, LR: 0.001383\n",
            "Epoch [100/200], Loss: 1.2194, LR: 0.001000\n",
            "Epoch [125/200], Loss: 1.2191, LR: 0.000617\n",
            "Epoch [150/200], Loss: 1.2189, LR: 0.000293\n",
            "Epoch [175/200], Loss: 1.2187, LR: 0.000076\n",
            "Epoch [200/200], Loss: 1.2187, LR: 0.000000\n",
            "\n",
            "==================================================\n",
            "ë²ˆì—­ ì„±ëŠ¥ ë¹„êµ\n",
            "==================================================\n",
            "ì…ë ¥: I love llamas\n",
            "ê¸°ë³¸ ëª¨ë¸: hou van lama's lama's lama's\n",
            "Attention ëª¨ë¸: hou van lama's lama's lama's\n",
            "\n",
            "ì…ë ¥: I like cats\n",
            "ê¸°ë³¸ ëª¨ë¸: hou van katten katten katten\n",
            "Attention ëª¨ë¸: hou van katten katten\n",
            "\n",
            "ì…ë ¥: I eat apples\n",
            "ê¸°ë³¸ ëª¨ë¸: eet appels appels appels appels\n",
            "Attention ëª¨ë¸: eet appels learning\n",
            "\n",
            "ì…ë ¥: I drink water\n",
            "ê¸°ë³¸ ëª¨ë¸: drink water water water water\n",
            "Attention ëª¨ë¸: drink water learning\n",
            "\n",
            "ì…ë ¥: I study machine learning\n",
            "ê¸°ë³¸ ëª¨ë¸: studeer machine learning learning learning\n",
            "Attention ëª¨ë¸: studeer machine learning learning\n",
            "\n",
            "==================================================\n",
            "'I love llamas' Attention ë¶„ì„\n",
            "==================================================\n",
            "ë²ˆì—­ ê²°ê³¼: hou van lama's lama's lama's\n",
            "ì •ë‹µ: Ik hou van lama's\n",
            "\n",
            "Attention ê°€ì¤‘ì¹˜ ë¶„ì„:\n",
            "\n",
            "'hou' ìƒì„± ì‹œ:\n",
            "  i       : 0.000 \n",
            "  love    : 1.000 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
            "  llamas  : 0.000 \n",
            "\n",
            "'van' ìƒì„± ì‹œ:\n",
            "  i       : 0.000 \n",
            "  love    : 1.000 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
            "  llamas  : 0.000 \n",
            "\n",
            "'lama's' ìƒì„± ì‹œ:\n",
            "  i       : 0.000 \n",
            "  love    : 0.000 \n",
            "  llamas  : 1.000 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
            "\n",
            "'lama's' ìƒì„± ì‹œ:\n",
            "  i       : 0.000 \n",
            "  love    : 0.000 \n",
            "  llamas  : 1.000 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
            "\n",
            "'lama's' ìƒì„± ì‹œ:\n",
            "  i       : 0.024 \n",
            "  love    : 0.000 \n",
            "  llamas  : 0.976 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
            "\n",
            "==================================================\n",
            "ê°œì„ ì‚¬í•­ ìš”ì•½\n",
            "==================================================\n",
            "1. Luong Attentionìœ¼ë¡œ ë³€ê²½ (ë” íš¨ìœ¨ì )\n",
            "2. ë‹¨ë°©í–¥ LSTM ì‚¬ìš© (ì•ˆì •ì„± í–¥ìƒ)\n",
            "3. ê°œì„ ëœ í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§ (CosineAnnealingLR)\n",
            "4. Label Smoothing ì ìš© (ì¼ë°˜í™” í–¥ìƒ)\n",
            "5. ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ ê°•í™” (ì•ˆì •ì„± í–¥ìƒ)\n",
            "6. Teacher Forcing ì ì§„ì  ê°ì†Œ\n",
            "7. ë” ë‚˜ì€ ì •ê·œí™” (AdamW + Weight Decay)\n",
            "\n",
            "ì˜ˆìƒ ê°œì„  íš¨ê³¼:\n",
            "- ë²ˆì—­ ì •í™•ë„ í–¥ìƒ\n",
            "- ë” ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ ìƒì„±\n",
            "- ì•ˆì •ì ì¸ í›ˆë ¨ ê³¼ì •\n",
            "- ëª…í™•í•œ Attention íŒ¨í„´\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "b8Nq06GEwWlx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Transformer ì•„í‚¤í…ì²˜ ì ìš©**"
      ],
      "metadata": {
        "id": "UFgoi4WccrQN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "from collections import Counter\n",
        "\n",
        "# ì‹œë“œ ì„¤ì •ìœ¼ë¡œ ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼ ë³´ì¥\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# 1. ë°ì´í„° ì¤€ë¹„ ë° ì „ì²˜ë¦¬ (ê¸°ì¡´ê³¼ ë™ì¼)\n",
        "class Vocabulary:\n",
        "    def __init__(self):\n",
        "        self.word2idx = {'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3}\n",
        "        self.idx2word = {0: '<PAD>', 1: '<SOS>', 2: '<EOS>', 3: '<UNK>'}\n",
        "        self.word_count = 4\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            self.word2idx[word] = self.word_count\n",
        "            self.idx2word[self.word_count] = word\n",
        "            self.word_count += 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.word_count\n",
        "\n",
        "def create_extended_datasets():\n",
        "    \"\"\"í™•ì¥ëœ ì˜ì–´-ë„¤ëœë€ë“œì–´ ë²ˆì—­ ë°ì´í„°ì…‹\"\"\"\n",
        "    english_sentences = [\n",
        "        \"I love llamas\",\n",
        "        \"I like cats\",\n",
        "        \"I eat apples\",\n",
        "        \"I drink water\",\n",
        "        \"I read books\",\n",
        "        \"I watch movies\",\n",
        "        \"I play games\",\n",
        "        \"I write code\",\n",
        "        \"I love programming\",\n",
        "        \"I study machine learning\",\n",
        "        \"I enjoy reading books\",\n",
        "        \"I prefer tea over coffee\",\n",
        "        \"I want to travel\",\n",
        "        \"I need some help\",\n",
        "        \"I can speak English\",\n",
        "        \"You are very kind\",\n",
        "        \"We learn together\",\n",
        "        \"They work hard\",\n",
        "        \"She loves music\",\n",
        "        \"He reads newspapers\"\n",
        "    ]\n",
        "\n",
        "    dutch_sentences = [\n",
        "        \"Ik hou van lama's\",\n",
        "        \"Ik hou van katten\",\n",
        "        \"Ik eet appels\",\n",
        "        \"Ik drink water\",\n",
        "        \"Ik lees boeken\",\n",
        "        \"Ik kijk naar films\",\n",
        "        \"Ik speel spelletjes\",\n",
        "        \"Ik schrijf code\",\n",
        "        \"Ik hou van programmeren\",\n",
        "        \"Ik studeer machine learning\",\n",
        "        \"Ik geniet van het lezen van boeken\",\n",
        "        \"Ik verkies thee boven koffie\",\n",
        "        \"Ik wil reizen\",\n",
        "        \"Ik heb wat hulp nodig\",\n",
        "        \"Ik kan Engels spreken\",\n",
        "        \"Je bent erg aardig\",\n",
        "        \"We leren samen\",\n",
        "        \"Ze werken hard\",\n",
        "        \"Ze houdt van muziek\",\n",
        "        \"Hij leest kranten\"\n",
        "    ]\n",
        "\n",
        "    return english_sentences, dutch_sentences\n",
        "\n",
        "def preprocess_data(english_sentences, dutch_sentences):\n",
        "    \"\"\"ë°ì´í„° ì „ì²˜ë¦¬ ë° ì–´íœ˜ ì‚¬ì „ ìƒì„±\"\"\"\n",
        "    en_vocab = Vocabulary()\n",
        "    nl_vocab = Vocabulary()\n",
        "\n",
        "    for sentence in english_sentences:\n",
        "        for word in sentence.lower().split():\n",
        "            en_vocab.add_word(word)\n",
        "\n",
        "    for sentence in dutch_sentences:\n",
        "        for word in sentence.lower().split():\n",
        "            nl_vocab.add_word(word)\n",
        "\n",
        "    return en_vocab, nl_vocab\n",
        "\n",
        "def sentence_to_indices(sentence, vocab, max_len=12):\n",
        "    \"\"\"ë¬¸ì¥ì„ ì¸ë±ìŠ¤ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜\"\"\"\n",
        "    words = sentence.lower().split()\n",
        "    indices = [vocab.word2idx.get(word, vocab.word2idx['<UNK>']) for word in words]\n",
        "\n",
        "    if len(indices) >= max_len:\n",
        "        indices = indices[:max_len-1]\n",
        "\n",
        "    while len(indices) < max_len:\n",
        "        indices.append(vocab.word2idx['<PAD>'])\n",
        "\n",
        "    return indices\n",
        "\n",
        "# 2. Positional Encoding êµ¬í˜„\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"Transformerì˜ ìœ„ì¹˜ ì¸ì½”ë”©\"\"\"\n",
        "\n",
        "    def __init__(self, d_model, max_seq_length=1000):\n",
        "        super().__init__()\n",
        "\n",
        "        pe = torch.zeros(max_seq_length, d_model)\n",
        "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
        "\n",
        "        # PE(pos, 2i) = sin(pos / 10000^(2i/d_model))\n",
        "        # PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n",
        "                           (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        pe = pe.unsqueeze(0)  # [1, max_seq_length, d_model]\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [batch_size, seq_length, d_model]\n",
        "        seq_length = x.size(1)\n",
        "        return x + self.pe[:, :seq_length, :]\n",
        "\n",
        "\n",
        "# 3. Multi-Head Attention êµ¬í˜„\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"Multi-Head Self-Attention ë©”ì»¤ë‹ˆì¦˜\"\"\"\n",
        "\n",
        "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "\n",
        "        # Q, K, Vë¥¼ ìœ„í•œ ì„ í˜• ë³€í™˜\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "        \"\"\"Scaled Dot-Product Attention\"\"\"\n",
        "        # Q: [batch_size, num_heads, query_len, d_k]\n",
        "        # K: [batch_size, num_heads, key_len, d_k]\n",
        "        # V: [batch_size, num_heads, value_len, d_k]\n",
        "\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        # scores: [batch_size, num_heads, query_len, key_len]\n",
        "\n",
        "        if mask is not None:\n",
        "            # ë§ˆìŠ¤í¬ ì°¨ì› ì¡°ì •\n",
        "            if mask.dim() == 2:  # [batch_size, key_len] - padding mask\n",
        "                mask = mask.unsqueeze(1).unsqueeze(1)  # [batch_size, 1, 1, key_len]\n",
        "            elif mask.dim() == 3:  # [batch_size, query_len, key_len] - causal mask\n",
        "                mask = mask.unsqueeze(1)  # [batch_size, 1, query_len, key_len]\n",
        "            elif mask.dim() == 4:  # [batch_size, 1, query_len, key_len] - ì´ë¯¸ ë§ìŒ\n",
        "                pass\n",
        "\n",
        "            # Boolean ë§ˆìŠ¤í¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë§ˆìŠ¤í‚¹\n",
        "            mask_value = -1e9\n",
        "            scores = scores.masked_fill(~mask, mask_value)\n",
        "\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "        attention_weights = self.dropout(attention_weights)\n",
        "\n",
        "        context = torch.matmul(attention_weights, V)\n",
        "        # context: [batch_size, num_heads, query_len, d_k]\n",
        "        return context, attention_weights\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size = query.size(0)\n",
        "        query_len = query.size(1)\n",
        "        key_len = key.size(1)\n",
        "        value_len = value.size(1)\n",
        "\n",
        "        # 1. Q, K, V ê³„ì‚°\n",
        "        Q = self.W_q(query)  # [batch_size, query_len, d_model]\n",
        "        K = self.W_k(key)    # [batch_size, key_len, d_model]\n",
        "        V = self.W_v(value)  # [batch_size, value_len, d_model]\n",
        "\n",
        "        # 2. Multi-headë¡œ ë¶„í•  (ê°ê°ì˜ ì‹œí€€ìŠ¤ ê¸¸ì´ ì‚¬ìš©)\n",
        "        Q = Q.view(batch_size, query_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        K = K.view(batch_size, key_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        V = V.view(batch_size, value_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        # ê²°ê³¼: Q[batch_size, num_heads, query_len, d_k], K[batch_size, num_heads, key_len, d_k]\n",
        "\n",
        "        # 3. Scaled Dot-Product Attention\n",
        "        context, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "\n",
        "        # 4. Multi-head ê²°ê³¼ ì—°ê²°\n",
        "        context = context.transpose(1, 2).contiguous().view(\n",
        "            batch_size, query_len, self.d_model\n",
        "        )\n",
        "\n",
        "        # 5. ìµœì¢… ì„ í˜• ë³€í™˜\n",
        "        output = self.W_o(context)\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "\n",
        "# 4. Feed-Forward Network\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\"Position-wise Feed-Forward Network\"\"\"\n",
        "\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # FFN(x) = max(0, xW1 + b1)W2 + b2\n",
        "        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n",
        "\n",
        "\n",
        "# 5. Transformer Encoder Layer\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    \"\"\"Transformer ì¸ì½”ë” ë ˆì´ì–´\"\"\"\n",
        "\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.self_attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # 1. Self-Attention + Residual Connection + Layer Norm\n",
        "        attn_output, _ = self.self_attention(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "\n",
        "        # 2. Feed-Forward + Residual Connection + Layer Norm\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout(ff_output))\n",
        "\n",
        "        return x\n",
        "\n",
        "# 6. Transformer Decoder Layer\n",
        "class TransformerDecoderLayer(nn.Module):\n",
        "    \"\"\"Transformer ë””ì½”ë” ë ˆì´ì–´\"\"\"\n",
        "\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.self_attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.cross_attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):\n",
        "        # 1. Self-Attention (with causal mask)\n",
        "        attn_output, _ = self.self_attention(x, x, x, tgt_mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "\n",
        "        # 2. Cross-Attention (Encoder-Decoder Attention)\n",
        "        cross_attn_output, cross_attn_weights = self.cross_attention(\n",
        "            x, encoder_output, encoder_output, src_mask\n",
        "        )\n",
        "        x = self.norm2(x + self.dropout(cross_attn_output))\n",
        "\n",
        "        # 3. Feed-Forward\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm3(x + self.dropout(ff_output))\n",
        "\n",
        "        return x, cross_attn_weights\n",
        "\n",
        "\n",
        "# 7. ì™„ì „í•œ Transformer ëª¨ë¸\n",
        "class Transformer(nn.Module):\n",
        "    \"\"\"ì™„ì „í•œ Transformer ëª¨ë¸\"\"\"\n",
        "\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=256, num_heads=8,\n",
        "                 num_encoder_layers=3, num_decoder_layers=3, d_ff=1024,\n",
        "                 max_seq_length=100, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.src_vocab_size = src_vocab_size\n",
        "        self.tgt_vocab_size = tgt_vocab_size\n",
        "\n",
        "        # ì„ë² ë”© ë ˆì´ì–´\n",
        "        self.src_embedding = nn.Embedding(src_vocab_size, d_model, padding_idx=0)\n",
        "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model, padding_idx=0)\n",
        "\n",
        "        # ìœ„ì¹˜ ì¸ì½”ë”©\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_seq_length)\n",
        "\n",
        "        # ì¸ì½”ë”\n",
        "        self.encoder_layers = nn.ModuleList([\n",
        "            TransformerEncoderLayer(d_model, num_heads, d_ff, dropout)\n",
        "            for _ in range(num_encoder_layers)\n",
        "        ])\n",
        "\n",
        "        # ë””ì½”ë”\n",
        "        self.decoder_layers = nn.ModuleList([\n",
        "            TransformerDecoderLayer(d_model, num_heads, d_ff, dropout)\n",
        "            for _ in range(num_decoder_layers)\n",
        "        ])\n",
        "\n",
        "        # ì¶œë ¥ íˆ¬ì˜\n",
        "        self.output_projection = nn.Linear(d_model, tgt_vocab_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        \"\"\"ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”\"\"\"\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def create_padding_mask(self, x, pad_idx=0):\n",
        "        \"\"\"íŒ¨ë”© ë§ˆìŠ¤í¬ ìƒì„±\"\"\"\n",
        "        return (x != pad_idx)  # Boolean íƒ€ì…ìœ¼ë¡œ ë°˜í™˜\n",
        "\n",
        "    def create_causal_mask(self, size):\n",
        "        \"\"\"ì¸ê³¼ì  ë§ˆìŠ¤í¬ ìƒì„± (ë¯¸ë˜ í† í° ë³´ì§€ ëª»í•˜ê²Œ)\"\"\"\n",
        "        mask = torch.triu(torch.ones(size, size), diagonal=1)\n",
        "        return (mask == 0)  # Boolean íƒ€ì…ìœ¼ë¡œ ë°˜í™˜\n",
        "\n",
        "    def encode(self, src, src_mask):\n",
        "        \"\"\"ì¸ì½”ë” forward pass\"\"\"\n",
        "        # ì„ë² ë”© + ìœ„ì¹˜ ì¸ì½”ë”©\n",
        "        x = self.src_embedding(src) * math.sqrt(self.d_model)\n",
        "        x = self.pos_encoding(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # ì¸ì½”ë” ë ˆì´ì–´ë“¤ í†µê³¼\n",
        "        for layer in self.encoder_layers:\n",
        "            x = layer(x, src_mask)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def decode(self, tgt, encoder_output, src_mask, tgt_mask):\n",
        "        \"\"\"ë””ì½”ë” forward pass\"\"\"\n",
        "        # ì„ë² ë”© + ìœ„ì¹˜ ì¸ì½”ë”©\n",
        "        x = self.tgt_embedding(tgt) * math.sqrt(self.d_model)\n",
        "        x = self.pos_encoding(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        cross_attentions = []\n",
        "\n",
        "        # ë””ì½”ë” ë ˆì´ì–´ë“¤ í†µê³¼\n",
        "        for layer in self.decoder_layers:\n",
        "            x, cross_attn = layer(x, encoder_output, src_mask, tgt_mask)\n",
        "            cross_attentions.append(cross_attn)\n",
        "\n",
        "        return x, cross_attentions\n",
        "\n",
        "    def forward(self, src, tgt, teacher_forcing_ratio=1.0):\n",
        "        \"\"\"ì „ì²´ forward pass\"\"\"\n",
        "        batch_size, tgt_seq_len = tgt.size()\n",
        "\n",
        "        # ë§ˆìŠ¤í¬ ìƒì„± (Boolean íƒ€ì…)\n",
        "        src_mask = self.create_padding_mask(src)  # [batch_size, src_len]\n",
        "        tgt_padding_mask = self.create_padding_mask(tgt)  # [batch_size, tgt_len]\n",
        "\n",
        "        # ì¸ê³¼ì  ë§ˆìŠ¤í¬ ìƒì„±\n",
        "        causal_mask = self.create_causal_mask(tgt_seq_len).to(tgt.device)  # [tgt_len, tgt_len]\n",
        "\n",
        "        # íƒ€ê²Ÿ ë§ˆìŠ¤í¬: íŒ¨ë”© ë§ˆìŠ¤í¬ì™€ ì¸ê³¼ì  ë§ˆìŠ¤í¬ ê²°í•©\n",
        "        # Broadcastingì„ í†µí•´ ì˜¬ë°”ë¥´ê²Œ ê²°í•©\n",
        "        tgt_mask = tgt_padding_mask.unsqueeze(1).unsqueeze(2)  # [batch_size, 1, 1, tgt_len]\n",
        "        causal_mask = causal_mask.unsqueeze(0).unsqueeze(0)   # [1, 1, tgt_len, tgt_len]\n",
        "        tgt_mask = tgt_mask & causal_mask  # Broadcastingìœ¼ë¡œ [batch_size, 1, tgt_len, tgt_len]\n",
        "\n",
        "        # ì¸ì½”ë”©\n",
        "        encoder_output = self.encode(src, src_mask)\n",
        "\n",
        "        # ë””ì½”ë”© (Teacher Forcing ì‚¬ìš©)\n",
        "        tgt_input = tgt[:, :-1]  # ë§ˆì§€ë§‰ í† í° ì œì™¸\n",
        "        decoder_output, cross_attentions = self.decode(\n",
        "            tgt_input, encoder_output, src_mask,\n",
        "            tgt_mask[:, :, :-1, :-1]  # í¬ê¸° ë§ì¶¤\n",
        "        )\n",
        "\n",
        "        # ì¶œë ¥ íˆ¬ì˜\n",
        "        output = self.output_projection(decoder_output)\n",
        "\n",
        "        return output, cross_attentions\n",
        "\n",
        "# 8. í›ˆë ¨ í•¨ìˆ˜\n",
        "# def train_transformer(model, train_data, en_vocab, nl_vocab, num_epochs=300):\n",
        "#     \"\"\"Transformer ëª¨ë¸ í›ˆë ¨\"\"\"\n",
        "#     criterion = nn.CrossEntropyLoss(ignore_index=0, label_smoothing=0.1)\n",
        "#     optimizer = optim.AdamW(model.parameters(), lr=0.0001, weight_decay=0.01,\n",
        "#                            betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "#     # ë” ê°„ë‹¨í•œ ìŠ¤ì¼€ì¤„ëŸ¬\n",
        "#     scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n",
        "\n",
        "#     model.train()\n",
        "#     max_len = 12\n",
        "\n",
        "#     for epoch in range(num_epochs):\n",
        "#         total_loss = 0\n",
        "\n",
        "#         # ë°ì´í„° ì…”í”Œ\n",
        "#         shuffled_data = train_data.copy()\n",
        "#         random.shuffle(shuffled_data)\n",
        "\n",
        "#         for batch_idx, (en_sentence, nl_sentence) in enumerate(shuffled_data):\n",
        "#             try:\n",
        "#                 # ë°ì´í„° ì¤€ë¹„ - ê¸¸ì´ ì—„ê²©í•˜ê²Œ ê´€ë¦¬\n",
        "#                 en_indices = sentence_to_indices(en_sentence, en_vocab, max_len)\n",
        "#                 nl_indices = sentence_to_indices(nl_sentence, nl_vocab, max_len-2)  # SOS, EOS ê³µê°„ í™•ë³´\n",
        "\n",
        "#                 # íƒ€ê²Ÿ ì‹œí€€ìŠ¤ êµ¬ì„±: <SOS> + ë¬¸ì¥ + <EOS>\n",
        "#                 nl_input = [nl_vocab.word2idx['<SOS>']] + nl_indices\n",
        "#                 nl_target = nl_indices + [nl_vocab.word2idx['<EOS>']]\n",
        "\n",
        "#                 # ì •í™•íˆ max_lenìœ¼ë¡œ ë§ì¶”ê¸°\n",
        "#                 if len(nl_input) > max_len:\n",
        "#                     nl_input = nl_input[:max_len]\n",
        "#                 if len(nl_target) > max_len:\n",
        "#                     nl_target = nl_target[:max_len]\n",
        "\n",
        "#                 # íŒ¨ë”© ì¶”ê°€\n",
        "#                 while len(nl_input) < max_len:\n",
        "#                     nl_input.append(nl_vocab.word2idx['<PAD>'])\n",
        "#                 while len(nl_target) < max_len:\n",
        "#                     nl_target.append(nl_vocab.word2idx['<PAD>'])\n",
        "\n",
        "#                 # í…ì„œ ë³€í™˜\n",
        "#                 src_tensor = torch.LongTensor([en_indices])\n",
        "#                 tgt_tensor = torch.LongTensor([nl_input])\n",
        "#                 target_tensor = torch.LongTensor([nl_target])\n",
        "\n",
        "#                 # ê¸¸ì´ ê²€ì¦\n",
        "#                 assert src_tensor.size(1) == max_len, f\"Source length mismatch: {src_tensor.size(1)} != {max_len}\"\n",
        "#                 assert tgt_tensor.size(1) == max_len, f\"Target input length mismatch: {tgt_tensor.size(1)} != {max_len}\"\n",
        "#                 assert target_tensor.size(1) == max_len, f\"Target output length mismatch: {target_tensor.size(1)} != {max_len}\"\n",
        "\n",
        "#                 # ìˆœì „íŒŒ\n",
        "#                 optimizer.zero_grad()\n",
        "#                 output, _ = model(src_tensor, tgt_tensor)\n",
        "\n",
        "#                 # ì¶œë ¥ ê¸¸ì´ ê²€ì¦\n",
        "#                 expected_output_len = max_len - 1  # tgt_tensorì—ì„œ ë§ˆì§€ë§‰ í† í° ì œì™¸\n",
        "#                 assert output.size(1) == expected_output_len, f\"Output length mismatch: {output.size(1)} != {expected_output_len}\"\n",
        "\n",
        "#                 # ì†ì‹¤ ê³„ì‚° - ì°¨ì› ë§ì¶”ê¸°\n",
        "#                 output_flat = output.reshape(-1, output.shape[-1])  # [batch*(max_len-1), vocab_size]\n",
        "#                 target_flat = target_tensor.reshape(-1)             # [batch*max_len]\n",
        "\n",
        "#                 # íƒ€ê²Ÿë„ ê°™ì€ ê¸¸ì´ë¡œ ë§ì¶”ê¸° (ì²« ë²ˆì§¸ í† í° ì œì™¸)\n",
        "#                 target_adjusted = target_tensor[:, :expected_output_len].reshape(-1)  # [batch*(max_len-1)]\n",
        "\n",
        "#                 assert output_flat.size(0) == target_adjusted.size(0), \\\n",
        "#                     f\"Batch size mismatch: output {output_flat.size(0)} != target {target_adjusted.size(0)}\"\n",
        "\n",
        "#                 loss = criterion(output_flat, target_adjusted)\n",
        "\n",
        "#                 # ì—­ì „íŒŒ\n",
        "#                 loss.backward()\n",
        "#                 torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "#                 optimizer.step()\n",
        "\n",
        "#                 total_loss += loss.item()\n",
        "\n",
        "#             except Exception as e:\n",
        "#                 print(f\"Error in batch {batch_idx}: {e}\")\n",
        "#                 print(f\"Sentence lengths - EN: {len(en_sentence.split())}, NL: {len(nl_sentence.split())}\")\n",
        "#                 continue\n",
        "\n",
        "#         scheduler.step()\n",
        "\n",
        "#         if (epoch + 1) % 50 == 0:\n",
        "#             avg_loss = total_loss / len(train_data) if len(train_data) > 0 else 0\n",
        "#             current_lr = scheduler.get_last_lr()[0]\n",
        "#             print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, LR: {current_lr:.6f}')\n",
        "\n",
        "# 8. í›ˆë ¨ í•¨ìˆ˜ (ìˆ˜ì •ë¨)\n",
        "def train_transformer(model, train_data, en_vocab, nl_vocab, num_epochs=300):\n",
        "    \"\"\"Transformer ëª¨ë¸ í›ˆë ¨ (ë°ì´í„° ì „ì²˜ë¦¬ ë¡œì§ ìˆ˜ì •)\"\"\"\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=0, label_smoothing=0.1)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=0.0001, weight_decay=0.01,\n",
        "                            betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "    # ë” ê°„ë‹¨í•œ ìŠ¤ì¼€ì¤„ëŸ¬\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n",
        "\n",
        "    model.train()\n",
        "    max_len = 12 # ì´ ê°’(max_len)ì´ 12ë¡œ ì„¤ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "\n",
        "        # ë°ì´í„° ì…”í”Œ\n",
        "        shuffled_data = train_data.copy()\n",
        "        random.shuffle(shuffled_data)\n",
        "\n",
        "        for batch_idx, (en_sentence, nl_sentence) in enumerate(shuffled_data):\n",
        "            try:\n",
        "                # --- START: ìˆ˜ì •ëœ ë°ì´í„° ì¤€ë¹„ ë¡œì§ ---\n",
        "\n",
        "                # 1. Source (ì˜ì–´) ì¤€ë¹„ (ê¸°ì¡´ê³¼ ë™ì¼)\n",
        "                #    [word1, word2, ..., <PAD>, <PAD>]\n",
        "                en_indices = sentence_to_indices(en_sentence, en_vocab, max_len)\n",
        "                src_tensor = torch.LongTensor([en_indices])\n",
        "\n",
        "                # 2. Target (ë„¤ëœë€ë“œì–´) ì¤€ë¹„ (ë²„ê·¸ ìˆ˜ì •)\n",
        "\n",
        "                # (a) ë‹¨ì–´ë¡œ ë³€í™˜\n",
        "                nl_words = nl_sentence.lower().split()\n",
        "\n",
        "                # (b) ì¸ë±ìŠ¤ë¡œ ë³€í™˜ (íŒ¨ë”© X)\n",
        "                nl_indices_raw = [nl_vocab.word2idx.get(w, nl_vocab.word2idx['<UNK>']) for w in nl_words]\n",
        "\n",
        "                # (c) <SOS>ì™€ <EOS>ë¥¼ ìœ„í•œ ê³µê°„(2ê°œ) í™•ë³´ (max_len-2)\n",
        "                if len(nl_indices_raw) > max_len - 2:\n",
        "                    nl_indices_raw = nl_indices_raw[:max_len - 2]\n",
        "\n",
        "                # (d) <SOS>ì™€ <EOS> í† í° ì¶”ê°€\n",
        "                # nl_input_list: [<SOS>, w1, w2, ...]\n",
        "                nl_input_list = [nl_vocab.word2idx['<SOS>']] + nl_indices_raw\n",
        "                # nl_target_list: [w1, w2, ..., <EOS>]\n",
        "                nl_target_list = nl_indices_raw + [nl_vocab.word2idx['<EOS>']]\n",
        "\n",
        "                # (e) <PAD> í† í°ìœ¼ë¡œ max_lenê¹Œì§€ ì±„ìš°ê¸°\n",
        "                #    ì´ì œ <EOS>ê°€ <PAD> ì•ì— ì •í™•íˆ ìœ„ì¹˜í•©ë‹ˆë‹¤.\n",
        "                while len(nl_input_list) < max_len:\n",
        "                    nl_input_list.append(nl_vocab.word2idx['<PAD>'])\n",
        "\n",
        "                while len(nl_target_list) < max_len:\n",
        "                    nl_target_list.append(nl_vocab.word2idx['<PAD>'])\n",
        "\n",
        "                # (f) í…ì„œ ë³€í™˜\n",
        "                tgt_tensor = torch.LongTensor([nl_input_list])\n",
        "                target_tensor = torch.LongTensor([nl_target_list])\n",
        "\n",
        "                # --- END: ìˆ˜ì •ëœ ë°ì´í„° ì¤€ë¹„ ë¡œì§ ---\n",
        "\n",
        "\n",
        "                # ê¸¸ì´ ê²€ì¦\n",
        "                assert src_tensor.size(1) == max_len, f\"Source length mismatch: {src_tensor.size(1)} != {max_len}\"\n",
        "                assert tgt_tensor.size(1) == max_len, f\"Target input length mismatch: {tgt_tensor.size(1)} != {max_len}\"\n",
        "                assert target_tensor.size(1) == max_len, f\"Target output length mismatch: {target_tensor.size(1)} != {max_len}\"\n",
        "\n",
        "                # ìˆœì „íŒŒ\n",
        "                optimizer.zero_grad()\n",
        "                output, _ = model(src_tensor, tgt_tensor)\n",
        "\n",
        "                # ì¶œë ¥ ê¸¸ì´ ê²€ì¦\n",
        "                expected_output_len = max_len - 1  # tgt_tensorì—ì„œ ë§ˆì§€ë§‰ í† í° ì œì™¸\n",
        "                assert output.size(1) == expected_output_len, f\"Output length mismatch: {output.size(1)} != {expected_output_len}\"\n",
        "\n",
        "                # ì†ì‹¤ ê³„ì‚° - ì°¨ì› ë§ì¶”ê¸°\n",
        "                output_flat = output.reshape(-1, output.shape[-1])  # [batch*(max_len-1), vocab_size]\n",
        "\n",
        "                # íƒ€ê²Ÿë„ ê°™ì€ ê¸¸ì´ë¡œ ë§ì¶”ê¸°\n",
        "                target_adjusted = target_tensor[:, :expected_output_len].reshape(-1)  # [batch*(max_len-1)]\n",
        "\n",
        "                assert output_flat.size(0) == target_adjusted.size(0), \\\n",
        "                    f\"Batch size mismatch: output {output_flat.size(0)} != target {target_adjusted.size(0)}\"\n",
        "\n",
        "                loss = criterion(output_flat, target_adjusted)\n",
        "\n",
        "                # ì—­ì „íŒŒ\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error in batch {batch_idx}: {e}\")\n",
        "                print(f\"Sentence lengths - EN: {len(en_sentence.split())}, NL: {len(nl_sentence.split())}\")\n",
        "                continue\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        if (epoch + 1) % 50 == 0:\n",
        "            avg_loss = total_loss / len(train_data) if len(train_data) > 0 else 0\n",
        "            current_lr = scheduler.get_last_lr()[0]\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, LR: {current_lr:.6f}')\n",
        "\n",
        "\n",
        "\n",
        "# 9. ë²ˆì—­ í•¨ìˆ˜ (Greedy Decoding)\n",
        "def translate_transformer(model, sentence, en_vocab, nl_vocab, max_len=12):\n",
        "    \"\"\"Transformerë¡œ ë²ˆì—­ (Greedy Decoding)\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # ì†ŒìŠ¤ ë¬¸ì¥ ì¸ì½”ë”©\n",
        "        en_indices = sentence_to_indices(sentence, en_vocab, max_len)\n",
        "        src_tensor = torch.LongTensor([en_indices])\n",
        "\n",
        "        # ì†ŒìŠ¤ ë§ˆìŠ¤í¬ ìƒì„±\n",
        "        src_mask = model.create_padding_mask(src_tensor)\n",
        "\n",
        "        # ì¸ì½”ë” ì¶œë ¥\n",
        "        encoder_output = model.encode(src_tensor, src_mask)\n",
        "\n",
        "        # ë””ì½”ë”© ì‹œì‘\n",
        "        result = []\n",
        "        tgt_input = torch.LongTensor([[nl_vocab.word2idx['<SOS>']]])\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            # í˜„ì¬ê¹Œì§€ì˜ íƒ€ê²Ÿ ì‹œí€€ìŠ¤ë¡œ ë””ì½”ë”©\n",
        "            current_len = tgt_input.size(1)\n",
        "            causal_mask = model.create_causal_mask(current_len).to(tgt_input.device)\n",
        "\n",
        "            # ë°°ì¹˜ ì°¨ì› ì¶”ê°€\n",
        "            tgt_self_mask = causal_mask.unsqueeze(0)  # [1, current_len, current_len]\n",
        "\n",
        "            decoder_output, _ = model.decode(tgt_input, encoder_output, src_mask, tgt_self_mask)\n",
        "\n",
        "            # ë§ˆì§€ë§‰ ìœ„ì¹˜ì˜ ì¶œë ¥ìœ¼ë¡œ ë‹¤ìŒ í† í° ì˜ˆì¸¡\n",
        "            next_token_logits = model.output_projection(decoder_output[:, -1, :])\n",
        "            predicted_id = next_token_logits.argmax(-1).item()\n",
        "\n",
        "            # ì¢…ë£Œ ì¡°ê±´\n",
        "            if predicted_id == nl_vocab.word2idx['<EOS>']:\n",
        "                break\n",
        "            if predicted_id == nl_vocab.word2idx['<PAD>']:\n",
        "                break\n",
        "\n",
        "            word = nl_vocab.idx2word[predicted_id]\n",
        "            result.append(word)\n",
        "\n",
        "            # ë‹¤ìŒ ì…ë ¥ì— ì˜ˆì¸¡ëœ í† í° ì¶”ê°€\n",
        "            tgt_input = torch.cat([\n",
        "                tgt_input,\n",
        "                torch.LongTensor([[predicted_id]])\n",
        "            ], dim=1)\n",
        "\n",
        "        return ' '.join(result)\n",
        "\n",
        "# 10. Beam Search ë””ì½”ë”© (ê³ ê¸‰)\n",
        "def beam_search_translate(model, sentence, en_vocab, nl_vocab, beam_size=3, max_len=12):\n",
        "    \"\"\"Beam Searchë¥¼ ì‚¬ìš©í•œ ë” ë‚˜ì€ ë²ˆì—­\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # ì†ŒìŠ¤ ì¸ì½”ë”©\n",
        "        en_indices = sentence_to_indices(sentence, en_vocab, max_len)\n",
        "        src_tensor = torch.LongTensor([en_indices])\n",
        "        src_mask = model.create_padding_mask(src_tensor)\n",
        "        encoder_output = model.encode(src_tensor, src_mask)\n",
        "\n",
        "        # Beam Search ì´ˆê¸°í™”\n",
        "        beams = [([nl_vocab.word2idx['<SOS>']], 0.0)]  # (sequence, score)\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            new_beams = []\n",
        "\n",
        "            for sequence, score in beams:\n",
        "                if sequence[-1] == nl_vocab.word2idx['<EOS>']:\n",
        "                    new_beams.append((sequence, score))\n",
        "                    continue\n",
        "\n",
        "                # í˜„ì¬ ì‹œí€€ìŠ¤ë¡œ ë””ì½”ë”©\n",
        "                tgt_input = torch.LongTensor([sequence])\n",
        "                current_len = tgt_input.size(1)\n",
        "                causal_mask = model.create_causal_mask(current_len).to(tgt_input.device)\n",
        "\n",
        "                # ë°°ì¹˜ ì°¨ì› ì¶”ê°€\n",
        "                tgt_self_mask = causal_mask.unsqueeze(0)  # [1, current_len, current_len]\n",
        "\n",
        "                decoder_output, _ = model.decode(tgt_input, encoder_output, src_mask, tgt_self_mask)\n",
        "                next_token_logits = model.output_projection(decoder_output[:, -1, :])\n",
        "\n",
        "                # Top-k í† í° ì„ íƒ\n",
        "                log_probs = F.log_softmax(next_token_logits, dim=-1)\n",
        "                top_k_probs, top_k_indices = torch.topk(log_probs, beam_size)\n",
        "\n",
        "                for i in range(beam_size):\n",
        "                    token_id = top_k_indices[0, i].item()\n",
        "                    token_score = top_k_probs[0, i].item()\n",
        "                    new_sequence = sequence + [token_id]\n",
        "                    new_score = score + token_score\n",
        "                    new_beams.append((new_sequence, new_score))\n",
        "\n",
        "            # ìƒìœ„ beam_sizeê°œ ì„ íƒ\n",
        "            beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_size]\n",
        "\n",
        "            # ëª¨ë“  ë¹”ì´ ì¢…ë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸\n",
        "            if all(seq[-1] == nl_vocab.word2idx['<EOS>'] for seq, _ in beams):\n",
        "                break\n",
        "\n",
        "        # ìµœê³  ì ìˆ˜ì˜ ì‹œí€€ìŠ¤ ì„ íƒ\n",
        "        best_sequence, _ = max(beams, key=lambda x: x[1])\n",
        "\n",
        "        # í† í°ì„ ë‹¨ì–´ë¡œ ë³€í™˜\n",
        "        result = []\n",
        "        for token_id in best_sequence[1:]:  # <SOS> ì œì™¸\n",
        "            if token_id == nl_vocab.word2idx['<EOS>']:\n",
        "                break\n",
        "            if token_id == nl_vocab.word2idx['<PAD>']:\n",
        "                break\n",
        "            result.append(nl_vocab.idx2word[token_id])\n",
        "\n",
        "        return ' '.join(result)\n",
        "\n",
        "# 11. Cross-Attention ë¶„ì„ í•¨ìˆ˜\n",
        "def analyze_cross_attention(model, sentence, en_vocab, nl_vocab, max_len=12):\n",
        "    \"\"\"Cross-Attention ê°€ì¤‘ì¹˜ ë¶„ì„ - ì•ˆì „í•œ ë²„ì „\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        try:\n",
        "            # ì†ŒìŠ¤ ì¸ì½”ë”©\n",
        "            en_indices = sentence_to_indices(sentence, en_vocab, max_len)\n",
        "            src_tensor = torch.LongTensor([en_indices])\n",
        "            src_mask = model.create_padding_mask(src_tensor)\n",
        "            encoder_output = model.encode(src_tensor, src_mask)\n",
        "\n",
        "            # ë²ˆì—­í•˜ë©´ì„œ attention ìˆ˜ì§‘\n",
        "            result_words = []\n",
        "            attention_matrices = []\n",
        "            tgt_input = torch.LongTensor([[nl_vocab.word2idx['<SOS>']]])\n",
        "\n",
        "            # ì…ë ¥ ë‹¨ì–´ë“¤\n",
        "            input_words = []\n",
        "            for idx in en_indices:\n",
        "                if idx == en_vocab.word2idx['<PAD>']:\n",
        "                    break\n",
        "                input_words.append(en_vocab.idx2word[idx])\n",
        "\n",
        "            for step in range(max_len-1):  # max_len-1ë¡œ ì œí•œ\n",
        "                current_len = tgt_input.size(1)\n",
        "                causal_mask = model.create_causal_mask(current_len).to(tgt_input.device)\n",
        "                tgt_self_mask = causal_mask.unsqueeze(0)\n",
        "\n",
        "                decoder_output, cross_attentions = model.decode(\n",
        "                    tgt_input, encoder_output, src_mask, tgt_self_mask\n",
        "                )\n",
        "\n",
        "                next_token_logits = model.output_projection(decoder_output[:, -1, :])\n",
        "                predicted_id = next_token_logits.argmax(-1).item()\n",
        "\n",
        "                if predicted_id == nl_vocab.word2idx['<EOS>']:\n",
        "                    break\n",
        "                if predicted_id == nl_vocab.word2idx['<PAD>']:\n",
        "                    break\n",
        "\n",
        "                word = nl_vocab.idx2word[predicted_id]\n",
        "                result_words.append(word)\n",
        "\n",
        "                # Cross-attention ë¶„ì„ (ì•ˆì „í•˜ê²Œ ì²˜ë¦¬)\n",
        "                if cross_attentions and len(cross_attentions) > 0:\n",
        "                    try:\n",
        "                        last_cross_attention = cross_attentions[-1]\n",
        "                        if last_cross_attention is not None and last_cross_attention.size(0) > 0:\n",
        "                            avg_attention = last_cross_attention.mean(dim=1)\n",
        "                            if avg_attention.size(1) > 0:  # ì‹œí€€ìŠ¤ ê¸¸ì´ í™•ì¸\n",
        "                                current_attention = avg_attention[0, -1, :len(input_words)].cpu().numpy()\n",
        "                                attention_matrices.append(current_attention)\n",
        "                    except Exception as e:\n",
        "                        print(f\"Attention extraction error at step {step}: {e}\")\n",
        "\n",
        "                tgt_input = torch.cat([\n",
        "                    tgt_input,\n",
        "                    torch.LongTensor([[predicted_id]])\n",
        "                ], dim=1)\n",
        "\n",
        "            return ' '.join(result_words), attention_matrices, input_words, result_words\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in attention analysis: {e}\")\n",
        "            return sentence, [], [], []\n",
        "\n",
        "# 12. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ - ì•ˆì „í•œ ë²„ì „\n",
        "def main():\n",
        "    \"\"\"ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜\"\"\"\n",
        "    print(\"=\" * 70)\n",
        "    print(\"Transformer ì•„í‚¤í…ì²˜ ì˜ì–´-ë„¤ëœë€ë“œì–´ ë²ˆì—­ ëª¨ë¸\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # ë°ì´í„° ì¤€ë¹„\n",
        "    english_sentences, dutch_sentences = create_extended_datasets()\n",
        "    en_vocab, nl_vocab = preprocess_data(english_sentences, dutch_sentences)\n",
        "    train_data = list(zip(english_sentences, dutch_sentences))\n",
        "\n",
        "    print(f\"ì˜ì–´ ì–´íœ˜ ìˆ˜: {len(en_vocab)}\")\n",
        "    print(f\"ë„¤ëœë€ë“œì–´ ì–´íœ˜ ìˆ˜: {len(nl_vocab)}\")\n",
        "    print(f\"í›ˆë ¨ ë°ì´í„° ìˆ˜: {len(train_data)}\")\n",
        "    print()\n",
        "\n",
        "    # ë°ì´í„° ê¸¸ì´ ë¶„ì„\n",
        "    print(\"ë°ì´í„° ê¸¸ì´ ë¶„ì„:\")\n",
        "    max_en_len = max(len(s.split()) for s in english_sentences)\n",
        "    max_nl_len = max(len(s.split()) for s in dutch_sentences)\n",
        "    print(f\"ìµœëŒ€ ì˜ì–´ ë¬¸ì¥ ê¸¸ì´: {max_en_len}\")\n",
        "    print(f\"ìµœëŒ€ ë„¤ëœë€ë“œì–´ ë¬¸ì¥ ê¸¸ì´: {max_nl_len}\")\n",
        "    print()\n",
        "\n",
        "    # Transformer ëª¨ë¸ ìƒì„± (ë” ì‘ì€ í¬ê¸°ë¡œ ì•ˆì •ì„± í™•ë³´)\n",
        "    model = Transformer(\n",
        "        src_vocab_size=len(en_vocab),\n",
        "        tgt_vocab_size=len(nl_vocab),\n",
        "        d_model=128,          # 256 â†’ 128 (ë©”ëª¨ë¦¬ ì ˆì•½)\n",
        "        num_heads=4,          # 8 â†’ 4 (ê³„ì‚° ë‹¨ìˆœí™”)\n",
        "        num_encoder_layers=2, # 3 â†’ 2 (ì•ˆì •ì„± í–¥ìƒ)\n",
        "        num_decoder_layers=2, # 3 â†’ 2 (ì•ˆì •ì„± í–¥ìƒ)\n",
        "        d_ff=512,            # 1024 â†’ 512 (ë©”ëª¨ë¦¬ ì ˆì•½)\n",
        "        dropout=0.1\n",
        "    )\n",
        "\n",
        "    print(\"Transformer ëª¨ë¸ êµ¬ì¡°:\")\n",
        "    print(f\"  ì„ë² ë”© ì°¨ì›: 128\")\n",
        "    print(f\"  Attention í—¤ë“œ ìˆ˜: 4\")\n",
        "    print(f\"  ì¸ì½”ë” ë ˆì´ì–´: 2\")\n",
        "    print(f\"  ë””ì½”ë” ë ˆì´ì–´: 2\")\n",
        "    print(f\"  Feed-Forward ì°¨ì›: 512\")\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"  ì´ íŒŒë¼ë¯¸í„° ìˆ˜: {total_params:,}\")\n",
        "    print()\n",
        "\n",
        "    # ëª¨ë¸ í›ˆë ¨ (ë” ì ì€ ì—í¬í¬ë¡œ ì•ˆì •ì„± í™•ë³´)\n",
        "    print(\"Transformer ëª¨ë¸ í›ˆë ¨ ì‹œì‘...\")\n",
        "    try:\n",
        "        train_transformer(model, train_data, en_vocab, nl_vocab, num_epochs=150)\n",
        "        print(\"í›ˆë ¨ ì™„ë£Œ!\")\n",
        "    except Exception as e:\n",
        "        print(f\"í›ˆë ¨ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
        "        print(\"ë” ê°„ë‹¨í•œ ì„¤ì •ìœ¼ë¡œ ì¬ì‹œë„...\")\n",
        "        # ë” ê°„ë‹¨í•œ ëª¨ë¸ë¡œ ì¬ì‹œë„\n",
        "        model = Transformer(\n",
        "            src_vocab_size=len(en_vocab),\n",
        "            tgt_vocab_size=len(nl_vocab),\n",
        "            d_model=64,\n",
        "            num_heads=2,\n",
        "            num_encoder_layers=1,\n",
        "            num_decoder_layers=1,\n",
        "            d_ff=256,\n",
        "            dropout=0.1\n",
        "        )\n",
        "        train_transformer(model, train_data, en_vocab, nl_vocab, num_epochs=100)\n",
        "    print()\n",
        "\n",
        "    # ë²ˆì—­ í…ŒìŠ¤íŠ¸\n",
        "    print(\"=\" * 50)\n",
        "    print(\"ë²ˆì—­ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    test_sentences = [\n",
        "        \"I love llamas\",\n",
        "        \"I like cats\",\n",
        "        \"I eat apples\",\n",
        "        \"I drink water\",\n",
        "        \"You are very kind\"\n",
        "    ]\n",
        "\n",
        "    print(\"Greedy Decoding ê²°ê³¼:\")\n",
        "    print(\"-\" * 30)\n",
        "    for sentence in test_sentences:\n",
        "        try:\n",
        "            translation = translate_transformer(model, sentence, en_vocab, nl_vocab)\n",
        "            print(f\"ì˜ì–´: {sentence}\")\n",
        "            print(f\"ë²ˆì—­: {translation}\")\n",
        "            print()\n",
        "        except Exception as e:\n",
        "            print(f\"ë²ˆì—­ ì˜¤ë¥˜ ({sentence}): {e}\")\n",
        "            continue\n",
        "\n",
        "    # Beam Search í…ŒìŠ¤íŠ¸ (ì„ íƒì )\n",
        "    print(\"Beam Search ê²°ê³¼:\")\n",
        "    print(\"-\" * 30)\n",
        "    for sentence in test_sentences[:3]:  # ì²˜ìŒ 3ê°œë§Œ\n",
        "        try:\n",
        "            translation = beam_search_translate(model, sentence, en_vocab, nl_vocab, beam_size=2)\n",
        "            print(f\"ì˜ì–´: {sentence}\")\n",
        "            print(f\"ë²ˆì—­: {translation}\")\n",
        "            print()\n",
        "        except Exception as e:\n",
        "            print(f\"Beam search ì˜¤ë¥˜ ({sentence}): {e}\")\n",
        "            continue\n",
        "\n",
        "    # Cross-Attention ë¶„ì„\n",
        "    print(\"=\" * 50)\n",
        "    print(\"'I love llamas' Cross-Attention ë¶„ì„\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    target_sentence = \"I love llamas\"\n",
        "    try:\n",
        "        translation, attention_matrices, input_words, result_words = analyze_cross_attention(\n",
        "            model, target_sentence, en_vocab, nl_vocab\n",
        "        )\n",
        "\n",
        "        print(f\"ì…ë ¥: {target_sentence}\")\n",
        "        print(f\"ë²ˆì—­: {translation}\")\n",
        "        print(f\"ì •ë‹µ: Ik hou van lama's\")\n",
        "        print()\n",
        "\n",
        "        if attention_matrices and result_words and len(attention_matrices) > 0:\n",
        "            print(\"Cross-Attention ê°€ì¤‘ì¹˜ (Encoderâ†’Decoder):\")\n",
        "            for i, (target_word, attention) in enumerate(zip(result_words, attention_matrices)):\n",
        "                if len(attention) == len(input_words):\n",
        "                    print(f\"\\n'{target_word}' ìƒì„± ì‹œ ì£¼ëª©í•˜ëŠ” ì˜ì–´ ë‹¨ì–´:\")\n",
        "                    for j, (source_word, weight) in enumerate(zip(input_words, attention)):\n",
        "                        bar = 'â–ˆ' * int(weight * 20)\n",
        "                        print(f\"  {source_word:8}: {weight:.3f} {bar}\")\n",
        "        else:\n",
        "            print(\"Attention ë¶„ì„ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Attention ë¶„ì„ ì˜¤ë¥˜: {e}\")\n",
        "\n",
        "\n",
        "    return model, en_vocab, nl_vocab\n",
        "\n",
        "\n",
        "#  ì„±ëŠ¥ í‰ê°€ ë° ë¶„ì„\n",
        "def evaluate_model_performance(model, en_vocab, nl_vocab):\n",
        "    \"\"\"ëª¨ë¸ ì„±ëŠ¥ ì •ëŸ‰ì  í‰ê°€\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"ëª¨ë¸ ì„±ëŠ¥ ì •ëŸ‰ì  í‰ê°€\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    test_cases = [\n",
        "        (\"I love llamas\", \"Ik hou van lama's\"),\n",
        "        (\"I like cats\", \"Ik hou van katten\"),\n",
        "        (\"I eat apples\", \"Ik eet appels\"),\n",
        "        (\"I drink water\", \"Ik drink water\"),\n",
        "        (\"You are very kind\", \"Je bent erg aardig\")\n",
        "    ]\n",
        "\n",
        "    correct_greedy = 0\n",
        "    correct_beam = 0\n",
        "\n",
        "    print(\"ë²ˆì—­ ì •í™•ë„ í‰ê°€:\")\n",
        "    print(\"ë¬¸ì¥\" + \" \" * 15 + \"| Greedy | Beam Search\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    for en_sentence, correct_nl in test_cases:\n",
        "        greedy_result = translate_transformer(model, en_sentence, en_vocab, nl_vocab)\n",
        "        beam_result = beam_search_translate(model, en_sentence, en_vocab, nl_vocab)\n",
        "\n",
        "        greedy_correct = greedy_result.strip().lower() == correct_nl.lower()\n",
        "        beam_correct = beam_result.strip().lower() == correct_nl.lower()\n",
        "\n",
        "        if greedy_correct:\n",
        "            correct_greedy += 1\n",
        "        if beam_correct:\n",
        "            correct_beam += 1\n",
        "\n",
        "        print(f\"{en_sentence:<20} | {'âœ“' if greedy_correct else 'âœ—':^6} | {'âœ“' if beam_correct else 'âœ—':^11}\")\n",
        "\n",
        "    greedy_accuracy = (correct_greedy / len(test_cases)) * 100\n",
        "    beam_accuracy = (correct_beam / len(test_cases)) * 100\n",
        "\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"Greedy Decoding ì •í™•ë„: {greedy_accuracy:.1f}%\")\n",
        "    print(f\"Beam Search ì •í™•ë„: {beam_accuracy:.1f}%\")\n",
        "    print(f\"Beam Search ê°œì„ ë„: +{beam_accuracy - greedy_accuracy:.1f}%\")\n",
        "\n",
        "# ì‹¤í–‰ë¶€\n",
        "if __name__ == \"__main__\":\n",
        "    # ë©”ì¸ ëª¨ë¸ í›ˆë ¨ ë° í…ŒìŠ¤íŠ¸\n",
        "    model, en_vocab, nl_vocab = main()\n",
        "\n",
        "\n",
        "    # ì„±ëŠ¥ í‰ê°€\n",
        "    evaluate_model_performance(model, en_vocab, nl_vocab)\n",
        "\n",
        "    print(\"\\n\" + \"-\" * 30)\n",
        "    print(\"Transformer ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\")\n",
        "    print(\"ì£¼ìš” ê°œì„ ì‚¬í•­:\")\n",
        "    print(\"â€¢ ë³‘ë ¬ ì²˜ë¦¬ë¡œ ë¹ ë¥¸ í›ˆë ¨\")\n",
        "    print(\"â€¢ Multi-Head Attentionìœ¼ë¡œ í’ë¶€í•œ í‘œí˜„\")\n",
        "    print(\"â€¢ Self-Attentionìœ¼ë¡œ ì¥ê±°ë¦¬ ì˜ì¡´ì„± í•´ê²°\")\n",
        "    print(\"â€¢ Beam Searchë¡œ ë” ì •í™•í•œ ë²ˆì—­\")\n",
        "    print(\"-\" * 30)"
      ],
      "metadata": {
        "id": "VaqQ3_tXcohB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55618d93-1768-4fc3-b361-078a4afde414"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "Transformer ì•„í‚¤í…ì²˜ ì˜ì–´-ë„¤ëœë€ë“œì–´ ë²ˆì—­ ëª¨ë¸\n",
            "======================================================================\n",
            "ì˜ì–´ ì–´íœ˜ ìˆ˜: 56\n",
            "ë„¤ëœë€ë“œì–´ ì–´íœ˜ ìˆ˜: 57\n",
            "í›ˆë ¨ ë°ì´í„° ìˆ˜: 20\n",
            "\n",
            "ë°ì´í„° ê¸¸ì´ ë¶„ì„:\n",
            "ìµœëŒ€ ì˜ì–´ ë¬¸ì¥ ê¸¸ì´: 5\n",
            "ìµœëŒ€ ë„¤ëœë€ë“œì–´ ë¬¸ì¥ ê¸¸ì´: 7\n",
            "\n",
            "Transformer ëª¨ë¸ êµ¬ì¡°:\n",
            "  ì„ë² ë”© ì°¨ì›: 128\n",
            "  Attention í—¤ë“œ ìˆ˜: 4\n",
            "  ì¸ì½”ë” ë ˆì´ì–´: 2\n",
            "  ë””ì½”ë” ë ˆì´ì–´: 2\n",
            "  Feed-Forward ì°¨ì›: 512\n",
            "  ì´ íŒŒë¼ë¯¸í„° ìˆ˜: 947,513\n",
            "\n",
            "Transformer ëª¨ë¸ í›ˆë ¨ ì‹œì‘...\n",
            "Epoch [50/150], Loss: 0.8358, LR: 0.000100\n",
            "Epoch [100/150], Loss: 0.7775, LR: 0.000050\n",
            "Epoch [150/150], Loss: 0.7538, LR: 0.000050\n",
            "í›ˆë ¨ ì™„ë£Œ!\n",
            "\n",
            "==================================================\n",
            "ë²ˆì—­ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸\n",
            "==================================================\n",
            "Greedy Decoding ê²°ê³¼:\n",
            "------------------------------\n",
            "ì˜ì–´: I love llamas\n",
            "ë²ˆì—­: ik hou van lama's\n",
            "\n",
            "ì˜ì–´: I like cats\n",
            "ë²ˆì—­: ik hou van katten\n",
            "\n",
            "ì˜ì–´: I eat apples\n",
            "ë²ˆì—­: ik eet appels\n",
            "\n",
            "ì˜ì–´: I drink water\n",
            "ë²ˆì—­: ik drink water\n",
            "\n",
            "ì˜ì–´: You are very kind\n",
            "ë²ˆì—­: je bent erg aardig\n",
            "\n",
            "Beam Search ê²°ê³¼:\n",
            "------------------------------\n",
            "ì˜ì–´: I love llamas\n",
            "ë²ˆì—­: ik hou van lama's\n",
            "\n",
            "ì˜ì–´: I like cats\n",
            "ë²ˆì—­: ik hou van katten\n",
            "\n",
            "ì˜ì–´: I eat apples\n",
            "ë²ˆì—­: ik eet appels\n",
            "\n",
            "==================================================\n",
            "'I love llamas' Cross-Attention ë¶„ì„\n",
            "==================================================\n",
            "ì…ë ¥: I love llamas\n",
            "ë²ˆì—­: ik hou van lama's\n",
            "ì •ë‹µ: Ik hou van lama's\n",
            "\n",
            "Cross-Attention ê°€ì¤‘ì¹˜ (Encoderâ†’Decoder):\n",
            "\n",
            "'ik' ìƒì„± ì‹œ ì£¼ëª©í•˜ëŠ” ì˜ì–´ ë‹¨ì–´:\n",
            "  i       : 0.698 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
            "  love    : 0.176 â–ˆâ–ˆâ–ˆ\n",
            "  llamas  : 0.126 â–ˆâ–ˆ\n",
            "\n",
            "'hou' ìƒì„± ì‹œ ì£¼ëª©í•˜ëŠ” ì˜ì–´ ë‹¨ì–´:\n",
            "  i       : 0.192 â–ˆâ–ˆâ–ˆ\n",
            "  love    : 0.362 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
            "  llamas  : 0.446 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
            "\n",
            "'van' ìƒì„± ì‹œ ì£¼ëª©í•˜ëŠ” ì˜ì–´ ë‹¨ì–´:\n",
            "  i       : 0.354 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
            "  love    : 0.397 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
            "  llamas  : 0.249 â–ˆâ–ˆâ–ˆâ–ˆ\n",
            "\n",
            "'lama's' ìƒì„± ì‹œ ì£¼ëª©í•˜ëŠ” ì˜ì–´ ë‹¨ì–´:\n",
            "  i       : 0.267 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
            "  love    : 0.175 â–ˆâ–ˆâ–ˆ\n",
            "  llamas  : 0.558 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
            "\n",
            "==================================================\n",
            "ëª¨ë¸ ì„±ëŠ¥ ì •ëŸ‰ì  í‰ê°€\n",
            "==================================================\n",
            "ë²ˆì—­ ì •í™•ë„ í‰ê°€:\n",
            "ë¬¸ì¥               | Greedy | Beam Search\n",
            "--------------------------------------------------\n",
            "I love llamas        |   âœ“    |      âœ“     \n",
            "I like cats          |   âœ“    |      âœ“     \n",
            "I eat apples         |   âœ“    |      âœ“     \n",
            "I drink water        |   âœ“    |      âœ“     \n",
            "You are very kind    |   âœ“    |      âœ“     \n",
            "--------------------------------------------------\n",
            "Greedy Decoding ì •í™•ë„: 100.0%\n",
            "Beam Search ì •í™•ë„: 100.0%\n",
            "Beam Search ê°œì„ ë„: +0.0%\n",
            "\n",
            "ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯\n",
            "Transformer ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\n",
            "ì£¼ìš” ê°œì„ ì‚¬í•­:\n",
            "â€¢ ë³‘ë ¬ ì²˜ë¦¬ë¡œ ë¹ ë¥¸ í›ˆë ¨\n",
            "â€¢ Multi-Head Attentionìœ¼ë¡œ í’ë¶€í•œ í‘œí˜„\n",
            "â€¢ Self-Attentionìœ¼ë¡œ ì¥ê±°ë¦¬ ì˜ì¡´ì„± í•´ê²°\n",
            "â€¢ Beam Searchë¡œ ë” ì •í™•í•œ ë²ˆì—­\n",
            "ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯ğŸ¯\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "OWUi7wzAwVEv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. LLMëª¨ë¸(ì‚¬ì „í•™ìŠµëª¨ë¸) ì‚¬ìš©**"
      ],
      "metadata": {
        "id": "9MCC_ahkcrnb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ê°„ë‹¨í•œ ì˜ì–´-ë„¤ëœë€ë“œì–´ ë²ˆì—­ê¸°\n",
        "# ì½”ë©ì—ì„œ ë¨¼ì € ì‹¤í–‰: !pip install transformers torch\n",
        "\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "def simple_translator():\n",
        "    # ë²ˆì—­ íŒŒì´í”„ë¼ì¸ ë¡œë“œ\n",
        "    print(\"ëª¨ë¸ ë¡œë”© ì¤‘...\")\n",
        "    translator = pipeline(\n",
        "        \"translation\",\n",
        "        model=\"Helsinki-NLP/opus-mt-en-nl\",\n",
        "        device=0 if torch.cuda.is_available() else -1\n",
        "    )\n",
        "    print(\"âœ… ë¡œë”© ì™„ë£Œ!\")\n",
        "    print()\n",
        "\n",
        "    # í…ŒìŠ¤íŠ¸ ë¬¸ì¥ë“¤\n",
        "    sentences = [\n",
        "        \"I love llamas\",\n",
        "        \"I like cats\",\n",
        "        \"I eat apples\",\n",
        "        \"I drink water\",\n",
        "        \"You are very kind\"\n",
        "    ]\n",
        "\n",
        "    # ë²ˆì—­ ì‹¤í–‰\n",
        "    print(\"ë²ˆì—­ ê²°ê³¼:\")\n",
        "    print(\"-\" * 20)\n",
        "    for sentence in sentences:\n",
        "        result = translator(sentence)\n",
        "        translation = result[0]['translation_text']\n",
        "        print(f\"ì˜ì–´: {sentence}\")\n",
        "        print(f\"ë„¤ëœë€ë“œì–´: {translation}\")\n",
        "        print()\n",
        "\n",
        "    return translator\n",
        "\n",
        "def translate_target_sentence(translator):\n",
        "    target_sentence = \"I love llamas\"\n",
        "    result = translator(target_sentence)\n",
        "    translation = result[0]['translation_text']\n",
        "\n",
        "    print(f\"ì˜ì–´: {target_sentence}\")\n",
        "    print(f\"ë„¤ëœë€ë“œì–´: {translation}\")\n",
        "    print(f\"ì •ë‹µ: Ik hou van lama's\")\n",
        "\n",
        "    # ì •í™•ë„ í™•ì¸\n",
        "    if \"hou van\" in translation.lower():\n",
        "        print(\"ê²°ê³¼: âœ… ì™„ë²½í•œ ë²ˆì—­!\")\n",
        "    else:\n",
        "        print(\"ê²°ê³¼: âš ï¸ ë¶€ë¶„ì  ì¼ì¹˜\")\n",
        "\n",
        "    print()\n",
        "\n",
        "# ë©”ì¸ ì‹¤í–‰\n",
        "if __name__ == \"__main__\":\n",
        "    # ê¸°ë³¸ ë²ˆì—­ í…ŒìŠ¤íŠ¸\n",
        "    translator = simple_translator()\n",
        "\n",
        "    # 'I love llamas' íŠ¹ë³„ ë²ˆì—­\n",
        "    translate_target_sentence(translator)\n",
        "\n",
        "    print(\"ë²ˆì—­ ì™„ë£Œ! ğŸ‰\")"
      ],
      "metadata": {
        "id": "zRZLHQNAcoqS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1aa9aaa9-843e-4689-a735-b2994f2cd83b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ëª¨ë¸ ë¡œë”© ì¤‘...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ë¡œë”© ì™„ë£Œ!\n",
            "\n",
            "ë²ˆì—­ ê²°ê³¼:\n",
            "--------------------\n",
            "ì˜ì–´: I love llamas\n",
            "ë„¤ëœë€ë“œì–´: Ik hou van lama's.\n",
            "\n",
            "ì˜ì–´: I like cats\n",
            "ë„¤ëœë€ë“œì–´: Ik hou van katten.\n",
            "\n",
            "ì˜ì–´: I eat apples\n",
            "ë„¤ëœë€ë“œì–´: Ik eet appels.\n",
            "\n",
            "ì˜ì–´: I drink water\n",
            "ë„¤ëœë€ë“œì–´: Ik drink water.\n",
            "\n",
            "ì˜ì–´: You are very kind\n",
            "ë„¤ëœë€ë“œì–´: U bent erg aardig.\n",
            "\n",
            "ì˜ì–´: I love llamas\n",
            "ë„¤ëœë€ë“œì–´: Ik hou van lama's.\n",
            "ì •ë‹µ: Ik hou van lama's\n",
            "ê²°ê³¼: âœ… ì™„ë²½í•œ ë²ˆì—­!\n",
            "\n",
            "ë²ˆì—­ ì™„ë£Œ! ğŸ‰\n"
          ]
        }
      ]
    }
  ]
}